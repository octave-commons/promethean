services:
  sonarqube:
    image: sonarqube:25.8.0.112029-community
    container_name: sonarqube
    ports: ["9000:9000"]
    environment:
      - SONAR_ES_BOOTSTRAP_CHECKS_DISABLE=true
    volumes:
      - sonarqube_data:/opt/sonarqube/data
      - sonarqube_logs:/opt/sonarqube/logs
      - sonarqube_extensions:/opt/sonarqube/extensions
    restart: always
  chromadb:
    image: ${CHROMA_IMAGE:-chromadb/chroma:1.0.21.dev56}
    restart: always
    volumes:
      - ./data/chroma:/index_data
    ports:
      - 127.0.0.1:8000:8000
  ollama:
    image: ollama/ollama:0.11.8
    restart: always
    ports: ["127.0.0.1:11434:11434"]
    gpus: all
    volumes:
      - /usr/share/ollama/.ollama/models:/root/.ollama/models
    environment:
      OLLAMA_HOST: "0.0.0.0:11434"
      OLLAMA_DEBUG: "INFO"
      OLLAMA_LLM_LIBRARY: "cublas"
      CUDA_VISIBLE_DEVICES: "0"
      NVIDIA_VISIBLE_DEVICES: "all"
      NVIDIA_DRIVER_CAPABILITIES: "all"
      OLLAMA_CONTEXT_LENGTH: 32000
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "curl -fsS http://127.0.0.1:11434/api/tags >/dev/null && nvidia-smi >/dev/null 2>&1",
        ]
      interval: 10s
      timeout: 5s
      retries: 6
  redis:
    image: redis:6.2-alpine
    container_name: redis
    restart: unless-stopped
    ports:
      - "127.0.0.1:6379:6379"
    command: redis-server --save 60 1 --loglevel warning
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  tei-embeddings:
    profiles: ["ai"]
    image: ghcr.io/huggingface/text-embeddings-inference:1.6
    environment:
      - MODEL_ID=${TEI_MODEL:-nomic-ai/nomic-embed-text-v1.5}
      - NUM_SHARD=1
    ports: ["127.0.0.1:8081:80"]
    networks: [prom-net]
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "bash",
          "-lc",
          "curl -fsS http://127.0.0.1/health >/dev/null || exit 1",
        ]
      interval: 10s
      timeout: 5s
      retries: 6
  mongodb:
    image: mongo:6-jammy
    container_name: mongodb
    restart: always
    ports:
      - "127.0.0.1:27017:27017"
    volumes:
      - mongo-data:/data/db
  nats:
    profiles: ["messaging"]
    image: nats:2.10
    command: ["-js", "-sd", "/data"]
    volumes:
      - nats_data:/data
    ports: ["4222:4222", "8222:8222"]
    restart: unless-stopped

  haystack:
    profiles: ["ai", "rag"]
    image: ${HAYSTACK_IMAGE:-deepset/haystack:base-v2.17.1}
    environment:
      - PIPELINE_YAML=/app/pipelines/default.yaml
    volumes:
      - ./data/haystack:/app/pipelines
    ports: ["8000:8000"]
    restart: unless-stopped
    depends_on: [opensearch, meilisearch, postgres]

  tei-clip:
    profiles: ["ai", "vision"]
    image: ghcr.io/huggingface/text-embeddings-inference:1.6
    environment:
      - MODEL_ID=${CLIP_MODEL:-openai/clip-vit-large-patch14}
      - TASK=feature-extraction
    ports: ["127.0.0.1:8082:80"]
    restart: unless-stopped
  qdrant:
    image: ${QDRANT_IMAGE:-qdrant/qdrant:v1.15.4}
    restart: unless-stopped
    volumes:
      - qdrant-data:/qdrant/storage

  # tor:
  #   image: ${TOR_PROXY_IMAGE:-dperson/torproxy@sha256:d161ddddd47b4d2a91b8fe93d61e81b0760c0452ab6983a35ed37452e24004f6}
  #   restart: unless-stopped
  #   healthcheck:
  #     test: ["CMD-SHELL", "nc -z 127.0.0.1 9050 || exit 1"]
  #     interval: 10s
  #     timeout: 3s
  #     retries: 10

  # privoxy:
  #   image: ${PRIVOXY_IMAGE:-vimagick/privoxy@sha256:10ebc7e0c44f7ad5ef6f92fe929bb48984b0c6fb632439f7255028fc016118b9}
  #   restart: unless-stopped
  #   environment:
  #     - FORWARD_SOCKS5=tor:9050
  #   depends_on:
  #     tor:
  #       condition: service_healthy
  # ---------- Whisper (CUDA) ----------
  whisper-faster-openai:
    image: ${WHISPER_IMAGE:-fedirz/faster-whisper-server:0.6.0-rc.3-cuda}
    environment:
      ASR_ENGINE: whisper
      ASR_MODEL: medium
      ASR_BEAM_SIZE: 5
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
    gpus: all
    restart: unless-stopped

  # ---------- OVMS (Intel iGPU/NPU) ----------
  # ovms-npu:
  #   image: ${OVMS_IMAGE:-openvino/model_server:2025.2.1}
  #   command: --config_path /config/config.json --rest_port 9090 --port 9001
  #   ports:
  #     - "9001:9001" # gRPC
  #     - "9090:9090" # REST
  #   volumes:
  #     - ./config/ovm.json:/config/config.json:ro
  #     - ./models/ov:/opt/models:ro
  #   restart: unless-stopped

  semgrep:
    image: ${SEMGREP_IMAGE:-semgrep/semgrep:1.92.0}
    working_dir: /src
    volumes:
      - ./:/src:ro # read-only mount is cautious, optional
    entrypoint: []
    command: >
      semgrep scan --config p/default --json
    # Optional environment if you need Semgrep AppSec features
    # environment:
    #   SEMGREP_APP_TOKEN: "${SEMGREP_APP_TOKEN}"
volumes:
  nats_data:
  qdrant-data:
  redis_data:
  sonarqube_data:
  sonarqube_logs:
  sonarqube_extensions:
  mongo-data:

networks:
  prom-net:
    driver: bridge
