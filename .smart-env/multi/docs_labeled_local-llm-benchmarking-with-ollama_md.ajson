
"smart_sources:docs/labeled/local-llm-benchmarking-with-ollama.md": {"path":"docs/labeled/local-llm-benchmarking-with-ollama.md","last_embed":{"hash":null},"embeddings":{},"last_read":{"hash":"1kmkz8z","at":1762869250264},"class_name":"SmartSource","last_import":{"mtime":1762546647567,"size":2943,"at":1762869250684,"hash":"1kmkz8z"},"blocks":{"#---frontmatter---":[1,19],"#":[20,48],"##Might barely fit in my gpu and run at an *okay spee*":[49,61],"##Might barely fit in my gpu and run at an *okay spee*#{1}":[50,61]},"outlinks":[],"metadata":{"uuid":"a5baf674-4b06-4af2-886e-9228909d814a","created_at":"2025-10-07T21:55:08Z","title":"2025.10.07.21.55.08","filename":"Local LLM Benchmarking with Ollama","description":"This guide explores running and benchmarking local LLMs using Ollama, focusing on models like Qwen3, DeepSeek, and Gemma. It covers practical steps for evaluation and benchmarking on limited hardware, emphasizing models that fit within GPU memory constraints.","tags":["#ollama","#local-llm","#benchmarking","#qwen3","#deepseek","#gemma","#gpu-constrained"]},"task_lines":[]},