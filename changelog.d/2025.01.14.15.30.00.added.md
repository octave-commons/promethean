Added model-specific prompt caching to ollama-queue.ts using ChromaDB/InMemoryChroma for similar prompts.

## Changes

- Added model-specific prompt caching functionality to `.opencode/tool/ollama-queue.ts`
- Implemented separate cache instances per model for proper isolation
- Similarity-based cache lookup using cosine similarity (0.85 threshold)
- Added cache management tools (`manageCache`) for stats and clearing
- Integrated caching into job processing for generate and chat job types
- Fallback to InMemoryChroma when ChromaDB is unavailable
- 24-hour cache expiration with similarity-based matching

## Features

- **Model Isolation**: Each model (llama2, mistral, etc.) has its own cache
- Automatic cache hits for semantically similar prompts within same model
- Cache statistics and management via tool interface
- Embedding generation using existing ollamaEmbed utility
- Graceful fallback when embedding generation fails
- Per-model cache size tracking in queue info

## Configuration

- `CACHE_SIMILARITY_THRESHOLD`: 0.85 (cosine similarity)
- `CACHE_MAX_AGE_MS`: 24 hours
- Cache works with generate and chat job types (not embeddings)
- Model-specific caches created on-demand
