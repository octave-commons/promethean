awesome—let’s lock this into a real, repeatable framework you can drop into your monorepo. i’ll give you:

* the **suite** to run (coding, tool-use, vision, knowledge)
* the **SDK shape** (TypeScript, ESM, AVA) for custom benchmarks
* a **config** that targets your 8 GB VRAM reality (Gemma3 “tools”, Gemma3n Tools, MiniCPM-V 4.5) + remote judges (OpenRouter / Z.ai)
* the boring-but-critical **repro & security knobs**

I’m grounding every claim to working docs/tools so you’re not chasing vibes.

---

## what we’re standardizing on

* **Tool-calling:** BFCL (Berkeley Function Calling). It’s an executable function-calling eval with a maintained CLI (`bfcl-eval`) that can talk to any OpenAI-compatible endpoint, including **Ollama’s `/v1`**. Use the “prompt” track for generic instruct models, and FC track if the model natively supports OpenAI tools. ([PyPI][1])
* **Coding:** Aider **Polyglot** (225 Exercism problems). It ships a Docker harness and supports **Ollama via `ollama_chat/<model>`**. This is the coding eval Unsloth itself uses for Dynamic GGUF claims. ([Aider][2])
* **Vision (local):** **MiniCPM-V 4.5** (Ollama card + GH docs). Good single-image VLM that will run on consumer rigs with CPU offload. ([Ollama][3])
* **Knowledge sanity check (optional):** **MMLU (Hendrycks test)**—use lm-eval later if you want, but the canonical dataset is here. ([GitHub][4])
* **Local models that actually fit ~8 GB with tools/reasoning:**

  * **Gemma 3 “tools” variants** (community tags adding tool schemas/templates on top of Gemma 3 IT/QAT). These make tool-calling work on small VRAM. Note: official `gemma3:*` images may **not** expose tools; pick a “tools” tag. ([Ollama][5])
  * **Gemma 3n Tools** (e2b/e4b “n-models” designed for low resource). Several “tools” builds exist and run on laptops. ([Ollama][6])

> Ollama added **tool-calling** & OpenAI-compatibility last year; you’ll point everything at `http://localhost:11434/v1`. ([Ollama][7])

---

## repo layout (flat pkgs, native ESM)

```
packages/
  bench-core/     # tiny SDK (types, providers, judge adapters, result store)
  bench-cli/      # CLI: runs matrix from YAML, shells Aider/BFCL, writes JSON/CSV/MD
  bench-web/      # lightweight Web Components viewer for results
```

### SDK interfaces (functional, immutable)

```ts
// packages/bench-core/src/types.ts
export type Msg = { role: 'system'|'user'|'assistant'|'tool'; content: string };
export type Tool = { name: string; description: string; parameters: Record<string, unknown> };

export type ChatReq = Readonly<{
  model: string;
  messages: readonly Msg[];
  tools?: readonly Tool[];
  tool_choice?: 'auto' | { type: 'function'; function: { name: string } };
  temperature?: number;
  max_tokens?: number;
}>;

export type ChatRes = Readonly<{
  id: string;
  model: string;
  messages: readonly Msg[];
  tool_calls?: readonly { name: string; arguments: Record<string, unknown> }[];
  usage?: { input_tokens: number; output_tokens: number };
}>;

export type Provider = (r: ChatReq) => Promise<ChatRes>;

export type Judger = (rubric: string, output: string, reference?: string) =>
  Promise<{ score: number; rationale: string; rubric: string }>;
```

```ts
// packages/bench-core/src/providers/openaiLike.ts
export const makeProvider = (baseUrl: string, apiKey?: string): Provider => async req => {
  const body = {
    model: req.model,
    messages: req.messages,
    tools: req.tools?.map(t => ({ type: 'function', function: t })),
    tool_choice: req.tool_choice,
    temperature: req.temperature ?? 0.2,
    max_tokens: req.max_tokens ?? 1024,
    stream: false,
  };
  const r = await fetch(`${baseUrl}/chat/completions`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json', ...(apiKey ? { Authorization: `Bearer ${apiKey}` } : {}) },
    body: JSON.stringify(body),
  });
  if (!r.ok) throw new Error(`HTTP ${r.status} ${await r.text()}`);
  const j = await r.json();
  const choice = j.choices?.[0];
  const toolCalls = (choice?.message?.tool_calls ?? []).map((c: any) => ({
    name: c.function?.name,
    arguments: (() => { try { return JSON.parse(c.function?.arguments ?? '{}'); } catch { return {}; } })(),
  }));
  return {
    id: j.id, model: j.model,
    messages: [{ role: choice?.message?.role ?? 'assistant', content: choice?.message?.content ?? '' }],
    tool_calls: toolCalls, usage: j.usage,
  };
};
```

```ts
// packages/bench-core/src/judges/llmJudge.ts
import { makeProvider } from '../providers/openaiLike.js';
export const makeLLMJudge = (baseUrl: string, apiKey: string|undefined, model: string): Judger => {
  const prov = makeProvider(baseUrl, apiKey);
  return async (rubric, output, reference) => {
    const sys = 'Be a strict evaluator. Return pure JSON: {"score":0|1,"rationale":"..."}';
    const usr = `Rubric:\n${rubric}\n\nOutput:\n${output}\n${reference ? `\nReference:\n${reference}` : ''}`;
    const res = await prov({ model, messages: [{role:'system',content:sys},{role:'user',content:usr}] });
    try {
      const j = JSON.parse(res.messages[0]?.content ?? '{}');
      return { score: Number(j.score ?? 0), rationale: String(j.rationale ?? ''), rubric };
    } catch { return { score: 0, rationale: 'judge parse error', rubric }; }
  };
};
```

---

## config: local models + remote judges

```yaml
# bench-cli/config.yaml
runs:
  - name: gemma3-tools-q3
    provider: ollama
    baseUrl: http://localhost:11434/v1
    model: orieg/gemma3-tools:4b-it-qat-q3_k_s   # low-VRAM, tools enabled
    categories: [tooluse, coding]

  - name: gemma3n-tools-e2b
    provider: ollama
    baseUrl: http://localhost:11434/v1
    model: mashriram/gemma3nTools:e2b           # tiny + tools
    categories: [tooluse]

  - name: minicpm-v-4.5
    provider: ollama
    baseUrl: http://localhost:11434/v1
    model: openbmb/minicpm-v4.5                 # vision
    categories: [vision]

judges:
  openrouter:
    baseUrl: https://openrouter.ai/api/v1
    apiKeyEnv: OPENROUTER_API_KEY
    model: openai/gpt-4o-mini                    # or any “nano” judge you pick

  zai:
    baseUrl: https://open.bigmodel.cn/api/paas/v4
    apiKeyEnv: ZAI_API_KEY
    model: glm-4.6
```

OpenRouter is OpenAI-compatible and centralizes “everything” models; Zhipu’s GLM-4.6 is also OpenAI-style. Use whichever you want as the default judge. ([OpenRouter][8])

---

## CLI: runners for existing benchmarks + your SDK

* **Polyglot (coding):** shell their Docker harness, feed `ollama_chat/<tag>`. ([Aider][2])
* **BFCL (tool-use):** run prompt categories first (`simple,parallel,multiple`) against your local `/v1`. ([PyPI][1])
* **Custom tasks (SDK):** you define tools/messages; we call your local model, then LLM-judge the output.

```ts
// packages/bench-cli/src/index.ts (excerpt)
import YAML from 'yaml'; import { execa } from 'execa';
import { makeProvider } from '../../bench-core/src/providers/openaiLike.js';
import { makeLLMJudge } from '../../bench-core/src/judges/llmJudge.js';

const runPolyglot = async (ollamaModel: string) =>
  execa('bash',['-lc',`cd .third_party/aider || { mkdir -p .third_party && git clone https://github.com/Aider-AI/aider.git .third_party/aider; }
    cd .third_party/aider && ./benchmark/docker_build.sh && ./benchmark/docker.sh &&
    docker exec -i aider-benchmark bash -lc "pip install -e .[dev] && ./benchmark/benchmark.py run --model ollama_chat/${ollamaModel} --edit-format whole --threads 1 --exercises-dir polyglot-benchmark"`],
    { stdio:'inherit' });

const runBFCL = async (baseUrl: string) =>
  execa('bash',['-lc',`python - <<'PY'\nimport subprocess\nsubprocess.check_call(['python','-m','pip','install','-U','bfcl-eval'])\nsubprocess.run(['bfcl','generate','--model','openai-prompt','--test-category','simple,parallel,multiple','--skip-server-setup','--timeout','180'],check=False)\nsubprocess.run(['bfcl','evaluate','--model','openai-prompt','--test-category','simple,parallel,multiple'],check=False)\nPY`],{ stdio:'inherit' });

const runCustom = async (baseUrl:string, model:string, judge:{baseUrl:string, key?:string, model:string}) => {
  const provider = makeProvider(baseUrl);
  const tools = [{ name:'add', description:'add two ints', parameters:{ type:'object', properties:{a:{type:'integer'}, b:{type:'integer'}}, required:['a','b'] } }];
  const res = await provider({ model, messages:[{role:'user',content:'Use tools to compute 23+19.'}], tools, tool_choice:'auto' });
  const judgeFn = makeLLMJudge(judge.baseUrl, judge.key, judge.model);
  return judgeFn('Score 1 if the tool call exists and args are correct.', JSON.stringify(res.tool_calls ?? []), '{"name":"add","arguments":{"a":23,"b":19}}');
};
```

---

## reproducibility & “fit-in-8 GB” checklist

1. **Use a tools-enabled tag.** Official `gemma3:*` may not expose tools; community “tools” builds do. If an integration says “does not support tools,” you grabbed the wrong tag. ([n8n Community][9])
2. **KV-cache quant + Flash-Attention.** In Ollama, enable `OLLAMA_KV_CACHE_TYPE=q8_0` and `OLLAMA_FLASH_ATTENTION=1` before `ollama serve` to shrink runtime memory while keeping quality mostly intact. (That’s how folks sneak bigger models onto small GPUs.) ([Ollama][10])
3. **Force decoding params** across runs (temp/top-p/top-k/max_tokens) to make scores comparable.
4. **Capture system info** for every run (GPU model/VRAM, Ollama version).
5. **Don’t expose port 11434** to the internet; Ollama’s OpenAI endpoint is unauthenticated by default. Put it behind localhost/reverse-proxy. ([System Weakness][11])

---

## vision slice (MiniCPM-V 4.5)

For a minimal VLM check:

* Pass 10–20 caption/DocVQA prompts (downscale big PDFs to 1024px long-side to keep memory sane).
* Record latency & judge with your remote “nano” model (“is the answer correct vs reference?” 0/1).
* MiniCPM-V 4.5 has an **Ollama** model page with instructions; pair with the official repo when you need prompt formats. ([Ollama][3])

---

## model matrix that’s realistic on 8 GB

* **Gemma3 tools 4B Q3_K_S / Q2_K** (local tool-use baseline). ([Ollama][5])
* **Gemma3n Tools e2b** (ultra-light tool-use). ([Ollama][6])
* **MiniCPM-V 4.5** (vision). ([Ollama][3])
* **One Unsloth Dynamic GGUF** (e.g., Qwen3-8B UD Q5_K_M) for coding runs only, if RAM allows—Unsloth shows Polyglot holds up at low bits; validate locally. ([Unsloth Docs][12])

---

## judging strategy

* Start **single-judge** (OpenRouter small model *or* Z.ai GLM-4.6). If you see variance, do **majority-of-3** across 3 cheaper judges.
* Both OpenRouter and Z.ai speak **OpenAI-like** chat; your `makeLLMJudge` works with either by swapping baseUrl + key. ([OpenRouter][8])

---

## why this is sane (and future-proof)

* You’re leaning on **Ollama’s** official **tool-calling** + **OpenAI compatibility** (stable) and battle-tested harnesses (Aider/BFCL) instead of homebrew scripts. ([Ollama][7])
* **Gemma3n** and **community Gemma3-tools** unlock tool-calling on small VRAM—exactly your “8 GB but needs tools/reasoning” requirement. ([Ollama][6])
* **MiniCPM-V 4.5** gives you a legitimate VLM for local testing with a clean Ollama card + upstream docs. ([Ollama][3])
* If/when you fine-tune with **Unsloth**, export **GGUF + Modelfile** and drop it into the same harness—Aider Polyglot is already what Unsloth uses in its claims. ([Unsloth Docs][12])

---

## quick start (commands)

```bash
# 1) local runtime with memory savers
export OLLAMA_FLASH_ATTENTION=1
export OLLAMA_KV_CACHE_TYPE=q8_0
ollama serve &

# 2) pull small tool models
ollama pull orieg/gemma3-tools:4b-it-qat-q3_k_s
ollama pull mashriram/gemma3nTools:e2b

# vision
ollama pull openbmb/minicpm-v4.5

# 3) judges
export OPENROUTER_API_KEY=...
# or
export ZAI_API_KEY=...

# 4) run suite via CLI (config.yaml as above)
pnpm --filter bench-cli start config.yaml
```

---

if you want, I can also add:

* a **web-component** result viewer (sortable tables, trend lines)
* a **CSV/Markdown** exporter optimized for Obsidian notes
* a **mermaid** diagram generator for each run’s tool-call graph

but the pieces above are enough to get credible numbers on **tool-use**, **coding**, **vision**, and an optional **knowledge** slice—on an 8 GB box—while judging with **OpenRouter** or **Z.ai**.

**sources**

* Ollama: **tool support** (July 2024) and **OpenAI compatibility** (/v1). ([Ollama][7])
* Aider **Polyglot** benchmark & leaderboards; Docker harness; `ollama_chat/<model>`. ([Aider][2])
* **BFCL** CLI (`bfcl-eval`) for function-calling. ([PyPI][1])
* **Unsloth Dynamic GGUFs** on Aider Polyglot (why low-bit can still hold up). ([Unsloth Docs][12])
* **MiniCPM-V 4.5** (Ollama model card) & repo. ([Ollama][3])
* **Gemma3** / **Gemma3n** & **community tools** tags (low-VRAM, tools-enabled). ([Ollama][13])
* **OpenRouter** API docs; **Zhipu GLM-4.6** news/docs (OpenAI-style endpoints). ([OpenRouter][8])
* **MMLU** dataset (Hendrycks test). ([GitHub][4])
* **Security**: avoid exposing Ollama 11434 unauthenticated. ([System Weakness][11])

Want me to drop this into `riatzukiza/promethean` as a pnpm workspace with AVA tests wired and a first Obsidian-friendly report template?

[1]: https://pypi.org/project/bfcl-eval/?utm_source=chatgpt.com "bfcl-eval"
[2]: https://aider.chat/docs/leaderboards/?utm_source=chatgpt.com "Aider LLM Leaderboards"
[3]: https://ollama.com/openbmb/minicpm-v4.5?utm_source=chatgpt.com "openbmb/minicpm-v4.5"
[4]: https://github.com/hendrycks/test?utm_source=chatgpt.com "Measuring Massive Multitask Language Understanding"
[5]: https://ollama.com/orieg/gemma3-tools?utm_source=chatgpt.com "orieg/gemma3-tools"
[6]: https://ollama.com/library/gemma3n?utm_source=chatgpt.com "gemma3n"
[7]: https://ollama.com/blog/tool-support?utm_source=chatgpt.com "Tool support · Ollama Blog"
[8]: https://openrouter.ai/docs/api-reference/overview?utm_source=chatgpt.com "OpenRouter API Reference | Complete API Documentation"
[9]: https://community.n8n.io/t/i-do-use-the-ollama-with-gemma3-12b/100959?utm_source=chatgpt.com "I do use the ollama with gemma3:12b - Questions"
[10]: https://ollama.com/blog/openai-compatibility?utm_source=chatgpt.com "OpenAI compatibility · Ollama Blog"
[11]: https://systemweakness.com/ollama-exposed-unauthenticated-access-vulnerability-could-leak-your-llm-models-996798a1082f?utm_source=chatgpt.com "Ollama Exposed: Unauthenticated Access Vulnerability ..."
[12]: https://docs.unsloth.ai/new/unsloth-dynamic-ggufs-on-aider-polyglot?utm_source=chatgpt.com "Unsloth Dynamic GGUFs on Aider Polyglot"
[13]: https://ollama.com/library/gemma3?utm_source=chatgpt.com "gemma3"
