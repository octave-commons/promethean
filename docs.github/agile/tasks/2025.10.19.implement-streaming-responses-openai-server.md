---
title: 'Implement Streaming Responses for OpenAI Compatibility'
description: "Add Server-Sent Events (SSE) streaming support to match OpenAI's streaming API capabilities"
status: 'ready'
priority: 'P1'
storyPoints: 5
tags: ['features', 'streaming', 'sse', 'openai-compatibility', 'enhancement']
assignee: ''
createdAt: '2025-10-19T00:00:00Z'
updatedAt: '2025-10-19T00:00:00Z'
lastCommitSha: ''
dependencies: ['2025.10.19.optimize-state-management-openai-server.md']
blocking: ['2025.10.19.implement-function-calling-openai-server.md']
epic: '2025.10.19.openai-server-security-hardening-epic.md'
---

## Task Overview

Implement streaming responses using Server-Sent Events (SSE) to provide real-time response generation, matching OpenAI's streaming API capabilities and improving user experience.

## Acceptance Criteria

1. **Streaming Implementation**

   - [ ] Server-Sent Events (SSE) endpoint implementation
   - [ ] Real-time token streaming from LLM providers
   - [ ] Proper SSE format and headers
   - [ ] Connection management and cleanup

2. **OpenAI Compatibility**

   - [ ] Match OpenAI's streaming response format
   - [ ] Support `stream: true` parameter
   - [ ] Proper chunk formatting with `data:` prefix
   - [ ] `[DONE]` termination message

3. **Error Handling**

   - [ ] Stream interruption handling
   - [ ] Connection timeout management
   - [ ] Error propagation in streams
   - [ ] Graceful stream termination

4. **Performance & Reliability**
   - [ ] Backpressure handling
   - [ ] Connection pooling for streams
   - [ ] Memory-efficient streaming
   - [ ] Stream health monitoring

## Technical Implementation Details

### Files to Modify/Create

**New Files:**

- `src/streaming/streamHandler.ts` - Main streaming logic
- `src/streaming/sseFormatter.ts` - SSE response formatting
- `src/streaming/streamManager.ts` - Stream lifecycle management
- `src/streaming/types.ts` - Streaming type definitions
- `src/streaming/middleware.ts` - Streaming middleware

**Modified Files:**

- `src/server/chatCompletionRoute.ts` - Add streaming support
- `src/server/createServer.ts` - Register streaming routes
- `src/openai/ollamaHandler.ts` - Add streaming to Ollama handler
- `src/openai/defaultHandler.ts` - Add streaming to default handler
- `package.json` - Add streaming dependencies

### Streaming Response Format

```typescript
// OpenAI-compatible streaming response format
interface StreamChunk {
  id: string;
  object: 'chat.completion.chunk';
  created: number;
  model: string;
  choices: StreamChoice[];
}

interface StreamChoice {
  index: number;
  delta: {
    role?: string;
    content?: string;
  };
  finish_reason?: string | null;
}

// SSE formatter
export class SSEFormatter {
  static formatChunk(chunk: StreamChunk): string {
    const data = JSON.stringify(chunk);
    return `data: ${data}\n\n`;
  }

  static formatDone(): string {
    return `data: [DONE]\n\n`;
  }

  static formatError(error: Error): string {
    const errorData = {
      error: {
        message: error.message,
        type: 'error',
        code: 'stream_error',
      },
    };
    return `data: ${JSON.stringify(errorData)}\n\n`;
  }
}
```

### Streaming Handler Implementation

```typescript
export class StreamingHandler {
  private activeStreams = new Map<string, StreamConnection>();

  async handleStreamingRequest(
    request: FastifyRequest,
    reply: FastifyReply,
    chatRequest: ChatCompletionRequest,
  ) {
    const streamId = this.generateStreamId();
    const stream = reply.raw;

    // Set SSE headers
    reply.raw.writeHead(200, {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      Connection: 'keep-alive',
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Headers': 'Cache-Control',
    });

    try {
      // Create stream connection
      const connection = new StreamConnection(streamId, stream);
      this.activeStreams.set(streamId, connection);

      // Start streaming response
      await this.streamResponse(connection, chatRequest);
    } catch (error) {
      const errorChunk = SSEFormatter.formatError(error as Error);
      stream.write(errorChunk);
    } finally {
      // Cleanup
      this.activeStreams.delete(streamId);
      stream.write(SSEFormatter.formatDone());
      stream.end();
    }
  }

  private async streamResponse(connection: StreamConnection, request: ChatCompletionRequest) {
    const startTime = Date.now();
    let accumulatedContent = '';

    try {
      // Get streaming response from LLM provider
      const streamGenerator = this.getLLMStream(request);

      for await (const chunk of streamGenerator) {
        const streamChunk: StreamChunk = {
          id: this.generateChunkId(),
          object: 'chat.completion.chunk',
          created: startTime,
          model: request.model,
          choices: [
            {
              index: 0,
              delta: {
                role: 'assistant',
                content: chunk.content,
              },
            },
          ],
        };

        // Send chunk to client
        const formattedChunk = SSEFormatter.formatChunk(streamChunk);
        connection.write(formattedChunk);

        accumulatedContent += chunk.content;

        // Handle backpressure
        if (connection.needsBackpressure()) {
          await connection.waitForDrain();
        }
      }

      // Send final chunk
      const finalChunk: StreamChunk = {
        id: this.generateChunkId(),
        object: 'chat.completion.chunk',
        created: startTime,
        model: request.model,
        choices: [
          {
            index: 0,
            delta: {},
            finish_reason: 'stop',
          },
        ],
      };

      connection.write(SSEFormatter.formatChunk(finalChunk));
    } catch (error) {
      throw new StreamingError(`Stream failed: ${error.message}`);
    }
  }
}
```

### Stream Connection Management

```typescript
export class StreamConnection {
  private isAlive = true;
  private lastActivity = Date.now();
  private readonly timeout: NodeJS.Timeout;

  constructor(
    public readonly id: string,
    private readonly stream: any,
    private readonly timeoutMs = 300000, // 5 minutes
  ) {
    // Set up timeout cleanup
    this.timeout = setTimeout(() => {
      this.close();
    }, timeoutMs);

    // Handle client disconnect
    stream.on('close', () => {
      this.close();
    });

    stream.on('error', (error: Error) => {
      console.error(`Stream ${id} error:`, error);
      this.close();
    });
  }

  write(data: string): boolean {
    if (!this.isAlive) return false;

    try {
      const result = this.stream.write(data);
      this.lastActivity = Date.now();
      this.resetTimeout();
      return result;
    } catch (error) {
      this.close();
      return false;
    }
  }

  needsBackpressure(): boolean {
    return !this.stream.writable || this.stream.writableLength > 1024;
  }

  async waitForDrain(): Promise<void> {
    return new Promise((resolve) => {
      this.stream.once('drain', resolve);
    });
  }

  close(): void {
    if (!this.isAlive) return;

    this.isAlive = false;
    clearTimeout(this.timeout);

    try {
      this.stream.end();
    } catch (error) {
      // Ignore cleanup errors
    }
  }

  private resetTimeout(): void {
    clearTimeout(this.timeout);
    this.timeout = setTimeout(() => {
      this.close();
    }, this.timeoutMs);
  }
}
```

### Route Integration

```typescript
// Modified chat completion route
export const chatCompletionRoute = async (fastify: FastifyInstance) => {
  fastify.post(
    '/chat/completions',
    {
      schema: chatCompletionSchema,
    },
    async (request, reply) => {
      const chatRequest = request.body as ChatCompletionRequest;

      if (chatRequest.stream) {
        // Handle streaming request
        return streamingHandler.handleStreamingRequest(request, reply, chatRequest);
      } else {
        // Handle non-streaming request
        return regularHandler.handleRegularRequest(request, reply, chatRequest);
      }
    },
  );
};
```

## Testing Requirements

1. **Unit Tests**

   - [ ] SSE formatting accuracy
   - [ ] Stream connection management
   - [ ] Backpressure handling
   - [ ] Error propagation in streams

2. **Integration Tests**

   - [ ] End-to-end streaming flow
   - [ ] Client connection handling
   - [ ] Stream interruption scenarios
   - [ ] Multiple concurrent streams

3. **Performance Tests**

   - [ ] Stream latency measurement
   - [ ] Memory usage during streaming
   - [ ] Concurrent stream capacity
   - [ ] Backpressure effectiveness

4. **Compatibility Tests**
   - [ ] OpenAI client compatibility
   - [ ] Various client implementations
   - [ ] Network condition handling
   - [ ] Protocol compliance

## Configuration

Add streaming configuration:

```typescript
export const streamingConfig = {
  enabled: true,
  maxConcurrentStreams: 100,
  streamTimeout: 300000, // 5 minutes
  chunkSize: 1024,
  backpressureThreshold: 1024,
  enableCompression: false, // Compression not recommended for SSE
  heartbeatInterval: 30000, // 30 seconds
  maxStreamDuration: 600000, // 10 minutes
};
```

## Client Examples

### JavaScript Client

```javascript
const response = await fetch('/chat/completions', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'gpt-3.5-turbo',
    messages: [{ role: 'user', content: 'Hello!' }],
    stream: true,
  }),
});

const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const { done, value } = await reader.read();
  if (done) break;

  const chunk = decoder.decode(value);
  const lines = chunk.split('\n');

  for (const line of lines) {
    if (line.startsWith('data: ')) {
      const data = line.slice(6);
      if (data === '[DONE]') return;

      try {
        const parsed = JSON.parse(data);
        console.log(parsed.choices[0].delta.content);
      } catch (e) {
        // Ignore parsing errors
      }
    }
  }
}
```

## Monitoring & Metrics

1. **Stream Metrics**

   - Active stream count
   - Stream duration statistics
   - Bytes transferred per stream
   - Error rates by stream type

2. **Performance Metrics**
   - Stream latency
   - Backpressure events
   - Connection success rate
   - Memory usage by streams

## Rollback Plan

1. Disable streaming endpoint
2. Remove streaming middleware
3. Restore original route handlers
4. Monitor for stability issues

## Success Metrics

- Streaming latency < 100ms first chunk
- Support for 100+ concurrent streams
- Zero stream memory leaks
- 100% OpenAI client compatibility

## Documentation Updates

1. Streaming API documentation
2. Client integration examples
3. Performance tuning guide
4. Troubleshooting streaming issues

---

**Risk Level**: Medium (Feature enhancement)
**Estimated Effort**: 4-5 days
**Dependencies**: State management optimization
**Blocked By**: State management completion
