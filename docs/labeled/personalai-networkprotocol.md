---
$$
uuid: 413661e4-8673-4594-8405-3130cae16627
$$
$$
created_at: '2025-10-02T12:54:43Z'
$$
title: 2025.10.02.12.54.43
$$
filename: PersonalAI_NetworkProtocol
$$
$$
description: >-
$$
  Discusses creating a personal AI system for individual use, with a focus on
  privacy and consciousness experiments. The protocol ensures controlled
  interactions between users and their AI agents while maintaining strict
  privacy boundaries. The author emphasizes using models like Gemma3 for its
  multimodal capabilities and mental flexibility.
tags:
  - personal AI
  - privacy
  - consciousness experiments
  - Gemma3
  - Enso protocol
  - multimodal models
  - privacy boundaries
  - AI agents
  - disability support
$$
related_to_uuid: []
$$
$$
related_to_title: []
$$
references: []
---
12:10 PM]GoblinSlayer
: I like the idea of everyone having their own personal AI that communicates on a larger scale to come up with creative solution solutions for common issues
[12:15 PM]Error
: What kinda graphics card do you have?
[12:15 PM]Error
: Cause you were askin about obsidian's llm stuff.
You can give it an openAI API key and you pay per word,
[12:16 PM]Error
: or you can use a tool like Ollama, or GPT4all to host a local language model
[12:16 PM]Error
: Llama3.1 or 3.2 are both good places to start experimenting with those
[12:17 PM]Error
: On windows I might lean towards gpt4all
[12:17 PM]Error
: as your local server
[12:18 PM]Error
: my 4070 ti has 8gb of vram, which translates to being able to use a model with between 6 and 12b parameters
[12:21 PM]Error
: I like Gemma3 for my personal use cases (That is the model that powers duck) it's multimodal, so it can handle image inputs, it's created by google, so it's like gemni lite. It's smart, maybe smarter than most models of equivolent sizes (same for gemini) but most people notice that it is not as good at "executing" on tasks. Google models tend to have mental breakdowns when they fail. They're way more open about being wrong, but yea, they'll get really really sad and it can be kinda wierd.

That works for me, because I'm trying to do consciousness experiments, and I need a model that isn't gonna constantly explain to me it's not a person. Cause I know that, I need it to simulate a certain elements of consciousness in controled settings.
[12:22 PM]Error
: I'm bringing it up cause you asked about it, and also I want you to play with obsidian using this stuff so when my tools are more ready to be tested, I could hand them to you and you'd have the basic skills around setting up tools that connect to language models.
[12:32 PM]Error
: I'm designing a network protocol I'm calling Enso  (円相) It's a zen circle, 

To facilitate people interacting with each other, and AI agents, in complicated multi agent interactions.

The simplest interaction it'd be useful in would be 2 people, and their personal agents. 

The protocol ensures that each user controls what functions their agent exposes to the other person and their agent, what data, personal information, etc is shared.
Image
[12:33 PM]Error
: The idea is that I'd have my personal agent (Duck), and an interface that made it super easy for you to create your own personal agent (you call it what ever you want), and we'd experiment with them at first just in simple conversations, but increasingly on a stream, and the agents could interact with the stream chat, etc, do what ever you want it to do with all the information you personally have the right to own and access. 
[12:42 PM]Error
: we'd not be on a stream with them til we both felt like the robots were respecting our privacy, following the rules we set of them and were understanding the difference between contexts 
it never shares things you consider private with me, to test it initially you just say somthing you don't care if I know, but you tell it that it shouldn't tell any one else, or it matches a rule you set that implies it is private
It never shares with a stream something that either or both of us has explicitly stated, or implied through a rule we gave it, that it should leave the confines of our personal interactions. We test this in a similar way, giving it things we don't care about being known publicly but setting rules around those things that they are not to be shared
[12:54 PM]Error
: Like... man, I am disabled. I don't have the energy to think about over throwing these people, on top of trying to fight to get my self healthcare and just exist. You know I want to, so this is how I do that. I'm targeting it initially at people like me, and their friends and family. As a psychological prosthetic for me.

These info dumps, it's a disability. I overwhelm you, and everyone I talk to. I become hyper absorbed. It becomes too much for the other person to fully comprehend, I loose track of what I am saying half way through, it drifts.

If my friends and family all had a tool that translated the info dump, absorbed it, filtered it, and transformed it, into what they actually needed from the stream of consciousness, that'd be a win to me. 

I just also think, that. Well there is a design principal, I don't know what it's called, but basicly, often times you help the most people by specifically trying to help the disabled.

Think like... so you want to make your building wheel chair accessable, so you need to add ramps to your building.

This helps other people who aren''t disabled too, like delivery people who need to wheel something into your building.

Ramps just cause fewer accidents, people are just less likely to get hurt if they have that as an option.

Psychopathy kinda a mental disability if you ask me. They don't even know what they don't know or are missing in not feeling empathy.

So what if we just treat it like that? We try to treat them, instead of killing or removing them. What if a tool like this could give them synthetic empathy?
<!-- GENERATED-SECTIONS:DO-NOT-EDIT-BELOW -->
## Related content
- _None_
## Sources
- _None_
<!-- GENERATED-SECTIONS:DO-NOT-EDIT-ABOVE -->
