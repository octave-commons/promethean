What you’re describing is a **multi-LLM routing** system (sometimes also called a prompt router) — you intake a prompt, classify/score it, and send it to the “best” downstream model (or chain) depending on complexity, domain, tone, urgency, etc. There’s a fair bit of recent literature and applied engineering on this. E.g. “Doing More with Less: Implementing Routing Strategies in LLM-based Systems” ([arXiv][1]), “MixLLM: Dynamic Routing in Mixed Large Language Models” ([arXiv][2]), “Building an LLM Router for High-Quality and Cost-Effective Responses” ([Anyscale][3]), and “INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling” ([arXiv][4]).

Below I’ll sketch a design path architecture + training strategy + iteration and flag pitfalls / trade-offs. We can then zoom into any piece you like (e.g. training the classifier, building domain models, evaluation).

---

## Design goals & trade-offs

Before diving into architecture, it helps to explicitly enumerate your optimization goals and constraints, because routing always involves trade-offs:

* **Quality / accuracy / relevance**: you want the final answer to be strong (not degrade by misrouting).
* **Cost (compute, latency, inference cost)**: stronger models are more expensive / slower.
* **Latency**: routing overhead + switching should not dominate.
* **Scalability & adaptability**: new models / domains should integrate easily.
* **Robustness / fallback**: misclassifications should degrade gracefully.
* **Explainability / observability**: you want to know why a prompt was sent to a given model.
* **Continuous learning / feedback**: you want to improve routing over time using feedback.

The literature frames routing as an optimization: for a given prompt *q*, pick model *m* from your candidate set **M** to maximize a score (e.g. accuracy, user satisfaction) minus cost, or equivalently maximize score subject to cost / latency constraints. ([arXiv][5])

Given those goals, here’s how you can design a system step by step.

---

## High-level architecture

Here’s a typical layered architecture for LLM routing this is a “pre-generation routing” design — you decide which model to call *before* generating the answer.

```
Prompt → Preprocessor → Router / Classifier / Scorer → Model selection → Downstream model invocation → Postprocessing & feedback → (optionally fallback or re-route)
```

Each piece has a role:
```
1. **Preprocessor / featurizer**
```
   * Tokenize, embed, extract metadata (length, domain cues, keywords, urgency cues)
   * Possibly run a lightweight heuristic or scoring e.g. “this is very short / trivial”
   * Incorporate user / context metadata (e.g. from user profile, conversation history, timestamps, explicit urgency flags)
```
2. **Router / classifier / scorer**
```
   * A small model (or ensemble) that takes features (or prompt embedding) and outputs:

     * a predicted “bucket” (domain, complexity, tone, urgency)
     * or a score vector over candidate models or cost-quality tradeoffs
   * Or a hybrid: rule-based heuristics + ML classifier fallback
```
3. **Model selector / policy**
```
   * Given router scores / classification + constraints (cost, current load, latency budgets), pick a downstream model
   * Might incorporate load balancing, soft fallback (if selected model fails, route to more general model), ensemble / cascade can be considered
```
4. **Invocation & postprocessing**
```
   * Call the selected LLM with prompt plus maybe chains / tools
   * Process output e.g. validate, sanitize, re-prompt, check for hallucinations
   * Feedback loop: log routing decision + quality signals (user rating, heuristics, downstream checks)
```
5. **Monitoring & adaptation**
```
   * Track routing accuracy, misroutes, cost vs quality tradeoffs
   * Use feedback to retrain or fine-tune the router
   * Ability to introduce new domain models or plug in replacements without retraining entire system

Optionally, you might also adopt *cascade routing* (generate from a cheaper model first, then if the answer is “not confident enough,” escalate to a stronger model) or *post-generation routing* (ask multiple models in parallel or in sequence, then pick best) — but those add complexity and cost. The simpler “pre-route, then execute” is a good baseline.

Some routing systems also use *semantic routing*, where prompts are embedded and compared to “reference prompt embeddings” for each domain/model, and you choose the closest. [Amazon Web Services, Inc.][6] Also, some use *LLM-assisted routing* (i.e. ask a lightweight LLM to classify the prompt). [Amazon Web Services, Inc.][6]

---

## “Buckets” you might route by

You suggested a few axes: complexity, domain, tone, urgency. You’ll want to define discrete “buckets” or a structured label space. Some possible axes:

* **Complexity / reasoning depth**: trivial, medium, multi-step reasoning, code-generation, math, chain-of-thought, planning tasks, multi-hop reasoning
* **Domain / specialization**: legal, medical, finance, software, casual chat, marketing, customer support, etc.
* **Tone / style / formality**: friendly, authoritative, persuasive, neutral, poetic
* **Urgency / latency sensitivity**: “fast reply needed” vs “batch can wait”
* **Safety / sensitivity**: e.g. sensitive domains (health, legal) might route to a safer model or one with stricter guardrails

You can treat each axis independently and either (a) make the router predict a multi-dimensional label tuple (complexity × domain × tone × urgency), or (b) collapse into a single composite label e.g. “complex-medical-urgent-formal” though that combinatorial explosion can be tricky.

A pragmatic approach is: pick 2–3 most important axes initially (say, complexity, domain, urgency), get the router to predict a label over a manageable set (e.g. 10–20 buckets), and allow fallback to “general-purpose” model when uncertain.

---

## Training / bootstrapping the router

Since you said you don’t yet have many domain-specific models, you can start with a bootstrap strategy:

1. **Collect a dataset of prompts + labels**

   * Gather a corpus of prompt examples (both real user ones and synthetic variations).
   * Label them manually or semi-automatically with your desired bucket labels (complexity, domain, urgency).
   * Optionally use heuristics or rules for weak labeling e.g. short prompts → “low complexity”, presence of “legal contract” → “legal domain”.
```
2. **Train a routing model / classifier**
```
   * Use prompt embeddings (e.g. via OpenAI embeddings or your base model) + metadata features + small neural classifier (e.g. MLP).
   * Optionally fine-tune a small LLM as a router (e.g. a 7B model) to take in the prompt and output the bucket. This is common in practice (Anyscale’s approach) ([Anyscale][3])
   * In training, include a “confidence” head so that the router can yield “low confidence” and route to fallback.
```
3. **Evaluate / simulate routing**
```
   * Use held-out prompt set to test: routing decisions vs “oracle” decision (you might simulate “oracle” by evaluating quality of several candidate models).
   * Measure misrouting cost (e.g. how often you send to too weak or too strong model), cost savings, latency.
```
4. **Iterate & refine**
```
   * As you build domain-specific models, add them to routing candidate pool and retrain or fine-tune router.
   * Use live feedback (user ratings, heuristic checks, “did this answer satisfy constraints”) to refine routing decisions.
```
5. **Fallback / cascades**
```
   * If router is low-confidence, either route to a general-purpose model, or do a cascade (first try cheaper model, then escalate if output fails checks).
   * Also implement **safety nets**: if downstream response is empty / low-quality / violates policy, fallback automatically.
```
6. **Continuous learning / adaptation**
```
   * Log routing decisions + response outcomes + user feedback.
   * Use online learning or periodic retraining to adjust routing weights.
   * Support **adding new models**: the router should generalize to new candidate models with minimal rework some routing research addresses this, e.g. Arch-Router’s ability to incorporate new models without full retraining ([arXiv][7])

In some works, the router also learns to predict *cost vs quality tradeoffs* per prompt-model pair (i.e. estimate what each candidate model would produce, in terms of quality and latency) and then pick the best tradeoff. MixLLM is one such approach: it estimates predicted quality and cost for each candidate model and picks assignment via a meta-decision maker balancing quality / cost / latency. ([arXiv][2])

Another recent advance is **InferenceDynamics**: it profiles the capability and knowledge of each model and uses that to route by domain / capability. ([arXiv][4])

---

## Example toy workflow

I’ll sketch a simplified path you could take with minimal models to get started:

1. Start with one general-purpose model e.g. GPT-4 and one “cheap fallback” model e.g. LLaMA 7B or GPT-3.5.
2. Build a small router classifier that takes prompt embedding + features (length, presence of technical keywords, sentiment, urgency marker) → label {“low complexity”, “high complexity”, “fallback”}.
3. Train router on synthetic + real prompts.
4. At runtime:

   * If router predicts “low complexity” with confidence above threshold, route to cheap model.
   * Else route to GPT-4.
   * If cheap-model output fails a sanity check (e.g. fails format, low confidence, “I don’t know”), then escalate to GPT-4.
5. Log everything, collect user feedback, gradually add intermediate or domain-specific models, retrain router when you have enough data.

Over time, you’ll accumulate clusters of prompts (e.g. “legal contract review,” “software debugging,” “poetic rewriting”), and you can train or fine-tune models specialized on those domains. Then expand your routing buckets to include domain-specific labels, and feed those to router.

Once you have 3–4 downstream candidate models with different “strength / cost / domain spectra,” you can rebuild router to classify among those 3–4 rather than binary cheap-versus-general.

---

## Considerations, pitfalls & caveats

* **Misrouting cost is asymmetric**: sending a complex prompt to a weak model is much more harmful (bad answer) than sending a trivial prompt to a strong model (waste of cost). So design routing to err *toward safety* when uncertain (i.e. route to stronger model on low confidence).
* **Latency overhead**: the router + feature extraction must be lightweight; you don’t want to nullify gains by heavy routing time.
* **Cold starts & unseen domains**: the router must handle prompts whose domain is outside its training distribution; fallback must be robust.
* **Model drift / evolving distribution**: over time, prompt types may shift, so routing model must be retrained or adaptively updated.
* **Explainability & auditability**: you’ll want to log which features influenced routing and possibly produce introspection (why it chose domain X).
* **Capacity / load balancing**: if some downstream models are overloaded, router should consider current load / latency in selection.
* **Scalability & adding new models**: avoid rigid coupling; your router architecture should allow plugging in new candidate models without retraining everything. Some research e.g. Arch-Router focuses on that. ([arXiv][7])
* **Ensembles / cascades**: more aggressive strategies (asking multiple models and comparing) improve quality but cost more. Might be better in high-stakes queries.
* **Evaluation metrics are tricky**: you need to decide how to measure routing success not just classification accuracy, but end-to-end quality vs cost. Many papers use benchmarks like MT-Bench, MMLU, GSM8K, etc. ([Anyscale][3])
* **Safety / bias / guardrails**: when routing sensitive prompts (medical, legal, etc.), you may want stricter filtering or only route to certified safe models.

---

## Mapping this to your context (you don’t have many specific models yet)

Given you’re starting without many domain-specific models, here's how I’d pragmatically approach it in functional/iterative fashion, aligned with your development style:

1. **Phase 0: baseline router + two models**

   * Choose one strong “general” LLM and one cheaper fallback.
   * Build a minimal router that classifies prompt complexity (and perhaps domain hints) into “cheap safe, or fallback to general.”
   * Log everything.

2. **Phase 1: cluster & observe prompt space**

   * Over time, cluster prompts via embeddings, e.g. UMAP + clustering to discover natural domains or frequent topics.
   * For frequent clusters, start collecting or fine-tuning domain-specific models.

3. **Phase 2: expand routing labels & candidate models**

   * When you add a domain model (e.g. “legal model”), update label set: router now predicts “legal-expert,” “general,” “cheap fallback,” etc.
   * Retrain router or fine-tune using accumulated logging + annotated data.

4. **Phase 3: more sophistication in routing logic**

   * Switch router to predict score vector over candidate models (not just classification).
   * Add weighting for cost / latency / model load into selection policy.
   * Optionally add cascade logic: run cheap model first, check answer confidence, and escalate if needed.
```
5. **Phase 4: continuous optimization & feedback**
```
   * Use user ratings / automated evaluation to compute “routing regret” (i.e. what if you had used model B instead).
   * Perform offline A/B experiments of different routing policies.
   * Support dynamic adaptation e.g. online learning, bandit-style routing — MixLLM’s approach is in that direction. ([arXiv][2])
   * Introduce new models / retire old ones as performance shifts.

---

## Sketch in TypeScript / FP style (very rough sketch)

Here’s a toy functional-style sketch of how you might code routing logic (just at a high level, not full):

```ts
// Types
type Prompt = { text: string; metadata: Record<string, any> }
type RouteLabel = "cheap" | "general" | "legal" | "medical" | ... 
type RouterOutput = { label: RouteLabel; confidences: Record<RouteLabel, number> }
type Model = { label: RouteLabel; call: (prompt: Prompt) => Promise<string>; cost: number }

// featurizer: map prompt → features
const featurize = (p: Prompt): number[] => {
  const emb = embed(p.text)  // e.g. OpenAI embedding or your own
  const length = p.text.length
  const hasLegalKeywords = +(/\bcontract|liability|agreement\b/.test(p.text))
  // more features...
  return [...emb, length, hasLegalKeywords]
}

// router: features → label
const routerModel = trainRouterModel(...)  // e.g. an MLP or small LLM

const routePrompt = (p: Prompt): RouterOutput => {
  const features = featurize(p)
  const confidences = routerModel.predictProbabilities(features)  // e.g. softmax over labels
  const label = argMax(confidences)  // pick highest
  return { label, confidences }
}

// selection policy (could incorporate cost, threshold, fallback)
const selectModel = (routerOut: RouterOutput, models: Model[]): Model => {
  // if confidence too low, fallback to general
  if (routerOut.confidences[routerOut.label] < 0.6) {
    return models.find(m => m.label === "general")!
  }
  const chosen = models.find(m => m.label === routerOut.label)
  if (chosen) return chosen
  // else fallback
  return models.find(m => m.label === "general")!
}

// main inference
const infer = async (p: Prompt, models: Model[]): Promise<string> => {
  const routed = routePrompt(p)
  const model = selectModel(routed, models)
  const response = await model.call(p)
  return response
}
```

Over time you can enrich `routerModel`, make `selectModel` consider current load, latency, fallback logic, etc.

---

If you'd like, I can help you design:

* A **router training pipeline** (data, loss, architecture, evaluation)
* A lightweight **fallback / cascade logic**
* How to **cluster prompt space** and surface candidate domains
* How to scale to many domain models
* Or even a small reference library / skeleton code for routing in TypeScript

Do you want me to pick one of those and dive deeper with code / formulas?

[1]: https://arxiv.org/html/2502.00409v1?utm_source=chatgpt.com "Doing More with Less – Implementing Routing Strategies ..."
[2]: https://arxiv.org/abs/2502.18482?utm_source=chatgpt.com "MixLLM: Dynamic Routing in Mixed Large Language Models"
[3]: https://www.anyscale.com/blog/building-an-llm-router-for-high-quality-and-cost-effective-responses?utm_source=chatgpt.com "Building an LLM Router for High-Quality and Cost-Effective ..."
[4]: https://arxiv.org/abs/2505.16303?utm_source=chatgpt.com "INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling"
[5]: https://arxiv.org/html/2502.00409v3?utm_source=chatgpt.com "A Survey on Routing Strategies for Resource Optimisation ..."
[6]: https://aws.amazon.com/blogs/machine-learning/multi-llm-routing-strategies-for-generative-ai-applications-on-aws/?utm_source=chatgpt.com "Multi-LLM routing strategies for generative AI applications ..."
[7]: https://arxiv.org/abs/2506.16655?utm_source=chatgpt.com "Arch-Router: Aligning LLM Routing with Human Preferences"
