---
uuid: 28c29ebe-489f-4fd3-8e12-e18c8d7df630
created_at: '2025-10-07T00:41:24Z'
title: 2025.10.07.00.41.24
filename: 2025.10.07.00.41.24
description: DocOps summary for 2025.10.07.00.41.24
tags:
  - 2025.10.07.00.41.24
  - docops
  - autogenerated
related_to_uuid: []
related_to_title: []
references: []
---
Absolutely. Here’s **Pantheon — Part 4: runnable skeleton + ops knobs**. It’s concrete enough to drop into your monorepo and start iterating tonight, while staying faithful to your rules: **night = local-only; zero cloud fallback**.

I’m weaving in sources where it matters (vLLM continuous batching / PagedAttention, QLoRA, Toolformer, Qwen function-calling, OpenVINO agent path) so you can sanity-check the design choices.

---

# 0) Why these building blocks (quick receipts)

* **Throughput at night** comes from **continuous batching** + **PagedAttention** (vLLM). This amortizes prefill and packs concurrent jobs efficiently—exactly what your many tiny, repeatable ops need. ([VLLM Docs][1])
* **Specialization without VRAM pain** = **QLoRA** adapters on your traces (4-bit base + low-rank adapters). Proven path for local finetuning. ([arXiv][2])
* **Tool-use reliability** = Qwen’s function-calling + strict JSON schemas and a deterministic loop; there’s a **Qwen-Agent** + **OpenVINO** example for fully local deployments. ([Qwen][3])
* **Trace mining for tool-calls** mirrors **Toolformer’s** self-supervised pattern: small models learn when/how to call tools from examples. ([arXiv][4])

---

# 1) Repo layout (flat pkgs, ESM, AVA)

```
packages/
  pantheon-core/          # types, zod IO, deterministic tool loop (DFA)
  pantheon-tools/         # fs/git/biome/test tools w/ strict schemas
  pantheon-agents/        # small specialists (qwen8b codegen, docops, reviewers)
  pantheon-runner/        # orchestrator (DAG), mode policy, batching queues
  pantheon-cli/           # CLI (init/run/train/report)
  pantheon-report/        # morning report (Web Component)
```

All packages `"license": "GPL-3.0-only"`, ESM, AVA for tests, functional TS style.

---

# 2) Night runner enforces **local-only** (policy + network)

```ts
// packages/pantheon-runner/src/policy.ts
export type Mode = "day" | "night";

export const buildPolicy = (mode: Mode) => ({
  mode,
  allowOrigins: mode === "night" ? ["local"] as const : ["local","external"] as const,
  enforce: () => {
    const banned = ["OPENAI_API_KEY","ANTHROPIC_API_KEY","MISTRAL_API_KEY"];
    if (banned.some(k => process.env[k])) {
      throw new Error("Night mode forbids cloud credentials in env");
    }
  }
});
```

(Recommended ops: run the night runner under a user with **egress blocked** via `nftables`/`iptables` to physically prevent outbound calls even if a tool goes rogue.)

---

# 3) Deterministic Tool Loop (DFA)

Small models behave if you force **schema-strict** tool calls and finite steps.

```ts
// packages/pantheon-core/src/loop.ts
import { z } from "zod";

const ToolCall = z.union([
  z.object({ type: z.literal("tool"), name: z.string(), args: z.unknown() }),
  z.object({ type: z.literal("final"), output: z.unknown() })
]);

export type Tool<T extends z.ZodTypeAny, R extends z.ZodTypeAny> = {
  name: string; args: T; result: R; call: (a: z.infer<T>) => Promise<z.infer<R>>;
};

export const runDeterministicLoop = async ({
  maxSteps, maxFailures, renderPrompt, tools, modelCall, validateFinal
}: {
  maxSteps: number; maxFailures: number;
  renderPrompt: (history: any[]) => string;
  tools: ReadonlyArray<Tool<any,any>>;
  modelCall: (prompt: string) => Promise<string>;   // returns JSON matching ToolCall
  validateFinal: (o: unknown) => unknown;           // zod.parse
}) => {
  const history: any[] = [];
  let failures = 0;

  for (let step = 0; step < maxSteps; step++) {
    const prompt = renderPrompt(history);
    const raw = await modelCall(prompt);
    const parsed = ToolCall.safeParse(JSON.parse(raw));
    if (!parsed.success) { if (++failures > maxFailures) throw new Error("schema error"); continue; }

    if (parsed.data.type === "tool") {
      const t = tools.find(x => x.name === parsed.data.name);
      if (!t) { if (++failures > maxFailures) throw new Error("unknown tool"); continue; }
      const args = t.args.parse(parsed.data.args);
      const res  = await t.call(args);
      const result = t.result.parse(res);
      history.push({ tool: t.name, args, result });
    } else {
      return validateFinal(parsed.data.output);
    }
  }
  throw new Error("maxSteps reached");
};
```

Why this structure? Qwen’s function-calling templates are stable and documented; keep to a locked “thick prompt” and a strict loop—no free-text side effects. ([Qwen][3])

---

# 4) Local engines: vLLM & OpenVINO

* **vLLM driver** (GPU): expose an OpenAI-compatible endpoint and let the runner batch requests continuously (high tokens/sec, amortized prefill via **PagedAttention**). ([VLLM Docs][1])
* **OpenVINO driver** (CPU/iGPU/NPU): follow the **Qwen-Agent + OpenVINO** example to keep Qwen2/Qwen3 small variants fully local. ([OpenVINO Documentation][5])

---

# 5) Tools with Zod schemas

```ts
// packages/pantheon-tools/src/index.ts
import { z } from "zod";
export const WriteFile = {
  name: "write_file",
  args: z.object({ path: z.string(), content: z.string() }),
  result: z.object({ ok: z.literal(true), path: z.string() }),
  async call({ path, content }: { path: string; content: string }) {
    // fs.writeFile + simple guards
    return { ok: true as const, path };
  }
};

export const RunBiome = {
  name: "run_biome",
  args: z.object({ cwd: z.string() }),
  result: z.object({ ok: z.boolean(), output: z.string().optional() }),
  async call({ cwd }: { cwd: string }) {
    // execa("pnpm", ["-w","biome","check","--write"], { cwd })
    return { ok: true, output: "" };
  }
};

export const ComputeDiff = {
  name: "compute_diff",
  args: z.object({ cwd: z.string() }),
  result: z.object({ diff: z.string(), files: z.number(), lines: z.number() }),
  async call({ cwd }: { cwd: string }) {
    // execa("git", ["diff","--no-color"]); parse counts
    return { diff: "", files: 0, lines: 0 };
  }
};
```

---

# 6) Two seed agents (docops + codegen)

```ts
// packages/pantheon-agents/src/doc.frontmatter.ts
import { z } from "zod";
import { runDeterministicLoop } from "@pantheon-core/loop";
import { WriteFile } from "@pantheon-tools";

const In  = z.object({ path: z.string(), body: z.string(), rules: z.record(z.any()) });
const Out = z.object({ path: z.string(), body: z.string() });

export const DocFrontmatter = {
  name: "doc.frontmatter",
  in: In, out: Out,
  runtime: { engine: "openvino", model: "Qwen2-3B-Instruct-int4", origin: "local" as const },
  tools: [WriteFile],
  prompt: (i: z.infer<typeof In>) => `…locked template with rules & examples…`,
  run: (input: z.infer<typeof In>) => runDeterministicLoop({
    maxSteps: 6, maxFailures: 2,
    renderPrompt: (h) => /* render thick prompt with last N tool results */,
    tools: [WriteFile],
    modelCall: localModelCall,      // your vLLM/OpenVINO client
    validateFinal: Out.parse
  })
};
```

```ts
// packages/pantheon-agents/src/codegen.qwen8b.ts
import { runDeterministicLoop } from "@pantheon-core/loop";
import { WriteFile, RunBiome, ComputeDiff } from "@pantheon-tools";
import { z } from "zod";

const Spec = z.object({ title: z.string(), scope: z.string(), acceptance: z.array(z.string()) });
const CodePatch = z.object({ diff: z.string(), stats: z.object({ files: z.number(), lines: z.number() }) });

export const CodegenQwen8B = {
  name: "codegen.qwen8b",
  in: Spec, out: CodePatch,
  runtime: { engine: "vllm", model: "Qwen3-8B-Instruct", origin: "local" as const },
  tools: [WriteFile, RunBiome, ComputeDiff],
  prompt: (s: z.infer<typeof Spec>) => `…thick prompt: FP TS, AVA, ESM, GPL-3.0-only; tool schemas…`,
  run: (spec: z.infer<typeof Spec>) => runDeterministicLoop({
    maxSteps: 8, maxFailures: 2,
    renderPrompt: (h) => /* render thick prompt */,
    tools: [WriteFile, RunBiome, ComputeDiff],
    modelCall: localModelCall,
    validateFinal: CodePatch.parse
  })
};
```

Why Qwen here? Qwen’s function-calling guidance and **Qwen-Agent** provide concrete templating and loop patterns that keep small models on-schema; there’s also the **OpenVINO** notebook for local agent pipelines. ([Qwen][3])

---

# 7) Night DAG (spec → code → test → dual-review → merge)

```yaml
# packages/pantheon-runner/flows/spec2code.yaml
name: spec2code
entry: plan.validate
nodes:
  - { id: plan.validate, agent: plan.validate, inputs: [root] }
  - { id: codegen,        agent: codegen.qwen8b, inputs: [plan.validate] }
  - { id: test,           agent: test.ava,       inputs: [codegen] }
  - { id: reviewA,        agent: review.peer,    inputs: [codegen, test] }
  - { id: reviewB,        agent: review.peer,    inputs: [codegen, test] }
  - { id: merge,          agent: merge.gate,     inputs: [reviewA, reviewB] }
output: merge
```

---

# 8) Queue + batching (keep vLLM full)

You want **wide** micro-tasks so the batcher stays fat. vLLM’s continuous batching/PagedAttention are the throughput multiplier here. ([VLLM Docs][1])

**Queue item (JSONL):**

```json
{"flow":"spec2code","input":{"title":"Add GPL headers","scope":"docs/","acceptance":["All *.md have GPL header"]}}
```

Runner drains the queue, groups by agent, and pushes prompts to the engine in batches to amortize prefill. (Rule of thumb: measure average batch size `b`; if `b` < ~8, your chunks are too coarse.)

---

# 9) Artifacts → training data (for QLoRA)

**Per node**: `input.json`, `prompt.txt`, `tool_calls.jsonl`, `output.json`, `diff.patch`, `tests.log`, `review.json`, `metrics.json`.

**Trace mining → JSONL pairs**:

* Tool-use: `{"prompt":"...", "tool_name":"write_file", "args":{...}}` (mirrors Toolformer’s supervision style). ([arXiv][4])
* Generation: `{"prompt":"...", "output":{…}}`

Then finetune **per agent** with **QLoRA** (adapters only, 4-bit base). Promote agent@version only if it beats the last on held-out acceptance tests. ([arXiv][2])

---

# 10) What to chart each morning (math you’ll actually use)

* **Queue stability**: treat night as M/M/c; utilization
  (\rho = \frac{\lambda}{c\mu}). Keep (\rho \le 0.8) to avoid tail blow-ups (Erlang-C for wait probability). ([Better Tomorrow with Computer Science][6])
* **Accepted outputs** with dual review:
  (E[\text{accepted}] \approx T \sum_i \mu_i, p_i^2) (assumes independent reviewers).
* **Reviewer agreement**: **Cohen’s κ** (alert if ( \kappa < 0.6)). ([ACM Digital Library][7])
* **Batching health**: average batch size (b), tokens/sec, GPU util; vLLM’s design specifically targets these (continuous batching + PagedAttention). ([VLLM Docs][1])

Tiny TS helpers you can drop in:

```ts
// kappa.ts
export const kappa = (a: Array<"pass"|"fail">, b: Array<"pass"|"fail">) => {
  const n = a.length; let po = 0;
  let pa=0, pf=0, qa=0, qf=0;
  for (let i=0;i<n;i++){ if (a[i]===b[i]) po++; a[i]==="pass"?pa++:pf++; b[i]==="pass"?qa++:qf++; }
  const p0 = po/n, pe = ((pa/n)*(qa/n))+((pf/n)*(qf/n));
  return (p0 - pe) / (1 - pe);
};

// erlangC-ish wait prob (rough)
export const erlangC = (lambda:number, mu:number, c:number) => {
  const rho = lambda/(c*mu); if (rho>=1) return 1;
  const r = lambda/mu;
  let sum = 0; for (let k=0;k<c;k++){ sum += Math.pow(r,k)/factorial(k); }
  const pc = (Math.pow(r,c)/ (factorial(c) * (1-rho))) / (sum + Math.pow(r,c)/(factorial(c)*(1-rho)));
  return pc;
};
```

---

# 11) Training (isolated) — QLoRA adapters

Even if you dislike Python, keep it isolated in a Docker image that consumes your JSONL traces and spits out `adapter.safetensors`. The **QLoRA** paper + reference repo are the canonical starting points. ([arXiv][8])

* Input: mined JSONL for the target agent.
* Method: 4-bit base (NF4), LoRA rank small (e.g., 8–16), short epochs (overfit avoidance), held-out evaluation mirroring your acceptance tests.
* Promote on measurable gains only.

---

# 12) Thick prompt (sketch) for Qwen tool-use

Keep it locked and boring (that’s the point). Qwen’s docs stress consistent templates for function calling. ([Qwen][3])

```
SYSTEM: Narrow specialist. Act ONLY via tool calls that match the JSON schemas below.
CONSTRAINTS: Max 8 steps; if acceptance cannot be met, finalize with status=fail.
TOOLS (JSON):
 - write_file: {"path":"string","content":"string"}
 - run_biome:  {"cwd":"string"}
 - compute_diff: {"cwd":"string"}

REPO RULES: Functional TS, immutability, AVA tests, Native ESM, GPL-3.0-only.
ACCEPTANCE: {{JSON}}
CONTEXT: {{short snippets}}
LAST_RESULTS: {{last 3 tool results}}
SPEC: {{JSON}}

NEXT:
{"type":"tool","name":"...","args":{...}}
or
{"type":"final","output":{"status":"ok|fail","notes":"...","metrics":{...}}}
```

---

# 13) vLLM / OpenVINO pointers you’ll want handy

* vLLM docs & repo (PagedAttention, continuous batching, OpenAI-compatible server). ([VLLM Docs][1])
* OpenVINO notebook: **Function-calling agent with Qwen** (step-by-step). ([OpenVINO Documentation][5])
* Qwen function-calling docs + Qwen-Agent patterns (backend templates). ([Qwen][3])
* Toolformer paper (why trace-mined tool use works). ([arXiv][4])
* QLoRA paper + repo (NF4, low-rank adapters, training scripts). ([arXiv][8])

---

# 14) 48-hour plan (doable)

1. Wire **DocFrontmatter** + **CodegenQwen8B** agents with the DFA loop; stand up vLLM (GPU) and OpenVINO (CPU/iGPU) locally. ([VLLM Docs][1])
2. Create a **queue** of 80–200 micro-tasks (doc headers/normalize), run a night batch, and chart: average `b`, tokens/sec, pass rate, κ.
3. Mine **green traces**, run a tiny **QLoRA** adapter for DocFrontmatter, deploy as `doc.frontmatter@v0.2`, measure deltas. ([arXiv][8])

You’ll feel slow the first few nights. Then the adapters and batching start compounding.

If you want me to output repo-ready files next (packages, CLI skeleton, vLLM/OpenVINO client shims, Web Component morning report), say the word; I’ll dump the scaffolds in full.

[1]: https://docs.vllm.ai/en/stable/index.html?utm_source=chatgpt.com "Welcome to vLLM"
[2]: https://arxiv.org/abs/2305.14314?utm_source=chatgpt.com "QLoRA: Efficient Finetuning of Quantized LLMs"
[3]: https://qwen.readthedocs.io/en/latest/framework/function_call.html?utm_source=chatgpt.com "Function Calling - Qwen"
[4]: https://arxiv.org/abs/2302.04761?utm_source=chatgpt.com "Toolformer: Language Models Can Teach Themselves to Use Tools"
[5]: https://docs.openvino.ai/2024/notebooks/llm-agent-functioncall-qwen-with-output.html?utm_source=chatgpt.com "Create Function-calling Agent using OpenVINO and Qwen ..."
[6]: https://insujang.github.io/2024-01-07/llm-inference-continuous-batching-and-pagedattention/?utm_source=chatgpt.com "LLM Inference: Continuous Batching and PagedAttention"
[7]: https://dl.acm.org/doi/10.5555/3666122.3669119?utm_source=chatgpt.com "Toolformer: language models can teach themselves to use ..."
[8]: https://arxiv.org/pdf/2305.14314?utm_source=chatgpt.com "QLORA: Efficient Finetuning of Quantized LLMs"
