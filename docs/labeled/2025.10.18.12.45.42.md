---
uuid: 5ac5ee43-6590-49cd-845b-5c2852a5ff67
created_at: '2025-10-18T12:45:42Z'
title: 2025.10.18.12.45.42
filename: 2025.10.18.12.45.42
description: DocOps summary for 2025.10.18.12.45.42
tags:
  - 2025.10.18.12.45.42
  - docops
  - autogenerated
---
I. Summary
A. We’ll add a new workspace package, `@promethean/autocommit`, that watches a git repo, auto-stages changes, and commits with LLM-generated messages (OpenAI-compatible endpoint; defaults to local Ollama at `http://localhost:11434/v1`).
B. Design centers on safe batching (debounced commits), explicit ignore rules (e.g., `.git/**`, `node_modules/**`), and a low-friction CLI (`autocommit`) suitable for Promethean’s pnpm/Nx monorepo.
C. LLM client targets `/v1/chat/completions` with pluggable `OPENAI_BASE_URL`, `OPENAI_API_KEY`, and `AUTOCOMMIT_MODEL` (default: a local Ollama model).
D. Implementation uses `chokidar` (file watching), `execa` (git ops), and `fetch` (LLM). If the LLM fails or diff is huge, it falls back to a deterministic Conventional-Commits style message.
E. Contracts: no edits outside the package; functional style; GPL-3.0 license headers; scripts for build/test/lint; unit tests stubbed; Kanban tasks provided below to track integration.

II. Analysis
A. Risk: committing too aggressively can pollute history; mitigated with debounce, max commit size, and `--dry-run`.
B. Risk: secrets accidentally committed; mitigated by deferring to `.gitignore` and offering `AUTOCOMMIT_EXCLUDE` globs.
C. LLM dependency: endpoint/model availability and latency; we provide a fast fallback message path.
D. Diff size: very large diffs can exceed context; we cap diff bytes and provide a filename summary block.
E. Cross-platform git: we shell out to the user’s git; ensure git is installed and repo is cleanly initialized.
F. Loop risk: commits triggering themselves; we ignore `.git/**` and re-arm debounce only on working tree changes.
G. Compliance: package ships GPL-3.0-or-later; no repo-wide config changes required.
H. Unknowns: your preferred default model (we’ll default to a local Ollama model like `llama3.1:8b`).
I. Trade-off: `git` CLI via `execa` vs `isomorphic-git`; CLI is simpler and more robust across environments.

III. Decisions
A. Package name: `@promethean/autocommit`; CLI: `autocommit`.
B. Default LLM target: OpenAI-compatible, base URL `http://localhost:11434/v1`, model `llama3.1:8b`.
C. Message style: Conventional Commits (e.g., `feat:`, `fix:`, `chore:`); ≤72-char subject, optional bullets body.
D. Safety: 10-second debounce; max 20kB staged diff sent to LLM; fallback message when LLM fails/times out.
E. Configuration via env and flags; zero required server changes to the monorepo.

IV. Artifacts (spec + code you can drop under `packages/autocommit/`)

### Directory layout

```
packages/autocommit/
  package.json
  tsconfig.json
  src/
    cli.ts
    index.ts
    git.ts
    llm.ts
    config.ts
    messages.ts
  LICENSE-HEADER
  README.md
  src/tests/autocommit.unit.test.ts (stub)
```

### `package.json`

```json
{
  "name": "@promethean/autocommit",
  "version": "0.1.0",
  "type": "module",
  "license": "GPL-3.0-or-later",
  "bin": { "autocommit": "./dist/cli.js" },
  "exports": { ".": "./dist/index.js" },
  "scripts": {
    "build": "tsc -p tsconfig.json",
    "typecheck": "tsc -p tsconfig.json --noEmit",
    "dev": "tsx src/cli.ts",
    "lint": "eslint .",
    "test": "ava"
  },
  "dependencies": {
    "chokidar": "^3.6.0",
    "commander": "^12.1.0",
    "execa": "^9.2.0",
    "picocolors": "^1.0.0",
    "zod": "^3.23.8"
  },
  "devDependencies": {
    "@types/node": "^20.14.2",
    "typescript": "^5.6.3"
  }
}
```

### `tsconfig.json`

```json
{
  "extends": "../../config/tsconfig.base.json",
  "compilerOptions": {
    "target": "ES2022",
    "module": "ES2022",
    "moduleResolution": "NodeNext",
    "outDir": "dist",
    "rootDir": "src",
    "declaration": true,
    "sourceMap": true,
    "strict": true,
    "skipLibCheck": true
  },
  "include": ["src"]
}
```

### `src/config.ts`

```ts
import { z } from "zod";

export const ConfigSchema = z.object({
  path: z.string().default(process.cwd()),
  debounceMs: z.coerce.number().int().positive().default(10_000),
  baseUrl: z.string().default(process.env.OPENAI_BASE_URL ?? "http://localhost:11434/v1"),
  apiKey: z.string().optional().default(process.env.OPENAI_API_KEY),
  model: z.string().default(process.env.AUTOCOMMIT_MODEL ?? "llama3.1:8b"),
  temperature: z.coerce.number().min(0).max(2).default(0.2),
  maxDiffBytes: z.coerce.number().int().positive().default(20_000),
  exclude: z.string().default(process.env.AUTOCOMMIT_EXCLUDE ?? ""),
  signoff: z.coerce.boolean().default(process.env.AUTOCOMMIT_SIGNOFF === "1"),
  dryRun: z.coerce.boolean().default(process.env.DRY_RUN === "1"),
});

export type Config = z.infer<typeof ConfigSchema>;
```

### `src/messages.ts` (prompt templates)

```ts
export const SYSTEM = [
  "You write precise Conventional Commit messages.",
  "Subject line ≤ 72 chars; imperative mood; lowercase type.",
  "If changes are mixed, choose the most user-visible type.",
  "Include a short bullet list body only when helpful."
].join(" ");

export const USER = (summary: string, fileList: string, diff: string) => `
Project summary:
${summary}

Changed files:
${fileList}

Unified diff (truncated if large):
${diff}

Write one Conventional Commit. If appropriate, add a short body with bullets.
Return ONLY the commit message text.
`;
```

### `src/git.ts`

```ts
import { execa } from "execa";

export async function gitRoot(cwd: string) {
  const { stdout } = await execa("git", ["rev-parse", "--show-toplevel"], { cwd });
  return stdout.trim();
}

export async function hasRepo(cwd: string) {
  try { await gitRoot(cwd); return true; } catch { return false; }
}

export async function statusPorcelain(cwd: string) {
  const { stdout } = await execa("git", ["status", "--porcelain"], { cwd });
  return stdout.trim();
}

export async function listChangedFiles(cwd: string) {
  const s = await statusPorcelain(cwd);
  return s ? s.split("\n").map(l => l.slice(3).trim()).filter(Boolean) : [];
}

export async function addAll(cwd: string) {
  await execa("git", ["add", "-A"], { cwd });
}

export async function hasStagedChanges(cwd: string) {
  try {
    await execa("git", ["diff", "--cached", "--quiet"], { cwd }); // exits 0 when no diff
    return false;
  } catch { return true; }
}

export async function stagedDiff(cwd: string, maxBytes: number) {
  const { stdout } = await execa("git", ["diff", "--cached", "--unified=0"], { cwd, maxBuffer: 10 * 1024 * 1024 });
  const buf = Buffer.from(stdout);
  return buf.byteLength > maxBytes ? buf.subarray(0, maxBytes).toString("utf8") + "\n…[truncated]" : stdout;
}

export async function repoSummary(cwd: string) {
  const { stdout: remote } = await execa("git", ["remote", "-v"], { cwd, reject: false });
  const { stdout: branch } = await execa("git", ["rev-parse", "--abbrev-ref", "HEAD"], { cwd, reject: false });
  return `branch: ${branch.trim() || "unknown"}; remote: ${remote.split("\n")[0] || "none"}`;
}

export async function commit(cwd: string, message: string, signoff = false) {
  const args = ["commit", "-m", message];
  if (signoff) args.push("--signoff");
  await execa("git", args, { cwd });
}
```

### `src/llm.ts`

```ts
type ChatMsg = { role: "system" | "user" | "assistant"; content: string };

export async function chatCompletion(opts: {
  baseUrl: string; apiKey?: string; model: string; temperature: number; messages: ChatMsg[];
}) {
  const res = await fetch(`${opts.baseUrl.replace(/\/+$/,"")}/v1/chat/completions`, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      ...(opts.apiKey ? { Authorization: `Bearer ${opts.apiKey}` } : {})
    },
    body: JSON.stringify({
      model: opts.model,
      messages: opts.messages,
      temperature: opts.temperature,
      stream: false
    })
  });
  if (!res.ok) throw new Error(`LLM error ${res.status}: ${await res.text()}`);
  const json = await res.json();
  const content = json?.choices?.[0]?.message?.content;
  if (typeof content !== "string" || !content.trim()) throw new Error("Empty LLM response");
  return content.trim();
}
```

### `src/index.ts` (core flow)

```ts
import chokidar from "chokidar";
import pc from "picocolors";
import { Config } from "./config.js";
import { addAll, commit, gitRoot, hasRepo, hasStagedChanges, listChangedFiles, repoSummary, stagedDiff } from "./git.js";
import { chatCompletion } from "./llm.js";
import { SYSTEM, USER } from "./messages.js";

export async function start(config: Config) {
  const cwd = config.path;
  if (!(await hasRepo(cwd))) throw new Error(`Not a git repo: ${cwd}`);
  const root = await gitRoot(cwd);
  const log = (s: string) => console.log(pc.dim(`[autocommit] ${s}`));
  const warn = (s: string) => console.warn(pc.yellow(`[autocommit] ${s}`));

  const ignored = [
    "**/.git/**", "**/node_modules/**", "**/.turbo/**", "**/dist/**",
    ...(config.exclude ? config.exclude.split(",").map(s => s.trim()).filter(Boolean) : [])
  ];

  let timer: NodeJS.Timeout | null = null;

  const schedule = () => {
    if (timer) clearTimeout(timer);
    timer = setTimeout(async () => {
      timer = null;
      try {
        await addAll(root);
        if (!(await hasStagedChanges(root))) return;
        const files = await listChangedFiles(root);
        const summary = await repoSummary(root);
        const diff = await stagedDiff(root, config.maxDiffBytes);
        const fileList = files.slice(0, 100).join(", ");

        let message: string;
        try {
          message = await chatCompletion({
            baseUrl: config.baseUrl,
            apiKey: config.apiKey,
            model: config.model,
            temperature: config.temperature,
            messages: [
              { role: "system", content: SYSTEM },
              { role: "user", content: USER(summary, fileList, diff) }
            ]
          });
          // normalize to one subject + optional body
          message = message.replace(/\r\n/g, "\n").trim();
        } catch (err: any) {
          warn(`LLM failed: ${err?.message ?? err}. Falling back.`);
          const type = files.some(f => f.match(/\.(ts|tsx|js|jsx|mjs|cjs)$/)) ? "feat" : "chore";
          message = `${type}: update ${files.length} file(s) [auto]\n\n- ${files.slice(0,10).join("\n- ")}${files.length>10? "\n- …" : ""}`;
        }

        if (config.dryRun) {
          log(pc.cyan(`DRY RUN commit:\n${message}`));
          return;
        }
        await commit(root, message, config.signoff);
        log(pc.green(`Committed ${files.length} file(s).`));
      } catch (e: any) {
        warn(`Commit cycle error: ${e?.message ?? e}`);
      }
    }, config.debounceMs);
  };

  const watcher = chokidar.watch(root, {
    ignoreInitial: true,
    ignored,
    persistent: true,
    awaitWriteFinish: { stabilityThreshold: 500, pollInterval: 100 }
  });

  watcher.on("all", (_event, _path) => schedule());
  log(`Watching ${root} (debounce ${config.debounceMs}ms). Ignored: ${ignored.join(", ")}`);
  return { close: () => watcher.close() };
}
```

### `src/cli.ts`

```ts
#!/usr/bin/env node
import { Command } from "commander";
import { ConfigSchema } from "./config.js";
import { start } from "./index.js";

const program = new Command();
program
  .name("autocommit")
  .description("Watch a git repo and auto-commit with LLM-generated messages.")
  .option("-p, --path <dir>", "repo root (defaults to cwd)")
  .option("-d, --debounce-ms <ms>", "debounce window in ms", "10000")
  .option("--base-url <url>", "OpenAI-compatible base URL (default: Ollama http://localhost:11434/v1)")
  .option("--api-key <key>", "API key (not required for local Ollama)")
  .option("-m, --model <model>", "model name for chat completions")
  .option("--temperature <t>", "sampling temperature", "0.2")
  .option("--max-diff-bytes <n>", "cap diff bytes sent to LLM", "20000")
  .option("--exclude <globs>", "comma-separated extra ignore globs")
  .option("--signoff", "append Signed-off-by", false)
  .option("--dry-run", "do not actually commit", false)
  .action(async (opts) => {
    const cfg = ConfigSchema.parse(opts);
    await start(cfg);
  });

program.parseAsync().catch(err => {
  console.error(err?.stack || String(err));
  process.exit(1);
});
```

### `README.md` (package quick start)

````md
# @promethean/autocommit

Watches your git repo and automatically stages + commits changes with LLM-generated messages.

## Defaults
- **Endpoint:** `OPENAI_BASE_URL` (default `http://localhost:11434/v1`)
- **Model:** `AUTOCOMMIT_MODEL` (default `llama3.1:8b`)
- **Key:** `OPENAI_API_KEY` (optional for local Ollama)

## Install & Run (workspace)
```bash
pnpm --filter @promethean/autocommit install
pnpm --filter @promethean/autocommit build
pnpm --filter @promethean/autocommit exec autocommit -- --dry-run
````

## CLI

```
autocommit --path . --debounce-ms 10000 --model llama3.1:8b --dry-run
```

## Safety

* Respects `.gitignore` plus `--exclude`.
* Debounces to avoid noisy histories.
* Caps diff bytes to protect tokens & context.
* Falls back to deterministic messages when LLM unavailable.

````

### `src/tests/autocommit.unit.test.ts` (stub)
```ts
import test from "ava";
import { ConfigSchema } from "../config.js";

test("config defaults", t => {
  const cfg = ConfigSchema.parse({});
  t.true(cfg.debounceMs > 0);
  t.truthy(cfg.baseUrl);
  t.truthy(cfg.model);
});
````

V. Actions

1. **Create Task**

* `title:` Add @promethean/autocommit package (LLM-generated commit messages)
* `tags:` [ "#framework-core", "#doc-this" ]
* `priority:` P1
* `complexity:` M
* `urgency:` Now

2. **Review Task Transition**

* `title:` Move "Add @promethean/autocommit package" to Ready
* `current_status:` Accepted
* `valid_transitions:` [ "Breakdown → Ready", "Ready → Todo", "Todo → In-Progress" ]
* `desired_transition:` Breakdown → Ready
* `rationale:` Scope, acceptance criteria, and artifacts above meet Ready gate; change is isolated to a new package.

3. **Produce Documentation Artifact**

* `type:` Spec
* `title:` Autocommit Package — Design & Safety Spec
* `rationale:` Capture behavior, defaults, and safety constraints for review and onboarding.
* `abstract:` A CLI/daemon that watches a git repo, batches changes, and commits with LLM-generated messages over an OpenAI-compatible API (default Ollama). Includes debounce, ignore rules, diff size caps, and Conventional-Commits output with a fallback path.

4. **Board triage** — Create follow-up tasks:

   * Wire up CI scripts (`build`, `lint`, `test`) and ensure DoD checks pass for new package.
   * Add README usage examples and a small GIF in docs (optional).

5. **Run checks** — After you drop files in `packages/autocommit/`:

   * `pnpm --filter @promethean/autocommit install && pnpm -r build && pnpm --filter @promethean/autocommit test` (capture logs as evidence).

6. **Evaluate integration** — Dry-run locally:

   * `autocommit --dry-run` on a throwaway repo; verify Conventional-Commits shape and debounce behavior.

7. **Process task** — When ready, move task to In-Progress, implement, then to Review with test evidence (build + unit tests) before Done.


