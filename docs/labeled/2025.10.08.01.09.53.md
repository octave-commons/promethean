---
uuid: 208d99df-534a-4a32-84ae-3634e6b0594b
created_at: '2025-10-08T01:09:53Z'
title: 2025.10.08.01.09.53
filename: 2025.10.08.01.09.53
description: DocOps summary for 2025.10.08.01.09.53
tags:
  - 2025.10.08.01.09.53
  - docops
  - autogenerated
---
Rad. Let’s ship the actual scaffold so you can run it on your 8 GB box and compare locals (Gemma 3 “tools”, Gemma 3n Tools, MiniCPM-V 4.5) against remote judges (OpenRouter / Z.ai).

I’m giving you a **pnpm workspace** (Native ESM, AVA, functional style) with:

* runners for **BFCL** (tool-use) and **Aider Polyglot** (coding)
* a minimal **vision** check (MiniCPM-V 4.5 via OpenAI-compatible `/v1` with `image_url`)
* an **SDK** to define your own benchmarks + a simple LLM judge

All the moving parts map to current, documented interfaces (Ollama OpenAI-compat + tools, Aider’s `ollama_chat`, BFCL CLI). ([Ollama][1])

---

# 1) Workspace skeleton

```
promethean/
  packages/
    bench-core/
      src/
        types.ts
        providers/openaiLike.ts
        judges/llmJudge.ts
        sdk/bench.ts
      package.json
      tsconfig.json
    bench-cli/
      src/index.ts
      config.example.yaml
      package.json
      tsconfig.json
      ava.config.mjs
      test/smoke.test.ts
    bench-web/
      index.html
      src/results-viewer.js
      package.json
      tsconfig.json
  package.json
  pnpm-workspace.yaml
```

**root/package.json**

```json
{
  "name": "promethean",
  "private": true,
  "type": "module",
  "license": "GPL-3.0-only",
  "scripts": {
    "build": "pnpm -r --parallel build",
    "test": "pnpm -r test"
  }
}
```

**pnpm-workspace.yaml**

```yaml
packages:
  - "packages/*"
```

---

# 2) Core SDK (types, provider, judge)

**bench-core/src/types.ts**

```ts
export type Message = Readonly<{ role: 'system'|'user'|'assistant'|'tool'; content: string; name?: string }>;
export type Tool = Readonly<{ name: string; description: string; parameters: Record<string, unknown> }>;

export type ChatRequest = Readonly<{
  model: string;
  messages: readonly Message[];
  tools?: readonly Tool[];
  tool_choice?: 'auto' | { type: 'function'; function: { name: string } };
  temperature?: number;
  max_tokens?: number;
}>;

export type ChatResponse = Readonly<{
  id: string;
  model: string;
  messages: readonly Message[];
  tool_calls?: readonly { name: string; arguments: Record<string, unknown> }[];
  usage?: { input_tokens: number; output_tokens: number };
}>;

export type Provider = (req: ChatRequest) => Promise<ChatResponse>;

export type JudgeScore = Readonly<{ score: number; rationale: string; rubric: string }>;
export type Judger = (rubric: string, output: string, reference?: string) => Promise<JudgeScore>;
```

**bench-core/src/providers/openaiLike.ts**

```ts
import type { Provider, ChatRequest, ChatResponse } from '../types.js';

const headers = (apiKey?: string) =>
  apiKey ? { 'Content-Type': 'application/json', 'Authorization': `Bearer ${apiKey}` }
         : { 'Content-Type': 'application/json' };

export const makeProvider = (baseUrl: string, apiKey?: string): Provider =>
  async (req: ChatRequest): Promise<ChatResponse> => {
    const body = {
      model: req.model,
      messages: req.messages,
      tools: req.tools?.map(t => ({ type: 'function', function: t })),
      tool_choice: req.tool_choice,
      temperature: req.temperature ?? 0.2,
      max_tokens: req.max_tokens ?? 1024,
      stream: false
    };
    const r = await fetch(`${baseUrl}/chat/completions`, { method: 'POST', headers: headers(apiKey), body: JSON.stringify(body) });
    if (!r.ok) throw new Error(`HTTP ${r.status} ${await r.text()}`);
    const j = await r.json();
    const choice = j.choices?.[0] ?? {};
    const toolCalls = (choice.message?.tool_calls ?? []).map((c: any) => {
      let args: Record<string, unknown> = {};
      try { args = JSON.parse(c.function?.arguments ?? '{}'); } catch {}
      return { name: c.function?.name, arguments: args };
    });
    return {
      id: j.id, model: j.model,
      messages: [{ role: choice.message?.role ?? 'assistant', content: choice.message?.content ?? '' }],
      tool_calls: toolCalls, usage: j.usage
    };
  };
```

**bench-core/src/judges/llmJudge.ts**

```ts
import { makeProvider } from '../providers/openaiLike.js';
import type { Judger } from '../types.js';

export const makeLLMJudge = (baseUrl: string, apiKey: string|undefined, model: string): Judger => {
  const prov = makeProvider(baseUrl, apiKey);
  return async (rubric, output, reference) => {
    const sys = 'Be a strict, concise evaluator. Return pure JSON: {"score":0|1,"rationale":"..."}';
    const usr = `Rubric:\n${rubric}\n\nOutput:\n${output}\n${reference ? `\nReference:\n${reference}` : ''}`;
    const res = await prov({ model, messages: [{role:'system',content:sys},{role:'user',content:usr}] });
    try {
      const j = JSON.parse(res.messages[0]?.content ?? '{}');
      return { score: Number(j.score ?? 0), rationale: String(j.rationale ?? ''), rubric };
    } catch {
      return { score: 0, rationale: 'judge parse error', rubric };
    }
  };
};
```

**bench-core/src/sdk/bench.ts** – for *your* custom benchmarks

```ts
import { makeProvider } from '../providers/openaiLike.js';
import type { Provider, Message, Tool, Judger } from '../types.js';

export type CustomCase = Readonly<{
  name: string;
  messages: readonly Message[];
  tools?: readonly Tool[];
  tool_choice?: 'auto' | { type: 'function'; function: { name: string } };
  reference?: string;
  rubric: string;
}>;

export const runCases = async (
  baseUrl: string,
  apiKey: string|undefined,
  model: string,
  cases: readonly CustomCase[],
  judge: Judger
) => {
  const client: Provider = makeProvider(baseUrl, apiKey);
  const results = [];
  for (const c of cases) {
    const res = await client({ model, messages: c.messages, tools: c.tools, tool_choice: c.tool_choice });
    const judged = await judge(c.rubric, JSON.stringify(res.tool_calls ?? res.messages), c.reference);
    results.push({ name: c.name, res, judged });
  }
  return results;
};
```

---

# 3) CLI (runs BFCL, Polyglot, and your custom tasks)

**bench-cli/config.example.yaml**

```yaml
runs:
  - name: gemma3-tools-q3
    provider: ollama
    baseUrl: http://localhost:11434/v1
    model: orieg/gemma3-tools:4b-it-qat-q3_k_s
    categories: [tooluse, coding]

  - name: gemma3n-tools-e2b
    provider: ollama
    baseUrl: http://localhost:11434/v1
    model: mashriram/gemma3nTools:e2b
    categories: [tooluse]

  - name: minicpm-v-4.5
    provider: ollama
    baseUrl: http://localhost:11434/v1
    model: openbmb/minicpm-v4.5
    categories: [vision]

judges:
  openrouter:
    baseUrl: https://openrouter.ai/api/v1
    apiKeyEnv: OPENROUTER_API_KEY
    model: openai/gpt-4o-mini

  zai:
    baseUrl: https://open.bigmodel.cn/api/paas/v4
    apiKeyEnv: ZAI_API_KEY
    model: glm-4.6
```

**bench-cli/src/index.ts**

```ts
#!/usr/bin/env node
import fs from 'node:fs/promises';
import path from 'node:path';
import YAML from 'yaml';
import { execa } from 'execa';
import { makeLLMJudge } from '../../bench-core/src/judges/llmJudge.js';
import { runCases } from '../../bench-core/src/sdk/bench.js';

const readYaml = (p: string) => fs.readFile(p, 'utf8').then(YAML.parse);
const env = (k?: string) => (k ? process.env[k] : undefined);
const mkdirp = (p: string) => fs.mkdir(path.dirname(p), { recursive: true });

const runPolyglot = async (modelTag: string) => {
  // Aider recommends ollama_chat/<model> + can bump context to 8192 on server. :contentReference[oaicite:1]{index=1}
  await execa('bash', ['-lc', `
    cd .third_party/aider || { mkdir -p .third_party && git clone https://github.com/Aider-AI/aider.git .third_party/aider; }
    cd .third_party/aider && ./benchmark/docker_build.sh && ./benchmark/docker.sh &&
    docker exec -i aider-benchmark bash -lc "pip install -e .[dev] &&
      ./benchmark/benchmark.py run --model ollama_chat/${modelTag} --edit-format whole --threads 1 --exercises-dir polyglot-benchmark"
  `], { stdio: 'inherit' });
};

const runBFCL = async (baseUrl: string) => {
  // bfcl-eval supports pre-existing OpenAI-compatible endpoints (Ollama /v1). :contentReference[oaicite:2]{index=2}
  await execa('bash', ['-lc', `
    python - <<'PY'
import subprocess, os
subprocess.check_call(['python','-m','pip','install','-U','bfcl-eval'])
open('.bfcl.env','w').write('OPENAI_API_BASE=${baseUrl}\\n')
subprocess.run(['bfcl','generate','--model','openai-prompt','--test-category','simple,parallel,multiple','--skip-server-setup'], check=False)
subprocess.run(['bfcl','evaluate','--model','openai-prompt','--test-category','simple,parallel,multiple'], check=False)
PY
  `], { stdio: 'inherit' });
};

const runVisionSmoke = async (baseUrl: string, model: string, imagePath: string, out: string) => {
  // Use OpenAI vision format: content = [{type:'text'}, {type:'image_url', image_url:'data:image/...;base64,...'}]. :contentReference[oaicite:3]{index=3}
  const img = await fs.readFile(imagePath, { encoding: 'base64' });
  const payload = {
    model,
    messages: [{
      role: 'user',
      content: [
        { type: 'text', text: 'What does this image show? Be concise.' },
        { type: 'image_url', image_url: `data:image/png;base64,${img}` }
      ]
    }]
  };
  await mkdirp(out);
  await fs.writeFile(out, JSON.stringify(payload));
};

const writeJson = async (file: string, data: unknown) =>
  mkdirp(file).then(() => fs.writeFile(file, JSON.stringify(data, null, 2)));

const main = async () => {
  const cfg = await readYaml(process.argv[2] ?? 'config.yaml');
  const judgeCfg = cfg.judges?.zai ?? cfg.judges?.openrouter;
  const judge = judgeCfg ? makeLLMJudge(judgeCfg.baseUrl, env(judgeCfg.apiKeyEnv), judgeCfg.model) : undefined;

  for (const spec of cfg.runs) {
    const base = spec.baseUrl || 'http://localhost:11434/v1';
    const outDir = `bench_results/${spec.name}`;
    // Quick custom sanity case: tool selection
    if (judge) {
      const r = await runCases(base, undefined, spec.model, [{
        name: 'add-23-19',
        messages: [{ role: 'user', content: 'Use tools to compute 23 + 19.' }],
        tools: [{ name: 'add', description: 'add two ints', parameters: { type:'object', properties:{a:{type:'integer'}, b:{type:'integer'}}, required:['a','b'] } }],
        tool_choice: 'auto',
        reference: '{"name":"add","arguments":{"a":23,"b":19}}',
        rubric: 'Score 1 if a tool call exists with name "add" and arguments a=23, b=19.'
      }], judge);
      await writeJson(`${outDir}/custom-tool.json`, r);
    }

    if (spec.categories.includes('coding')) await runPolyglot(spec.model);           // :contentReference[oaicite:4]{index=4}
    if (spec.categories.includes('tooluse')) await runBFCL(base);                    // :contentReference[oaicite:5]{index=5}
    if (spec.categories.includes('vision')) await runVisionSmoke(base, spec.model, 'samples/image.png', `${outDir}/vision-request.json`);
  }
};

main().catch(e => { console.error(e); process.exit(1); });
```

**bench-cli/ava.config.mjs**

```js
export default { files: ['test/*.test.ts'], extensions: { ts: 'module' } }
```

**bench-cli/test/smoke.test.ts**

```ts
import test from 'ava';
import { makeLLMJudge } from '../../bench-core/src/judges/llmJudge.js';
test('judge factory exists', t => { t.truthy(makeLLMJudge); });
```

---

# 4) Tiny web viewer (Web Components)

**bench-web/index.html**

```html
<!doctype html>
<html>
  <head><meta charset="utf-8"><title>Bench Results</title></head>
  <body>
    <bench-results></bench-results>
    <script type="module" src="./src/results-viewer.js"></script>
  </body>
</html>
```

**bench-web/src/results-viewer.js**

```js
const readJSON = async path => fetch(path).then(r => r.json());
const by = k => (a,b) => (a[k] > b[k] ? 1 : -1);

export class BenchResults extends HTMLElement {
  async connectedCallback() {
    const files = await window.showDirectoryPicker?.().catch(()=>null);
    const rows = [];
    if (files) {
      for await (const [name, handle] of files.entries()) {
        if (!name.endsWith('.json')) continue;
        const file = await handle.getFile();
        const data = await file.text().then(JSON.parse).catch(()=>null);
        if (!data) continue;
        rows.push({ name, score: Array.isArray(data) ? data[0]?.judged?.score ?? null : data.judged?.score ?? null });
      }
    }
    this.innerHTML = `
      <style>table{border-collapse:collapse}td,th{border:1px solid #ddd;padding:6px}</style>
      <h1>Bench Results</h1>
      <p>Pick your <code>bench_results</code> folder when prompted.</p>
      <table><thead><tr><th>file</th><th>score</th></tr></thead>
        <tbody>${rows.sort(by('name')).map(r=>`<tr><td>${r.name}</td><td>${r.score ?? ''}</td></tr>`).join('')}</tbody>
      </table>`;
  }
}
customElements.define('bench-results', BenchResults);
```

---

# 5) Runbook (8 GB-friendly)

1. **Start Ollama with memory savers**

```bash
export OLLAMA_CONTEXT_LENGTH=8192   # Aider recommends bumping ctx; default can be small. :contentReference[oaicite:6]{index=6}
export OLLAMA_FLASH_ATTENTION=1
export OLLAMA_KV_CACHE_TYPE=q8_0    # Official FAQ: quantized KV when Flash Attention is on. :contentReference[oaicite:7]{index=7}
ollama serve &
```

Ollama’s **OpenAI-compatible /v1** (chat/completions) and **tool calling** are supported; we’ll point both BFCL and your SDK at this. ([Ollama][1])

2. **Pull models that actually fit**

```bash
ollama pull orieg/gemma3-tools:4b-it-qat-q3_k_s     # tools-enabled Gemma3 variant (low-VRAM). :contentReference[oaicite:9]{index=9}
ollama pull mashriram/gemma3nTools:e2b               # tiny Gemma3n + tools. :contentReference[oaicite:10]{index=10}
ollama pull openbmb/minicpm-v4.5                     # VLM with llama.cpp/Ollama support. :contentReference[oaicite:11]{index=11}
```

3. **Judges**

```bash
export OPENROUTER_API_KEY=...   # OpenAI-style unified API. :contentReference[oaicite:12]{index=12}
# or
export ZAI_API_KEY=...          # GLM-4.6 OpenAI-style API. :contentReference[oaicite:13]{index=13}
```

4. **Run**

```bash
pnpm i
cp packages/bench-cli/config.example.yaml packages/bench-cli/config.yaml
pnpm --filter bench-cli start packages/bench-cli/config.yaml
```

5. **View results**
   Open `packages/bench-web/index.html`, choose the `bench_results/` folder; you’ll see quick scores. (Polyglot and BFCL produce their own detailed artifacts too.) ([Aider][2])

---

# 6) Vision example (MiniCPM-V 4.5)

Your CLI already writes a proper “image_url” OpenAI payload for Ollama. That’s the format Ollama’s OpenAI-compat docs demonstrate (text + base64 image). If you want a one-shot curl to sanity-check: ([Ollama][3])

```bash
b64=$(base64 -w0 samples/image.png)
curl -s http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model":"openbmb/minicpm-v4.5",
    "messages":[{"role":"user","content":[
      {"type":"text","text":"What is in this image?"},{"type":"image_url","image_url":"data:image/png;base64,'"${b64}"'"}
    ]}]
  }' | jq '.choices[0].message.content'
```

Model card + repo confirm MiniCPM-V 4.5 offers GGUF/llama.cpp/Ollama support for efficient CPU-heavy local runs. ([Ollama][4])

---

# 7) Add MMLU (optional)

When you want a general-knowledge sanity check, pull **MMLU** from the Hendrycks repo or the HF dataset; plug into lm-eval later. For now, just note the canonical sources. ([GitHub][5])

---

# 8) Notes that save you time

* **Context & truncation:** Aider explicitly shows starting Ollama with `OLLAMA_CONTEXT_LENGTH=8192`; without that you’ll silently drop prompt/history on some rigs. (People hit 4k/2k defaults; if you see truncation, verify with `ollama ps`.) ([Aider][6])
* **KV-cache quant:** Official FAQ documents `OLLAMA_KV_CACHE_TYPE` (default `f16`). Flip to `q8_0` for big memory wins; if a specific model (e.g., some Gemma3 builds) gets flaky, revert to `f16`. ([Ollama][7])
* **Tool-enabled Gemma3:** Use tags that **explicitly** say “tools” (Ollama Search shows several well-maintained ones). Don’t assume base `gemma3:*` exposes tools. ([Ollama][8])
* **Aider Polyglot:** it’s the benchmark Unsloth leans on for Dynamic GGUF claims; the docker harness isolates code execution and supports `ollama_chat/<model>`. ([Aider][2])
* **BFCL:** use **prompt** categories against plain instruct models first; move to FC tracks if a model truly implements OpenAI tools natively. PyPI docs cover “pre-existing OpenAI-compatible endpoints.” ([PyPI][9])

---

## Mermaid (suite topology)

```mermaid
flowchart TB
  subgraph Local[Local rig (8 GB VRAM)]
    O[Ollama /v1 (tools+vision)]
    M1[Gemma3 Tools]
    M2[Gemma3n Tools]
    V1[MiniCPM-V 4.5]
    O --> M1
    O --> M2
    O --> V1
  end
  subgraph Harness[Bench harnesses]
    B[BFCL (tool-use)]
    P[Aider Polyglot (coding)]
    C[Custom SDK tasks]
  end
  subgraph Judges[Remote judges]
    R1[OpenRouter judge]
    R2[Z.ai GLM-4.6 judge]
  end
  Harness --> O
  C --> R1
  C --> R2
```

---

If you want, I can add:

* a `bench-core/src/vision.ts` helper that does base64 + size-cap,
* a CSV+Markdown exporter tailored for Obsidian,
* and a tiny “model matrix” runner so you can sweep multiple tags (e.g., `q2_k`, `q3_k_s`) and plot pass-2 vs latency.

But as-is, this framework runs:

* **BFCL** against your local **/v1** (tool-use),
* **Polyglot** via Docker (coding),
* **a minimal VLM check** (MiniCPM-V 4.5),
* **custom tasks** with a pluggable LLM judge (OpenRouter or Z.ai).

All of that is aligned to the current docs for Ollama OpenAI-compat, tool support, Aider, BFCL, and MiniCPM-V 4.5. ([Ollama][1])

If you want me to drop this into `riatzukiza/promethean` as a real PR (pnpm workspace + scripts wired), say the word and I’ll generate the exact files.

[1]: https://ollama.com/blog/openai-compatibility?utm_source=chatgpt.com "OpenAI compatibility · Ollama Blog"
[2]: https://aider.chat/docs/leaderboards/?utm_source=chatgpt.com "Aider LLM Leaderboards"
[3]: https://docs.ollama.com/openai "OpenAI compatibility - Ollama"
[4]: https://ollama.com/openbmb/minicpm-v4.5?utm_source=chatgpt.com "openbmb/minicpm-v4.5"
[5]: https://github.com/hendrycks/test?utm_source=chatgpt.com "Measuring Massive Multitask Language Understanding"
[6]: https://aider.chat/docs/llms/ollama.html?utm_source=chatgpt.com "Ollama"
[7]: https://docs.ollama.com/faq?utm_source=chatgpt.com "FAQ"
[8]: https://ollama.com/search?q=gemma3+tools&utm_source=chatgpt.com "gemma3 tools · Ollama Search"
[9]: https://pypi.org/project/bfcl-eval/?utm_source=chatgpt.com "bfcl-eval"
