---
uuid: 48fd63e4-64bc-4839-86ad-9297b1bdd4cd
created_at: '2025-10-07T22:13:53Z'
title: 2025.10.07.22.13.53
filename: 2025.10.07.22.13.53
description: DocOps summary for 2025.10.07.22.13.53
tags:
  - 2025.10.07.22.13.53
  - docops
  - autogenerated
related_to_uuid: []
related_to_title: []
references: []
---
love this direction. unsloth gives you three wins that map to your goals:

1. dynamic GGUFs that actually hold up on serious coding evals (Polyglot/Aider), not just toy scores. vendor-claimed, but it’s exactly the benchmark you wanted to run yourself. ([Unsloth Docs][1])
2. a clean Docker path for local fine-tuning/RL without dependency hell. ([Unsloth Docs][2])
3. one-click(ish) export to **GGUF** + auto **Modelfile** for **Ollama**, which fixes the #1 cause of “my finetune is gibberish in Ollama”: wrong chat template. ([Unsloth Docs][3])

below is a compact, practical pipeline that’s unsloth-native, runs **Aider Polyglot** for coding, and (optionally) **BFCL** for tool-use. it assumes you’ll start from Unsloth’s prebuilt GGUFs (Qwen3-8B/14B, Qwen3-Coder, DeepSeek-V3.1) and/or export your own with Unsloth.

---

# tl;dr recommendations

* If your rig can’t comfortably host 14B, start with **Unsloth Qwen3-8B GGUF** (UD Q5_K_M) and compare to **14B UD Q5_K_M**—same template, same ctx. ([Hugging Face][4])
* For serious coding, add **Qwen3-Coder-30B A3B (UD)** if RAM allows; it’s what multiple third-party runs (AMD/Cline) found to be the first “reliably agentic” local coder tier. ([Cline][5])
* For reasoning + coding stress, throw in **DeepSeek-V3.1 (UD 3-bit or 1-bit) thinking + no-thinking**; Unsloth shows strong **Polyglot** results even at 1–3 bit. Verify locally. ([Unsloth Docs][1])

---

# files to add

## `models.tsv`

(edit paths; `mode` toggles thinking for models that support it)

```
# tag<TAB>source<TAB>num_ctx<TAB>mode
qwen3-8b-ud-q5km      hf:unsloth/Qwen3-8B-GGUF/Qwen3-8B-Q5_K_M.gguf      8192   no_think
qwen3-14b-ud-q5km     hf:unsloth/Qwen3-14B-GGUF/Qwen3-14B-Q5_K_M.gguf    8192   no_think
qwen3-coder-30b-ud-q4 hf:unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF/...    8192   no_think
deepseek-v3.1-ud-3bit hf:unsloth/DeepSeek-V3.1-GGUF-v2/DeepSeek-...      8192   think
deepseek-v3.1-ud-1bit hf:unsloth/DeepSeek-V3.1-GGUF-v2/DeepSeek-...      8192   no_think
```

(see Unsloth model pages/collections for exact filenames; many UD quants exist, from IQ1 to Q5_K_M.) ([Hugging Face][6])

## `bench_unsloth.sh`

```bash
#!/usr/bin/env bash
set -euo pipefail

# -- knobs you can tweak ------------------------------------------------------
MODELS_TSV="${1:-models.tsv}"
RESULTS_DIR="${RESULTS_DIR:-$PWD/bench_results}"
AIDER_REPO="${AIDER_REPO:-https://github.com/Aider-AI/aider.git}"
OL_PREFIX="ud"                        # local Ollama namespace
CTX_FALLBACK=8192
mkdir -p "$RESULTS_DIR"

# Prefer KV quant + flash-attn to fit more layers on GPU
export OLLAMA_KV_CACHE_TYPE="${OLLAMA_KV_CACHE_TYPE:-q8_0}"
export OLLAMA_FLASH_ATTENTION="${OLLAMA_FLASH_ATTENTION:-1}"

need() { command -v "$1" >/dev/null || { echo "missing: $1"; exit 1; }; }
need ollama
need docker
need python3
need git

# -- helper: fetch a GGUF from HF if 'hf:' scheme is used ---------------------
resolve_source() {
  local src="$1"
  if [[ "$src" == hf:* ]]; then
    # require 'huggingface-cli' or curl; simplest is to ask user to pre-download
    echo ">> Please download: ${src#hf:}  (put the .gguf at ./models/)"
    local fname="$(basename "${src}")"
    echo "$PWD/models/$fname"
  else
    echo "$src"
  fi
}

# -- helper: create Modelfile with an explicit Qwen3-compatible template ------
# NOTE: best is letting Unsloth auto-generate the Modelfile during export
# (it preserves the *exact* chat template used in training). If you exported
# yourself with Unsloth, use *that* Modelfile. Otherwise we fall back to a
# conservative template and plain chat.  See: docs "Saving to Ollama".
# https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama
make_modelfile() {
  local gguf="$1" ctx="$2" mode="$3" out="$4"
  cat > "$out" <<EOF
FROM $gguf
PARAMETER num_ctx ${ctx}
# If your GGUF expects a specific chat template (eg jinja with <|im_start|>),
# prefer the Modelfile Unsloth generated. Wrong templates = gibberish.
# https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-gguf
TEMPLATE """{{ .System }}
{{ .Prompt }}"""
# You can pin decoding for reproducibility across runs:
# PARAMETER temperature 0.2
# PARAMETER top_p 0.95
EOF
  # Optional thinking/no-thinking priming on first user turn (Aider ignores system)
  if [[ "$mode" == "think" ]]; then
    echo '# thinking mode prompt priming handled by benchmark harness' >> "$out"
  fi
}

# -- build an Ollama variant from a GGUF --------------------------------------
build_variant() {
  local tag="$1" gguf="$2" ctx="${3:-$CTX_FALLBACK}" mode="$4"
  local modelfile="$(mktemp)"
  make_modelfile "$gguf" "$ctx" "$mode" "$modelfile"
  local full="local/${OL_PREFIX}-${tag}"
  echo ">> ollama create $full"
  ollama create "$full" -f "$modelfile"
  rm -f "$modelfile"
  echo "$full"
}

# -- Aider Polyglot (coding) --------------------------------------------------
ensure_polyglot() {
  if [ ! -d "$RESULTS_DIR/aider" ]; then
    git clone "$AIDER_REPO" "$RESULTS_DIR/aider"
    (cd "$RESULTS_DIR/aider" && ./benchmark/docker_build.sh)
  fi
}
run_polyglot() {
  local model_tag="$1" name="polyglot-${model_tag//\//-}-$(date +%Y%m%d-%H%M%S)"
  (cd "$RESULTS_DIR/aider" && ./benchmark/docker.sh)
  docker exec -i aider-benchmark bash -lc "
    set -e
    pip install -e .[dev]
    ./benchmark/benchmark.py ${name} \
      --model ollama_chat/${model_tag} \
      --edit-format whole \
      --threads 1 \
      --exercises-dir polyglot-benchmark"
  docker cp aider-benchmark:/workspace/aider/tmp.benchmarks "$RESULTS_DIR"
}

# -- BFCL (tool-use) - optional ------------------------------------------------
ensure_bfcl() {
  if [ ! -d "$RESULTS_DIR/.bfcl_venv" ]; then
    python3 -m venv "$RESULTS_DIR/.bfcl_venv"
    source "$RESULTS_DIR/.bfcl_venv/bin/activate"
    pip install --upgrade pip bfcl-eval
    deactivate
  fi
}
run_bfcl() {
  local model_tag="$1" out="$RESULTS_DIR/bfcl/${model_tag//\//-}"
  mkdir -p "$out"
  source "$RESULTS_DIR/.bfcl_venv/bin/activate"
  export BFCL_PROJECT_ROOT="$out"
  printf "LOCAL_SERVER_ENDPOINT=127.0.0.1\nLOCAL_SERVER_PORT=11434\n" > "$out/.env"
  # Most GGUF instruct models aren't FC-trained; expect low absolute scores.
  bfcl generate --model openai-prompt --test-category simple,parallel,multiple --skip-server-setup || true
  bfcl evaluate --model openai-prompt --test-category simple,parallel,multiple || true
  deactivate
}

# -- main ---------------------------------------------------------------------
ensure_polyglot
ensure_bfcl

while read -r line; do
  [[ -z "$line" || "$line" =~ ^# ]] && continue
  tag="$(echo "$line" | awk '{print $1}')"
  src="$(echo "$line" | awk '{print $2}')"
  ctx="$(echo "$line" | awk '{print $3}')"; ctx="${ctx:-$CTX_FALLBACK}"
  mode="$(echo "$line" | awk '{print $4}')"; mode="${mode:-no_think}"

  gguf_path="$(resolve_source "$src")"
  [ -f "$gguf_path" ] || { echo "Missing GGUF: $gguf_path"; continue; }

  full_tag="$(build_variant "$tag" "$gguf_path" "$ctx" "$mode")"

  # warm once
  curl -sS http://localhost:11434/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d "{\"model\":\"${full_tag}\",\"messages\":[{\"role\":\"user\",\"content\":\"ping\"}]}" >/dev/null || true

  run_polyglot "$full_tag"
  run_bfcl "$full_tag"
done < "$MODELS_TSV"

echo "== done =="
echo "Polyglot results under $RESULTS_DIR/tmp.benchmarks"
echo "BFCL results under $RESULTS_DIR/bfcl"
```

---

## why this works (and what to watch)

* **Polyglot/Aider** is what Unsloth uses to claim coding wins for Dynamic GGUFs; it’s an independent harness with pass-2 scoring, and it’s dockerized for safe execution of model-written code. You’re reproducing their setup, not trusting a tweet. ([Unsloth Docs][1])
* **Modelfile/template discipline matters.** Unsloth is explicit: mismatched chat templates cause gibberish/loops after export; their “Saving to Ollama” path auto-generates the right Modelfile. If you export yourself, use their exact template; my script uses a conservative fallback, but prefer Unsloth’s generated file when you have it. ([Unsloth Docs][3])
* **Export/quant from your own finetunes**: Unsloth exposes `model.save_pretrained_gguf(..., quantization_method="q4_k_m" | "q8_0" | "f16" | … )`. Use that if you’re training; it’s straight from their docs. ([Unsloth Docs][7])
* **Docker training** with Unsloth is one command; it ships Jupyter + notebooks. That’s your on-ramp for LoRA/DPO/GRPO finetunes before re-exporting to GGUF and re-running the bench. ([Unsloth Docs][2])
* **Model picks** you can actually download today: Unsloth Qwen3-8B/14B GGUFs, Qwen3 collections, DeepSeek-V3.1 UD sets, and Qwen3-Coder docs. ([Hugging Face][4])
* **Tool-use reality check**: BFCL measures structured function calling. Most GGUF instructs aren’t FC-tuned, so treat BFCL as a *sanity check trend*, not a leaderboard climb—unless you add a true FC model as a baseline. (Unsloth notes better tool use for DeepSeek-V3.1, but still, verify.) ([Hugging Face][8])

---

## quick “make it fit” notes

* Use UD **Q5_K_M** first; it’s typically the best quality/VRAM trade. If you’re RAM-bound, try lower-bit UD variants (IQ3/IQ2) and see how Polyglot pass-2 moves. (Unsloth’s Aider page shows startlingly strong low-bit runs; validate on your machine.) ([Unsloth Docs][1])
* Keep **KV cache quant** on (`OLLAMA_KV_CACHE_TYPE=q8_0`) and **Flash Attention** on. That usually lets you slide a bigger model in without cratering quality on coding tasks.
* On AMD stacks: Cline’s AMD sweep suggests **Qwen3-Coder-30B A3B (4-bit)** as the first “reliable agent” tier; if you’re trying agentic coding, that’s the proven local choice. ([Cline][5])

---

## optional: train + export with Unsloth Docker

```bash
# 1) start Unsloth container
docker run -d -e JUPYTER_PASSWORD="changeme" -p 8888:8888 -v $PWD/work:/workspace/work --gpus all unsloth/unsloth
# 2) open http://localhost:8888 and use the notebooks to finetune (LoRA/RL)
# 3) in your notebook/script, export GGUF:
#    model.save_pretrained_merged("merged_model", tokenizer, save_method="merged_16bit")
#    model.save_pretrained_gguf("ggufs", tokenizer, quantization_method="q5_k_m")
# 4) then re-run bench_unsloth.sh
```

(Those docker flags/steps are straight from Unsloth’s Docker guide; the GGUF save API is in their “Saving to GGUF”.) ([Unsloth Docs][2])

---

## extra evals (if you want breadth, not just coding)

* **MMLU (Hendrycks)** for general knowledge: pair the dataset with your preferred harness (lm-eval) to see if super-low-bit UD quants are “good enough” outside code. Unsloth references 5-shot MMLU in their quant write-ups; you can reproduce. ([GitHub][9])

---

if you want this wrapped as a **native ESM TS CLI** (flat package, AVA tests, functional style) that shells these steps and writes an Obsidian-friendly markdown/CSV of results, I’ll scaffold it next. for now, the bash above lets you pull UD GGUFs (or your own exports), build **Ollama** variants, and run **Aider Polyglot** + **BFCL** in one go—with the right citations to keep us honest.

[1]: https://docs.unsloth.ai/new/unsloth-dynamic-ggufs-on-aider-polyglot "Unsloth Dynamic GGUFs on Aider Polyglot | Unsloth Documentation"
[2]: https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker "How to Train LLMs with Unsloth & Docker | Unsloth Documentation"
[3]: https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-ollama "Saving to Ollama | Unsloth Documentation"
[4]: https://huggingface.co/unsloth/Qwen3-8B-GGUF?local-app=ollama "unsloth/Qwen3-8B-GGUF · Hugging Face"
[5]: https://cline.bot/blog/local-models-amd "Which local models actually work with Cline? AMD tested them all - Cline Blog"
[6]: https://huggingface.co/collections/unsloth/qwen3-680edabfb790c8c34a242f95?utm_source=chatgpt.com "Qwen3 - a unsloth Collection"
[7]: https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-gguf "Saving to GGUF | Unsloth Documentation"
[8]: https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF?utm_source=chatgpt.com "unsloth/DeepSeek-V3.1-GGUF"
[9]: https://github.com/hendrycks/test "GitHub - hendrycks/test: Measuring Massive Multitask Language Understanding | ICLR 2021"
