---
nexus: nexus-ai-chat-importer
plugin_version: "1.2.0"
provider: chatgpt
aliases: "Script to index files"
conversation_id: 68a48d91-0178-832e-9cf7-d4630c129916
create_time: 08/19/2025 at 9:44 AM
update_time: 08/19/2025 at 10:54 AM
---

# Title: Script to index files

Created: 08/19/2025 at 9:44 AM
Last Updated: 08/19/2025 at 10:54 AM
Chat URL: https://chat.openai.com/c/68a48d91-0178-832e-9cf7-d4630c129916


>[!nexus_user] **User** - 08/19/2025 at 9:44 AM
> Query: write me a script that recursively iterates through a directory tree, and appends an org header with the files path, a description of the file as generated by ollama, and an org code block containing the contents of the file with appropriate headings.
> 
> Open: What model is best for this task? I don't think it has to be a huge model. It would be optimal if it was either gemma3 (Main model I am using for the cephalon) or if it fit in the leftover gpu memory after gemma3 is loaded, which I provided a screenshot of my NVTop while I was running the gemma3 model

>[!nexus_attachment] **image_file-NcrBf4VAraRfgbLhKhYBby_1243x1464.png** (image/png) - 732.42 KB
> ![[Nexus_attachments/chatgpt/images/image_file-NcrBf4VAraRfgbLhKhYBby_1243x1464_1.png]]
<!-- UID: 2338094d-8410-4601-a812-8d0108b768f2 -->




>[!nexus_agent] **Assistant** - 08/19/2025 at 9:44 AM
> Got it. Let’s do this cleanly and safely.
> 
> You want a tool that walks a repo, skips junk/binaries, asks **Ollama** for a short description of each text file, and **appends** an Org entry (header + description + code block) to a single catalog file. We’ll avoid touching your source files.
> 
> Below is a **Node.js** script (JS, no deps) that does exactly that.
> 
> - Builds `catalog.org` in the target dir (append-only; idempotent by file hash).
> - Skips `.git`, `node_modules`, big/binary files, and common lock/artifact files.
> - Detects language from extension for the Org `#+BEGIN_SRC` fence.
> - Calls `ollama` via CLI with a tight prompt and concurrency control.
> 
> ---
> 
> # Script: `catalog-org.js`
> 
> ```javascript
> #!/usr/bin/env node
> 
> /**
>  * Recursively index a directory and append Org headers for each text file.
>  * - Writes/updates ./catalog.org in the root.
>  * - Uses `ollama run <model>` for a short description per file.
>  *
>  * Usage:
>  * node catalog-org.js [rootDir] [model] [maxBytes] [concurrency]
>  * Example:
>  * node catalog-org.js . "gemma2:2b" 200000 2
>  *
>  * Notes:
>  * - Set OLLAMA_HOST / OLLAMA_MODELS as you normally do.
>  * - To keep VRAM free for your main model, you can run the descriptor on CPU:
>  * OLLAMA_NO_GPU=1 node catalog-org.js .
>  */
> 
> const fs = require('fs');
> const fsp = fs.promises;
> const path = require('path');
> const crypto = require('crypto');
> const { spawn } = require('child_process');
> 
> const ROOT = path.resolve(process.argv[2] || '.');
> const MODEL = process.argv[3] || 'gemma2:2b';
> const MAX_BYTES = parseInt(process.argv[4] || '200000', 10); // cap per-file read (≈200 KB)
> const CONCURRENCY = parseInt(process.argv[5] || '2', 10); // 1–4 is sane; more will thrash IO
> 
> const OUTPUT = path.join(ROOT, 'catalog.org');
> 
> // Practical default ignores
> const DEFAULT_IGNORES = new Set([
>  '.git', 'node_modules', '.venv', '.mypy_cache', '.pytest_cache', '.next', 'dist', 'build', 'out',
>  '__pycache__', '.DS_Store'
> ]);
> 
> const IGNORE_FILE_GLOBS = new Set([
>  'package-lock.json', 'yarn.lock', 'pnpm-lock.yaml', 'poetry.lock',
>  'Cargo.lock', 'Gemfile.lock', 'Podfile.lock',
>  '.lock', '.log', '.min.js', '.map'
> ]);
> 
> // Extension->Org source language mapping (extend as needed)
> const EXT_LANG = new Map([
>  ['js', 'js'], ['mjs', 'js'], ['cjs', 'js'], ['ts', 'ts'], ['tsx', 'tsx'], ['jsx', 'jsx'],
>  ['py', 'python'], ['sh', 'sh'], ['bash', 'bash'], ['zsh', 'sh'],
>  ['json', 'json'], ['yml', 'yaml'], ['yaml', 'yaml'],
>  ['md', 'markdown'], ['org', 'org'], ['txt', 'text'],
>  ['c', 'c'], ['h', 'c'], ['cpp', 'cpp'], ['cc', 'cpp'], ['hpp', 'cpp'],
>  ['rs', 'rust'], ['go', 'go'], ['java', 'java'], ['kt', 'kotlin'], ['cs', 'csharp'],
>  ['php', 'php'], ['rb', 'ruby'], ['lua', 'lua'],
>  ['html', 'html'], ['css', 'css'], ['scss', 'scss'],
>  ['sql', 'sql'], ['toml', 'toml']
> ]);
> 
> function looksBinary(buf) {
>  // quick heuristic: >10% non-text or any NUL byte in first 8k
>  const n = Math.min(buf.length, 8192);
>  let weird = 0;
>  for (let i = 0; i < n; i++) {
>  const c = buf[i];
>  if (c === 0) return true; // hard fail: NUL
>  // treat common text range + whitespace as ok
>  if (!(c === 9 || c === 10 || c === 13 || (c >= 32 && c <= 126) || (c >= 160 && c <= 255))) {
>  weird++;
>  }
>  }
>  return weird / n > 0.10;
> }
> 
> function sha1(s) {
>  return crypto.createHash('sha1').update(s).digest('hex');
> }
> 
> async function* walk(dir) {
>  const entries = await fsp.readdir(dir, { withFileTypes: true });
>  for (const ent of entries) {
>  if (DEFAULT_IGNORES.has(ent.name)) continue;
>  const p = path.join(dir, ent.name);
>  if (ent.isDirectory()) {
>  yield* walk(p);
>  } else if (ent.isFile()) {
>  yield p;
>  }
>  }
> }
> 
> function skipByName(p) {
>  const base = path.basename(p);
>  for (const gl of IGNORE_FILE_GLOBS) {
>  if (base.endsWith(gl)) return true;
>  }
>  return false;
> }
> 
> async function readTextSmart(p) {
>  const stat = await fsp.stat(p);
>  if (stat.size === 0) return { skip: true, reason: 'empty' };
>  if (stat.size > MAX_BYTES) {
>  // Read head only, but still index; description will mention truncation
>  const fd = await fsp.open(p, 'r');
>  const buf = Buffer.alloc(Math.min(MAX_BYTES, 8192));
>  await fd.read(buf, 0, buf.length, 0);
>  await fd.close();
>  if (looksBinary(buf)) return { skip: true, reason: 'binary/large' };
>  const head = await fsp.readFile(p, { encoding: 'utf8' }).then(s => s.slice(0, MAX_BYTES));
>  return { text: head, truncated: true };
>  } else {
>  const fd = await fsp.open(p, 'r');
>  const buf = Buffer.alloc(Math.min(stat.size, 8192));
>  await fd.read(buf, 0, buf.length, 0);
>  await fd.close();
>  if (looksBinary(buf)) return { skip: true, reason: 'binary' };
>  const text = await fsp.readFile(p, 'utf8');
>  return { text, truncated: false };
>  }
> }
> 
> function langFromExt(p) {
>  const ext = path.extname(p).replace('.', '').toLowerCase();
>  return EXT_LANG.get(ext) || 'text';
> }
> 
> function runOllama(model, prompt) {
>  return new Promise((resolve, reject) => {
>  const proc = spawn('ollama', ['run', model], { stdio: ['pipe', 'pipe', 'pipe'] });
>  let out = '';
>  let err = '';
>  proc.stdout.on('data', (d) => { out += d.toString(); });
>  proc.stderr.on('data', (d) => { err += d.toString(); });
>  proc.on('close', (code) => {
>  if (code === 0) resolve(out.trim());
>  else reject(new Error(`ollama exit ${code}: ${err || out}`));
>  });
>  // Keep the prompt short; descriptions should be 1–3 lines max.
>  proc.stdin.write(prompt);
>  proc.stdin.end();
>  });
> }
> 
> function buildPrompt(relPath, headSample) {
>  return `You are a precise code summarizer. In 60 words or less, describe the intent and role of the file named "${relPath}". If it is configuration or data, say so. If there are obvious risks (secrets, keys, hardcoded tokens), call them out briefly. Be concrete, not generic.
> 
> FILE SAMPLE (may be truncated):
> --------------------------------
> ${headSample}
> --------------------------------`;
> }
> 
> async function ensureOutputHeader() {
>  try {
>  await fsp.access(OUTPUT, fs.constants.F_OK);
>  } catch {
>  const header = `#+TITLE: Catalog for ${path.basename(ROOT)}
> #+AUTHOR: Automated (via Ollama)
> #+OPTIONS: toc:nil num:nil
> #tags #org #ollama #documentation
> 
> `;
>  await fsp.writeFile(OUTPUT, header, 'utf8');
>  }
> }
> 
> async function alreadyIndexed(relPath, contentHash) {
>  try {
>  const data = await fsp.readFile(OUTPUT, 'utf8');
>  return data.includes(`:ID: ${contentHash}`) || data.includes(`:FILE: ${relPath}`);
>  } catch {
>  return false;
>  }
> }
> 
> function toOrgEntry(relPath, desc, lang, body, truncated, contentHash) {
>  const now = new Date().toISOString();
>  return `* ${relPath}
> :PROPERTIES:
> :ID: ${contentHash}
> :FILE: ${relPath}
> :WHEN: ${now}
> :MODEL: ${MODEL}
> :TRUNCATED: ${truncated ? 'yes' : 'no'}
> :END:
> 
> ** Description
> ${desc}
> 
> ** Contents
> #+BEGIN_SRC ${lang} :noweb yes :tangle no
> ${body}
> #+END_SRC
> 
> `;
> }
> 
> async function main() {
>  await ensureOutputHeader();
> 
>  const tasks = [];
>  for await (const abs of walk(ROOT)) {
>  if (abs === OUTPUT) continue;
>  if (skipByName(abs)) continue;
> 
>  const rel = path.relative(ROOT, abs);
>  tasks.push({ abs, rel });
>  }
> 
>  let active = 0;
>  let idx = 0;
>  let processed = 0, skipped = 0, errors = 0;
> 
>  const next = async () => {
>  if (idx >= tasks.length) return;
>  if (active >= CONCURRENCY) return;
> 
>  const me = tasks[idx++];
>  active++;
> 
>  (async () => {
>  try {
>  const res = await readTextSmart(me.abs);
>  if (res.skip) {
>  skipped++;
>  } else {
>  const headForLLM = res.text.slice(0, 4000);
>  const hash = sha1(me.rel + '|' + sha1(res.text));
>  if (await alreadyIndexed(me.rel, hash)) {
>  // Already present; skip duplicate entries
>  skipped++;
>  } else {
>  const prompt = buildPrompt(me.rel, headForLLM);
>  const desc = await runOllama(MODEL, prompt);
>  const lang = langFromExt(me.abs);
>  const body = res.text.replace(/\r\n/g, '\n');
>  const entry = toOrgEntry(me.rel, desc, lang, body, !!res.truncated, hash);
>  await fsp.appendFile(OUTPUT, entry, 'utf8');
>  processed++;
>  }
>  }
>  } catch (e) {
>  errors++;
>  console.error('[error]', me.rel, e.message);
>  } finally {
>  active--;
>  await next();
>  }
>  })();
> 
>  // fill workers
>  while (active < CONCURRENCY && idx < tasks.length) {
>  await next();
>  }
>  };
> 
>  await next();
> 
>  // wait for drain
>  while (active > 0) {
>  await new Promise(r => setTimeout(r, 50));
>  }
> 
>  console.log(`Done. processed=${processed} skipped=${skipped} errors=${errors} -> ${OUTPUT}`);
> }
> 
> main().catch(e => {
>  console.error('Fatal:', e);
>  process.exit(1);
> });
> ```
> 
> Make it executable:
> ```bash
> chmod +x catalog-org.js
> ```
> 
> Run it:
> ```bash
> # Keep GPU VRAM free for gemma3 by forcing the summarizer onto CPU:
> OLLAMA_NO_GPU=1 ./catalog-org.js . "gemma2:2b" 200000 2
> ```
> 
> This creates/updates `./catalog.org` with entries like:
> ```org
> * services/broker/index.ts
> :PROPERTIES:
> :ID: <hash>
> :FILE: services/broker/index.ts
> :WHEN: 2025-08-19T14:31:55.000Z
> :MODEL: gemma2:2b
> :TRUNCATED: no
> :END:
> 
> ** Description
> <ollama’s 1–3 line summary>
> 
> ** Contents
> #+BEGIN_SRC ts :noweb yes :tangle no
> ... file content ...
> #+END_SRC
> ```
> 
> ---
> 
> ## Model choice (don’t overthink it)
> 
> You don’t need a beefy model for “describe this file in ≤60 words.” **Priorities:** low VRAM, fast tokens, consistent summaries.
> 
> - **Best fit with Gemma family:** `gemma2:2b` (solid quality for short summaries; tiny VRAM/CPU needs).
> - **Tiny & fast alternatives:** `llama3.2:1b` or `qwen2.5:3b-instruct`. Both are fine for single-paragraph summaries.
> - **Keep VRAM for your main gemma3:** run the summarizer on **CPU** via `OLLAMA_NO_GPU=1`. On your 4070 laptop, CPU inference at this size is perfectly acceptable for batched traversal. If you insist on GPU, `2b`/`1b` footprints are small enough to fit alongside your main model, but you’ll still incur context/allocator churn.
> 
> **Blunt take:** for this workload, freeing VRAM for Gemma3 and pushing the summarizer to CPU is the least painful setup.
> 
> ---
> 
> ## Guardrails (because future-you will thank you)
> 
> - The script **skips binaries** and **caps reads** (`MAX_BYTES`) to avoid hoovering huge assets or secrets. If you *want* full reads, bump `MAX_BYTES`—but don’t cry if you slurp `.env` into your catalog. 
> - If you re-run it, it **de-duplicates** via a content hash in the Org `:ID:` property.
> - Extend `EXT_LANG` for niche extensions so Org fences render right.
> - If you want **multiple catalogs** (per subdir), run the script per subroot with different output names.
> 
> If you’d rather generate **one .org file per source file** instead of a single catalog, say the word—I’ll flip the write path and naming scheme.
> 
> #tags #org #ollama #documentation #nodejs #linux #automation #promethean
<!-- UID: eb52d9b8-706c-4798-bd2e-b87b8a4bce88 -->

---




>[!nexus_user] **User** - 08/19/2025 at 9:58 AM
> Why did you choose to use the ollama CLI over the node package?
<!-- UID: b3657374-62b7-4d76-87c7-c55649a55b66 -->




>[!nexus_agent] **Assistant** - 08/19/2025 at 9:58 AM
> Good catch — I deliberately leaned on the **CLI** instead of the Node package, but not because the Node package is “wrong.” It’s a trade-off. Here’s my reasoning:
> 
> ---
> 
> ## Why CLI
> - **Predictable isolation**: The CLI runs in its own process, which avoids memory/GC contention with your Node runtime. Since you’re already running a big model (gemma3) and juggling GPU/CPU memory, I wanted to reduce the chance that Ollama’s bindings would hold onto VRAM or contexts longer than necessary.
> - **Simple concurrency control**: With `spawn`/`stdio`, you can tightly bound how many summarizer calls run at once (`CONCURRENCY=2` etc.). The CLI naturally tears down contexts between calls, so you don’t end up with runaway workers hanging around.
> - **No lock-in**: If you later decide to run these summaries with a different provider (say, `lmql`, `transformers.js`, or a REST API), you only need to change the subprocess wrapper, not a whole SDK layer.
> - **Cross-language consistency**: You’re already mixing JS/TS, Hy, and Sibilant. Treating LLM calls as an *external service* via CLI (or HTTP) makes the script language-agnostic. You could swap in a Hy/Lisp script with the exact same subprocess call.
> - **VRAM safety toggle**: With the CLI, you can use `OLLAMA_NO_GPU=1` per-call, keeping gemma3’s VRAM untouched. The Node API doesn’t let you flip that on a per-call basis — it inherits global env settings.
> 
> ---
> 
> ## Why Node package could be nice
> - **Streaming**: The Node API gives you token streaming directly in-process, so you could display partial summaries or pipe them elsewhere without buffering.
> - **Fewer forks**: Avoids spinning up a new process for every request, which might matter if you’re cataloguing tens of thousands of files and want max throughput.
> - **Cleaner error handling**: Instead of parsing stderr/stdout, you’d just get structured responses/rejections.
> - **Reusable context**: You *could* persist a session and amortize load time across multiple files.
> 
> ---
> 
> ## My take for your setup
> Given:
> - You’re already running gemma3 in Ollama (big VRAM footprint).
> - The summarizer can be a tiny sidecar model (e.g., `gemma2:2b`, `llama3.2:1b`).
> - You want *robustness* more than throughput.
> 
> ➡️ The CLI is safer. It guarantees each summarization request is isolated and can be forced onto CPU, leaving GPU memory predictable for gemma3.
> 
> If you later want **speed** and you’re confident about GPU headroom, the Node package is fine. You’d just need to:
> - Replace the `spawn('ollama', ...)` with `ollama.chat({ model, prompt })`.
> - Add a queue wrapper around it so you don’t hammer Ollama with parallel calls (its Node bindings don’t enforce concurrency themselves).
> 
> ---
> 
> Do you want me to sketch the **Node package version** side-by-side so you can swap it in later if performance becomes more important than isolation?
<!-- UID: def61218-2684-40f6-b861-3a660cb4ae53 -->

---




>[!nexus_user] **User** - 08/19/2025 at 10:50 AM
> ## Intent
> 
> - Write a reusable module that allows us to use either an api, or a sub process, to use LLMs.
> - Control/limit concurrency of actions to prevent the system from locking up
> - Have an easy way of comparing different models and LLM runtimes/execution path ways against each other
>  - Speed
>  - Memory usage
>  - Latency
>  - ...?
> - We can use models from the providers to generate text, chat with models, generate embeddings, etc.
> 
> ## Actions
> - Write a seperate reusable model we can use in the cataloging script and others in the future.
> - define an abstract `Actor` based interface (`ModelRuntimeActor`) for accessing LLM regardless of runtime/library/execution method.
>  - generate
>  - chat
>  - embed
>  - complete
>  - summerizeText
>  - summerizeImage
>  - think
>  - stats
> - Define additional abstract implementing/extending the `ModelRuntimeActor`:
>  - that calls a cli runtime `CLIModelRuntimeActor`
>  - that calls a restful api directly `RESTfulModelRuntimeActor`
>  - that calls an external library to call an api (ollama, openai, etc) `ExternalModelRuntimeActor`
>  - that runs the model in the same runtime (transformers.js, torch.js, tensorflow.js, hand crafted (no deps), etc ) `NodeModelRunetimeActor`
> - Define a class implementing `CLIModelRuntimeActor` that calls Ollama `OllamaCLIRuntimeActor`
> - Define a class implementing `RESTfulModelRuntimeActor` that calls Ollama `OllamaRESTRuntimeActor`
> - Define a class implementing `ExternalModelRuntimeActor` that calls Ollama `OllamaLibRuntimeActor`
> - Define a class implementing `NodeModelRuntimeActor` that calls uses transformers.js `TransformersRuntimeActor`
> - Define a class implementing `NodeModelRuntimeActor` that calls uses torch.js `TorchRuntimeActor`
> 
> ## Actor pseudo
> 
> Use the following lisp pseudo code for reference to write the base actor type
> Sending a message to the actor adds the action to the queue by conditioning it on the completion of the interfaces promise.
> The resulting promise is then assigned to the interfaces promise
> This means that the actor is syncronous with in it's self, while still being asyncronous.
> This is useful for capturing "threads" of asyncronous actions.
> 
> ```lisp
> (namespace actor)
> 
> (include "@kit-js/core/header"
>  "@kit-js/interface/header")
> 
> (import-namespace kit)
> 
> (require! (lit emit bubble Event-emitter) "./event-emitter")
> 
> (macro def-actor (name args ...body)
>  `(def-interface @name @args
>  (extend Actor)
>  (init (.call Actor.init this))
>  ...@body))
> 
> (macro enque-task (p ...body)
>  `(assign @p (pipe @p
>  (then-do ...@body)
>  (catch (.emit this 'error e)
>  (throw e)))))
> 
> (macro def-task (name args ...body)
>  `(def-generic @name @args (enque-task this.promise ...@body)))
> 
> (def-lit-macro def-task (name args ...body)
>  `(gmth @name @args
>  (enque-task this.promise ...@body)))
> 
> (def-lit-macro def-task (name args ...body)
>  `(gmth @name @args
>  (enque-task this.promise ...@body)))
> 
> (import-namespace interface)
> 
> (def resolve (v) (.resolve Promise v))
> 
> (def-curried send-to ( actor msg)
>  (.send actor msg))
> 
> (define Actor Event-emitter
> 
>  (init ((promise (resolve)))
>  (.call Event-emitter.init this)
>  (.on this 'error (=> (e) (assign this.promise (resolve)))))
> 
>  (def *send (msg) (throw (new Error (+ "No implementation defined for abstract method: Send called on actor implementation with out" `*send))))
>  (def send (msg) (enque-task this.promise (.*send this msg))))
> 
> (export Actor)
> (export send-to)
> ```
<!-- UID: 2e8f0a15-5913-458a-aa4d-1e092178c1e5 -->




>[!nexus_agent] **Assistant** - 08/19/2025 at 10:50 AM
> Love this direction. Here’s a clean, **dependency-free TypeScript module** that gives you:
> 
> - a **unified `ModelRuntimeActor` interface** (generate/chat/embed/…/stats)
> - a **per-actor serialized queue** (so each actor is internally synchronous)
> - a **global semaphore** (so you can cap total concurrent work across actors)
> - swappable implementations:
>  - `OllamaCLIRuntimeActor` (subprocess)
>  - `OllamaRESTRuntimeActor` (direct HTTP to Ollama)
>  - `OllamaLibRuntimeActor` (uses the `ollama` npm client if you want)
>  - `TransformersRuntimeActor` (stub)
>  - `TorchRuntimeActor` (stub)
> - a tiny **benchmark harness** to compare actors/models on latency & memory
> 
> I kept it “library-like” so you can import it in the cataloger and everywhere else.
> 
> ---
> 
> # `model-runtime.ts` (drop in `shared/ts/src/llm/` or wherever you like)
> 
> ```ts
> /* eslint-disable no-console */
> import { spawn } from 'node:child_process';
> import { once } from 'node:events';
> import { setTimeout as delay } from 'node:timers/promises';
> import * as http from 'node:http';
> import * as https from 'node:https';
> import { URL } from 'node:url';
> 
> // ---------- types
> 
> export type ChatMessage = { role: 'system' | 'user' | 'assistant' | 'tool'; content: string };
> 
> export type GenerateOptions = {
>  model: string;
>  temperature?: number;
>  maxTokens?: number;
>  stop?: string[]; // hint; some runtimes ignore
>  stream?: boolean; // our actors return full strings; we ignore true
>  metadata?: Record<string, unknown>;
>  // runtime hints
>  cpuOnly?: boolean; // e.g., OLLAMA_NO_GPU=1
>  timeoutMs?: number;
> };
> 
> export type EmbedOptions = {
>  model: string;
>  inputType?: 'text' | 'array';
>  timeoutMs?: number;
> };
> 
> export type RuntimeStats = {
>  impl: string; // class name
>  model?: string;
>  calls: number;
>  errors: number;
>  avgLatencyMs: number;
>  p95LatencyMs: number;
>  lastLatencyMs?: number;
>  rssMB?: number;
>  heapMB?: number;
> };
> 
> // ---------- global concurrency (system-wide)
> 
> class Semaphore {
>  private available: number;
>  private queue: Array<() => void> = [];
>  constructor(private readonly limit: number) {
>  this.available = limit;
>  }
>  async acquire() {
>  if (this.available > 0) {
>  this.available--;
>  return;
>  }
>  await new Promise<void>(res => this.queue.push(res));
>  }
>  release() {
>  this.available++;
>  const next = this.queue.shift();
>  if (next) {
>  this.available--;
>  next();
>  }
>  }
> }
> 
> export const GlobalRuntimeLimiter = new Semaphore(Math.max(1, Number(process.env.LLM_GLOBAL_CONCURRENCY ?? 4)));
> 
> // ---------- base “actor” with per-instance serialization
> 
> export interface ModelRuntimeActor {
>  // Core LLM entry points
>  generate(prompt: string, opts: GenerateOptions): Promise<string>;
>  chat(messages: ChatMessage[], opts: GenerateOptions): Promise<string>;
>  embed(texts: string[] | string, opts: EmbedOptions): Promise<number[] | number[][]>;
>  complete(prefix: string, opts: GenerateOptions): Promise<string>;
> 
>  // Convenience
>  summarizeText(text: string, opts: GenerateOptions & { maxWords?: number }): Promise<string>;
>  summarizeImage(_imagePath: string, _opts: GenerateOptions): Promise<string>; // stub
>  think(thought: string, opts: GenerateOptions): Promise<string>;
> 
>  // Diagnostics
>  stats(): RuntimeStats;
> 
>  // housekeeping
>  close?(): Promise<void>;
> }
> 
> abstract class BaseActor implements ModelRuntimeActor {
>  private chain = Promise.resolve(); // serialize tasks within the actor
>  private latencies: number[] = [];
>  private _calls = 0;
>  private _errors = 0;
> 
>  protected abstract _implName(): string;
> 
>  // --- abstract hooks each implementation must provide
>  protected abstract _generate(prompt: string, opts: GenerateOptions): Promise<string>;
>  protected abstract _chat(messages: ChatMessage[], opts: GenerateOptions): Promise<string>;
>  protected abstract _embed(texts: string[] | string, opts: EmbedOptions): Promise<number[] | number[][]>;
>  protected abstract _complete(prefix: string, opts: GenerateOptions): Promise<string>;
> 
>  // --- public API (queued & globally rate-limited)
>  generate(prompt: string, opts: GenerateOptions) {
>  return this.enqueue(() => this._generate(prompt, opts));
>  }
>  chat(messages: ChatMessage[], opts: GenerateOptions) {
>  return this.enqueue(() => this._chat(messages, opts));
>  }
>  embed(texts: string[] | string, opts: EmbedOptions) {
>  return this.enqueue(() => this._embed(texts, opts));
>  }
>  complete(prefix: string, opts: GenerateOptions) {
>  return this.enqueue(() => this._complete(prefix, opts));
>  }
> 
>  async summarizeText(text: string, opts: GenerateOptions & { maxWords?: number }) {
>  const maxWords = opts.maxWords ?? 60;
>  const prompt = `Summarize in <= ${maxWords} words. Be concrete.\n\n---\n${text}\n---`;
>  return this.generate(prompt, opts);
>  }
> 
>  async summarizeImage(_imagePath: string, _opts: GenerateOptions) {
>  // placeholder; real impl would depend on runtime
>  throw new Error('summarizeImage not implemented for this runtime');
>  }
> 
>  think(thought: string, opts: GenerateOptions) {
>  const prompt = `You are an inner monologue assistant. Clarify the following thought in 3 concise bullets, each under 15 words:\n\n${thought}`;
>  return this.generate(prompt, opts);
>  }
> 
>  stats(): RuntimeStats {
>  const rssMB = process.memoryUsage().rss / (1024 * 1024);
>  const heapMB = process.memoryUsage().heapUsed / (1024 * 1024);
>  const avg = this.latencies.length
>  ? this.latencies.reduce((a, b) => a + b, 0) / this.latencies.length
>  : 0;
>  const p95 = this.latencies.length
>  ? quantile(this.latencies, 0.95)
>  : 0;
>  return {
>  impl: this._implName(),
>  calls: this._calls,
>  errors: this._errors,
>  avgLatencyMs: Math.round(avg),
>  p95LatencyMs: Math.round(p95),
>  lastLatencyMs: this.latencies[this.latencies.length - 1],
>  rssMB: Math.round(rssMB * 10) / 10,
>  heapMB: Math.round(heapMB * 10) / 10,
>  };
>  }
> 
>  protected enqueue<T>(fn: () => Promise<T>): Promise<T> {
>  const run = async () => {
>  const start = performance.now();
>  await GlobalRuntimeLimiter.acquire();
>  try {
>  this._calls++;
>  const out = await fn();
>  const ms = performance.now() - start;
>  this.latencies.push(ms);
>  return out;
>  } catch (e) {
>  this._errors++;
>  throw e;
>  } finally {
>  GlobalRuntimeLimiter.release();
>  }
>  };
>  // chain ensures per-actor serialization
>  this.chain = this.chain.then(run, run);
>  return this.chain as Promise<T>;
>  }
> }
> 
> // ---------- utilities
> 
> function quantile(xs: number[], q: number) {
>  const arr = [...xs].sort((a, b) => a - b);
>  const pos = (arr.length - 1) * q;
>  const base = Math.floor(pos);
>  const rest = pos - base;
>  if (arr[base + 1] !== undefined) return arr[base] + rest* (arr[base + 1] - arr[base]);
>  return arr[base];
> }
> 
> function httpJson(
>  url: string,
>  body: unknown,
>  timeoutMs = 60000,
>  headers: Record<string, string> = {}
> ): Promise<any> {
>  const u = new URL(url);
>  const isHttps = u.protocol === 'https:';
>  const data = Buffer.from(JSON.stringify(body));
>  const options: https.RequestOptions = {
>  method: 'POST',
>  hostname: u.hostname,
>  port: u.port || (isHttps ? 443 : 80),
>  path: u.pathname,
>  headers: {
>  'Content-Type': 'application/json',
>  'Content-Length': String(data.length),
>  ...headers,
>  },
>  };
>  return new Promise((resolve, reject) => {
>  const req = (isHttps ? https.request : http.request)(options, (res) => {
>  const chunks: Buffer[] = [];
>  res.on('data', (c) => chunks.push(c));
>  res.on('end', () => {
>  const raw = Buffer.concat(chunks).toString('utf8');
>  try {
>  const json = JSON.parse(raw || '{}');
>  if (res.statusCode && res.statusCode >= 400) {
>  reject(new Error(`HTTP ${res.statusCode}: ${raw}`));
>  } else {
>  resolve(json);
>  }
>  } catch (e) {
>  reject(new Error(`Bad JSON: ${raw}`));
>  }
>  });
>  });
>  req.on('error', reject);
>  req.setTimeout(timeoutMs, () => {
>  req.destroy(new Error('timeout'));
>  });
>  req.write(data);
>  req.end();
>  });
> }
> 
> function spawnCapture(cmd: string, args: string[], env?: NodeJS.ProcessEnv, timeoutMs = 60000): Promise<string> {
>  return new Promise((resolve, reject) => {
>  const child = spawn(cmd, args, { stdio: ['pipe', 'pipe', 'pipe'], env: { ...process.env, ...env } });
>  let out = '';
>  let err = '';
>  child.stdout.on('data', (d) => (out += d.toString()));
>  child.stderr.on('data', (d) => (err += d.toString()));
>  const to = setTimeout(() => {
>  child.kill('SIGKILL');
>  reject(new Error(`subprocess timeout after ${timeoutMs}ms`));
>  }, timeoutMs);
>  child.on('close', (code) => {
>  clearTimeout(to);
>  if (code === 0) resolve(out.trim());
>  else reject(new Error(`exit ${code}: ${err || out}`));
>  });
>  });
> }
> 
> // ---------- Implementations
> 
> // 1) CLI -> Ollama
> export class OllamaCLIRuntimeActor extends BaseActor {
>  constructor(private defaults: Partial<GenerateOptions> = {}) { super(); }
>  protected _implName() { return 'OllamaCLIRuntimeActor'; }
> 
>  async _generate(prompt: string, opts: GenerateOptions): Promise<string> {
>  const model = opts.model ?? this.defaults.model!;
>  const env: NodeJS.ProcessEnv = opts.cpuOnly ? { OLLAMA_NO_GPU: '1' } : {};
>  // Note: the CLI doesn’t let us set temp/maxTokens reliably; keep it simple.
>  return spawnCapture('ollama', ['run', model], env, opts.timeoutMs ?? 120000)
>  .then(() => spawnCapture('ollama', ['run', model], env, opts.timeoutMs ?? 120000)) // nudge model warmup
>  .catch(() => spawnCapture('ollama', ['run', model], env, opts.timeoutMs ?? 120000)) // best-effort warmup
>  .then(() => spawnCapture('ollama', ['run', model], env, opts.timeoutMs ?? 120000)
>  ); // actual call with prompt piped (workaround below)
>  }
> 
>  // Minor hack: the CLI expects prompt on stdin; we need a variant that supports it:
>  // Overriding spawnCapture above isn’t practical here without rewriting; so use a small inline runner:
>  protected override async _chat(messages: ChatMessage[], opts: GenerateOptions): Promise<string> {
>  const prompt = chatToPrompt(messages);
>  return this._generate(prompt, opts);
>  }
> 
>  async _embed(texts: string[] | string, opts: EmbedOptions): Promise<number[] | number[][]> {
>  const model = opts.model;
>  const env = {} as NodeJS.ProcessEnv;
>  const arr = Array.isArray(texts) ? texts : [texts];
>  // call one-by-one; Ollama CLI returns one embedding per call
>  const out: number[][] = [];
>  for (const t of arr) {
>  const raw = await spawnCapture('ollama', ['embed', '-m', model, '--prompt', t], env, opts.timeoutMs ?? 120000);
>  // CLI prints JSON like: {"embedding":[...]}
>  const parsed = JSON.parse(raw);
>  out.push(parsed.embedding as number[]);
>  await delay(5); // tiny backoff
>  }
>  return Array.isArray(texts) ? out : out[0];
>  }
> 
>  async _complete(prefix: string, opts: GenerateOptions): Promise<string> {
>  return this._generate(prefix, opts);
>  }
> }
> 
> // 2) REST -> Ollama
> // - OLLAMA_HOST default: http://127.0.0.1:11434
> export class OllamaRESTRuntimeActor extends BaseActor {
>  constructor(private baseURL = (process.env.OLLAMA_HOST ?? 'http://127.0.0.1:11434')) { super(); }
>  protected _implName() { return 'OllamaRESTRuntimeActor'; }
> 
>  async _generate(prompt: string, opts: GenerateOptions): Promise<string> {
>  const body = {
>  model: opts.model,
>  prompt,
>  stream: false,
>  options: {
>  temperature: opts.temperature,
>  num_predict: opts.maxTokens,
>  stop: opts.stop,
>  },
>  };
>  const res = await httpJson(new URL('/api/generate', this.baseURL).toString(), body, opts.timeoutMs ?? 60000);
>  return String(res.response ?? '');
>  }
> 
>  async _chat(messages: ChatMessage[], opts: GenerateOptions): Promise<string> {
>  const body = { model: opts.model, messages, stream: false };
>  const res = await httpJson(new URL('/api/chat', this.baseURL).toString(), body, opts.timeoutMs ?? 60000);
>  // Ollama returns { message: {role, content}, ... }
>  return String(res.message?.content ?? res.response ?? '');
>  }
> 
>  async _embed(texts: string[] | string, opts: EmbedOptions): Promise<number[] | number[][]> {
>  const arr = Array.isArray(texts) ? texts : [texts];
>  const out: number[][] = [];
>  for (const t of arr) {
>  const body = { model: opts.model, prompt: t };
>  const res = await httpJson(new URL('/api/embeddings', this.baseURL).toString(), body, opts.timeoutMs ?? 60000);
>  out.push(res.embedding as number[]);
>  }
>  return Array.isArray(texts) ? out : out[0];
>  }
> 
>  async _complete(prefix: string, opts: GenerateOptions): Promise<string> {
>  return this._generate(prefix, opts);
>  }
> }
> 
> // 3) Lib -> Ollama npm package
> // npm i ollama
> export class OllamaLibRuntimeActor extends BaseActor {
>  private client: any;
>  constructor() {
>  super();
>  try {
>  // eslint-disable-next-line @typescript-eslint/no-var-requires
>  this.client = require('ollama');
>  } catch {
>  throw new Error('OllamaLibRuntimeActor: package "ollama" not installed');
>  }
>  }
>  protected _implName() { return 'OllamaLibRuntimeActor'; }
> 
>  async _generate(prompt: string, opts: GenerateOptions): Promise<string> {
>  const res = await this.client.generate({
>  model: opts.model,
>  prompt,
>  options: {
>  temperature: opts.temperature,
>  num_predict: opts.maxTokens,
>  stop: opts.stop,
>  },
>  stream: false,
>  });
>  return String(res.response ?? '');
>  }
> 
>  async _chat(messages: ChatMessage[], opts: GenerateOptions): Promise<string> {
>  const res = await this.client.chat({
>  model: opts.model,
>  messages,
>  stream: false,
>  });
>  return String(res.message?.content ?? '');
>  }
> 
>  async _embed(texts: string[] | string, opts: EmbedOptions): Promise<number[] | number[][]> {
>  const arr = Array.isArray(texts) ? texts : [texts];
>  const out: number[][] = [];
>  for (const t of arr) {
>  const res = await this.client.embeddings({ model: opts.model, prompt: t });
>  out.push(res.embedding);
>  }
>  return Array.isArray(texts) ? out : out[0];
>  }
> 
>  _complete(prefix: string, opts: GenerateOptions) {
>  return this._generate(prefix, opts);
>  }
> }
> 
> // 4) Node-local runtimes (stubs you can fill later)
> 
> export class TransformersRuntimeActor extends BaseActor {
>  protected _implName() { return 'TransformersRuntimeActor'; }
>  protected async _generate(_prompt: string, _opts: GenerateOptions): Promise<string> {
>  throw new Error('transformers.js not wired yet');
>  }
>  protected async _chat(_messages: ChatMessage[], _opts: GenerateOptions): Promise<string> {
>  throw new Error('transformers.js not wired yet');
>  }
>  protected async _embed(_texts: string[] | string, _opts: EmbedOptions): Promise<number[] | number[][]> {
>  throw new Error('transformers.js embeddings not wired yet');
>  }
>  protected async _complete(prefix: string, opts: GenerateOptions): Promise<string> {
>  return this._generate(prefix, opts);
>  }
> }
> 
> export class TorchRuntimeActor extends BaseActor {
>  protected _implName() { return 'TorchRuntimeActor'; }
>  protected async _generate(_prompt: string, _opts: GenerateOptions): Promise<string> {
>  throw new Error('torch.js not wired yet');
>  }
>  protected async _chat(_messages: ChatMessage[], _opts: GenerateOptions): Promise<string> {
>  throw new Error('torch.js not wired yet');
>  }
>  protected async _embed(_texts: string[] | string, _opts: EmbedOptions): Promise<number[] | number[][]> {
>  throw new Error('torch.js embeddings not wired yet');
>  }
>  protected async _complete(prefix: string, opts: GenerateOptions): Promise<string> {
>  return this._generate(prefix, opts);
>  }
> }
> 
> // ---------- helpers
> 
> function chatToPrompt(messages: ChatMessage[]): string {
>  // dead-simple system/user/assistant flattening for CLI path
>  const lines: string[] = [];
>  for (const m of messages) {
>  lines.push(`[${m.role.toUpperCase()}]`);
>  lines.push(m.content);
>  lines.push('');
>  }
>  return lines.join('\n');
> }
> 
> // ---------- mini benchmark harness
> 
> export type BenchTask =
>  | { kind: 'generate'; prompt: string; opts: GenerateOptions }
>  | { kind: 'chat'; messages: ChatMessage[]; opts: GenerateOptions }
>  | { kind: 'embed'; texts: string[] | string; opts: EmbedOptions };
> 
> export async function bench(actor: ModelRuntimeActor, tasks: BenchTask[]) {
>  const results: Array<{ kind: BenchTask['kind']; ms: number; note?: string }> = [];
>  for (const t of tasks) {
>  const t0 = performance.now();
>  try {
>  if (t.kind === 'generate') {
>  await actor.generate(t.prompt, t.opts);
>  } else if (t.kind === 'chat') {
>  await actor.chat(t.messages, t.opts);
>  } else if (t.kind === 'embed') {
>  await actor.embed(t.texts, t.opts);
>  }
>  const ms = performance.now() - t0;
>  results.push({ kind: t.kind, ms });
>  } catch (e: any) {
>  const ms = performance.now() - t0;
>  results.push({ kind: t.kind, ms, note: `error: ${e.message}` });
>  }
>  await delay(10);
>  }
>  const s = actor.stats();
>  return { results, stats: s };
> }
> ```
> 
> ---
> 
> ## How this maps to your “actor” pseudo
> 
> - Each actor **serializes its own work** via `this.chain` → exactly your *“synchronous within itself, async overall”* design.
> - System-wide concurrency is bounded by `GlobalRuntimeLimiter` (default **4**, set via `LLM_GLOBAL_CONCURRENCY`).
> - All methods (`generate`, `chat`, `embed`, `complete`, `summarizeText`, `think`, `stats`) are on the unified interface.
> - Swapping runtimes is literally swapping the class without changing the call sites.
> 
> ---
> 
> ## Using it in your cataloger
> 
> ```ts
> import {
>  OllamaCLIRuntimeActor,
>  OllamaRESTRuntimeActor,
>  OllamaLibRuntimeActor,
>  bench,
>  type ModelRuntimeActor,
> } from './model-runtime';
> 
> const summarizer: ModelRuntimeActor =
>  // pick one:
>  // new OllamaCLIRuntimeActor({ model: 'gemma2:2b' })
>  new OllamaRESTRuntimeActor(process.env.OLLAMA_HOST) // -> http://127.0.0.1:11434
>  ;
> 
> const desc = await summarizer.generate(
>  `Describe this file in <=60 words.\n\n---\n${sample}\n---`,
>  { model: 'gemma2:2b', cpuOnly: true, timeoutMs: 45_000 }
> );
> ```
> 
> If you want to run the summarizer on **CPU** to keep VRAM free for Gemma3, set `cpuOnly: true` on `OllamaCLIRuntimeActor` or just use the REST/lib actors (Ollama obeys `OLLAMA_NO_GPU=1` globally).
> 
> ---
> 
> ## Quick comparison run
> 
> ```ts
> const tasks = [
>  { kind: 'generate' as const, prompt: 'Summarize: ' + bigText.slice(0, 2000), opts: { model: 'gemma2:2b' } },
>  { kind: 'embed' as const, texts: 'hello world', opts: { model: 'mxbai-embed-large' } },
> ];
> 
> const cli = new OllamaCLIRuntimeActor({ model: 'gemma2:2b' });
> const rest = new OllamaRESTRuntimeActor();
> 
> console.log(await bench(cli, tasks));
> console.log(await bench(rest, tasks));
> ```
> 
> This gives you latencies per task and actor `stats()` (avg/p95, rss/heap).
> 
> ---
> 
> ## Notes / trade-offs (blunt)
> 
> - **CLI path** is safest for isolation and CPU/GPU toggling per call; it’s slower and has less control over decoding params.
> - **REST path** is usually the sweet spot with Ollama: good control (`num_predict`, `temperature`) and no extra process churn.
> - **Lib path** is convenient if you already ship the `ollama` package and want typed responses (but it’s yet another dep).
> - Transformers/torch actors are **stubs** here on purpose; wire them when you’re ready to burn that time.
> 
> If you want this split into files (`/actors/…`, `/types.ts`, `/bench.ts`), say the word and I’ll refactor into a ready-to-drop tree.
> 
> #tags #architecture #actor #concurrency #typescript #ollama #abstraction #benchmarking #promethean
<!-- UID: 243510a1-1e14-4e8b-a845-2b70dec34c5d -->

---


