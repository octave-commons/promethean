{"id":"","title":"Create piper MCP tool interface","status":"","priority":"","owner":"","labels":[],"created":"","path":"docs/agile/tasks/Create piper MCP tool interface.md","content":""}
{"id":"","title":"Stabilize monitoring token bucket tests","status":"","priority":"p3","owner":"err","labels":["board:auto","lang:ts"],"created":"","path":"docs/agile/tasks/TASK-2025-09-27-monitoring-test.md","content":"<hr class=\"__chatgpt_plugin\">\n\n<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n### role::assistant\n\nHere's a suggested revision of your context section:\n\n## Context\n### Changes and Updates\n- **What changed?**: Received a request to fix `@promethean/monitoring:test` by eliminating flakiness from wall-clock sleeps.\n- **Where?**: `packages/monitoring`.\n- **Why now?**: Package tests can intermittently fail when real time drifts.\n\n## Inputs / Artifacts\n- (changelog.d/2025.09.27.19.45.04.md)\n\n## Definition of Done\n- [x] `pnpm --filter @promethean/monitoring test` passes deterministically.\n- [x] ESLint clean on touched files.\n- [x] Clock injection wired through the package exports.\n\n## Plan\n1. Inject a configurable clock into `TokenBucket` and store state immutably.\n2. Update token bucket tests to advance a synthetic clock instead of waiting.\n3. Run package tests and lint the touched files.\n\n## Relevant Resources\n- `packages/monitoring/src/limiter.ts`\n- `packages/monitoring/tests/limiter.test.js`\n- `packages/monitoring/src/index.ts`\n\n### Smart Connections Configuration\n```smart-connections\n{\n  \"render_markdown\": true,\n  \"show_full_path\": false,\n  \"exclude_blocks_from_source_connections\": false,\n  \"exclude_frontmatter_blocks\": true,\n  \"expanded_view\": false,\n  \"results_limit\": \"20\",\n  \"exclude_inlinks\": false,\n  \"exclude_outlinks\": false\n}\n```\n\n### Smart ChatGPT Configuration\n```smart-chatgpt\n```\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\nTASK-2025-09-27-monitoring-test\n\n<% tp.app.commands.executeCommandById(\"chatgpt-md:call-chatgpt-api\") %>\n"}
{"id":"","title":"Align fetch_ovm_models entrypoint detection","status":"","priority":"p3","owner":"gpt-5-codex","labels":["board:auto","lang:ts"],"created":"","path":"docs/agile/tasks/align-fetch-ovm-entrypoint.md","content":"<hr class=\"__chatgpt_plugin\">\n\n<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n### role::assistant\n\nHere's a suggested revision of your context section:\n\n## Context\n### Changes and Updates\n- **What changed?**: Script should detect entry invocation using pathToFileURL comparisons.\n- **Where?**: `scripts/fetch_ovm_models.mjs` and supporting tests/logs.\n- **Why now?**: Relative path invocations currently fail to print model names, breaking developer workflow.\n\n## Inputs / Artifacts\n- (config/ovm.json)\n- (scripts/fetch_ovm_models.mjs)\n- (tests/scripts/fetch_ovm_models.test.js)\n\n## Definition of Done\n- [ ] fetch_ovm_models prints model names when run via relative path.\n- [ ] Automated test covering script remains green.\n- [ ] PR merged: (link TBD summarizing CLI guard fix.)\n\n## Plan\n1. Update CLI guard to use pathToFileURL with resolved argv path.\n2. Smoke test script via `node scripts/fetch_ovm_models.mjs`.\n3. Run relevant AVA test suite.\n\n## Relevant Resources\n\nYou might find [this](link) useful while working on this task.\n\n### Smart Connections Configuration\n```smart-connections\n{\n  \"render_markdown\": true,\n  \"show_full_path\": false,\n  \"exclude_blocks_from_source_connections\": false,\n  \"exclude_frontmatter_blocks\": true,\n  \"expanded_view\": false,\n  \"results_limit\": \"20\",\n  \"exclude_inlinks\": false,\n  \"exclude_outlinks\": false\n}\n```\n\n### Smart ChatGPT Configuration\n```smart-chatgpt\n```\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\n<% tp.file.title() %>\n\n<% tp.app.commands.executeCommandById(\"chatgpt-md:call-chatgpt-api\") %>\n"}
{"id":"","title":"also-batman","status":"","priority":"","owner":"","labels":[],"created":"","path":"docs/agile/tasks/also-batman.md","content":""}
{"id":"","title":"batman","status":"","priority":"","owner":"","labels":[],"created":"","path":"docs/agile/tasks/batman.md","content":""}
{"id":"","title":"discord bot squad","status":"","priority":"","owner":"","labels":[],"created":"","path":"docs/agile/tasks/discord bot squad.md","content":""}
{"id":"","title":"ecs_board_planning_note","status":"","priority":"","owner":"","labels":[],"created":"","path":"docs/agile/tasks/ecs_board_planning_note.md","content":"# ECS Board Planning Note\n\n## üìå Purpose\nDefine expectations for an ECS-powered kanban experience so delivery teams can design and implement UI slices that stay aligned with existing markdown + CLI workflows.\n\n## üë• Target Personas\n- **Board Manager (Agents & Ops):** Needs authoritative, low-friction controls to curate tasks, enforce WIP limits, and keep the markdown board in sync.\n- **Individual Contributors:** Want quick ways to discover work, self-assign, and record progress without learning new tooling.\n- **Stakeholders / Reviewers:** Require fast visibility into status, blockers, and scope deltas without leaving canonical documentation.\n\n## üß≠ Desired Interactions\n### Drag & Drop\n- Reorder tasks within a column with optimistic UI updates that reconcile with markdown regeneration on save/commit.\n- Column transitions must validate WIP limits before persisting, surfacing inline warnings if a move would exceed caps defined in regenerated board metadata.\n\n### Keyboard Flows\n- Shortcut-driven column changes (e.g., `[`/`]` to move left/right) that mirror `kanban move_up/move_down/update_status` semantics for accessibility parity.\n- Inline edit/save commands that respect markdown frontmatter fields (status, priority, labels) so regenerated tasks remain valid.\n\n### Filters & Views\n- Quick filters for labels, priority, assignee (future), and status states while retaining markdown as the persisted source.\n- Saved filter presets stored client-side to avoid mutating markdown/CLI artifacts; shareable query URLs for stakeholders.\n\n## ‚öôÔ∏è Performance & Latency\n- Interactions should feel instant; target <150‚ÄØms for drag/drop acknowledgement and <400‚ÄØms for filtered view refreshes on typical boards (<500 tasks).\n- Persisted changes must round-trip through the markdown/CLI pipeline within the same commit/PR cycle‚Äîno background divergence.\n\n## üõ†Ô∏è Coexistence with Markdown & CLI\n- Markdown tasks (`docs/agile/tasks/*.md`) stay authoritative; UI edits must emit operations that reuse `packages/kanban/src/index.ts` commands (push/pull/sync/regenerate).\n- UI should call into the CLI layer (direct spawn or API wrapper) instead of duplicating regeneration logic to ensure parity with `sync` and `regenerate` flows.\n- Conflicts resolved via CLI rules: when UI detects divergence, prompt the operator to run `kanban sync`/`regenerate` before finalizing UI state.\n\n## ‚úÖ Acceptance Criteria (aligned with `docs/agile/agents.md`)\n1. Markdown task files remain the single source of truth; UI cannot persist state that lacks a corresponding markdown update.\n2. WIP limits from regenerated board headers block UI transitions that would exceed caps, matching board-manager enforcement duties.\n3. Regeneration cycle (`kanban regenerate`) stays the blessed way to rebuild `kanban.md`; UI provides a one-click affordance that shells out to this command.\n4. Task status changes always emit updates compatible with CLI push/pull flows so board + tasks remain synchronized.\n5. System logs every UI-induced move for audit, mirroring the agents‚Äô responsibility to document transitions before moving to **Done**.\n\n## ‚ùì Open Questions for Stakeholders\n- Which teams own the ECS board UI (frontend) versus CLI maintenance to guarantee parity long-term?\n- Do we need role-based permissions before GA, or can board managers act as the gatekeepers for early phases?\n- What telemetry is required (e.g., audit trail, performance metrics) to satisfy governance/compliance expectations?\n- Should saved filters be shareable server-side or remain personal until multi-user coordination requirements are clearer?\n- Is offline support (draft changes before sync) required, or can we assume always-on connectivity to the CLI layer?\n\n## üì£ Circulation & Sign-off\n- Share draft with: Platform DX (CLI owners), Board Ops/Agents, Product stakeholders for ECS initiative.\n- Required approvals: `[ ]` Product, `[ ]` Engineering, `[ ]` Operations.\n- Record sign-off decisions back in the linked kanban task before implementation begins.\n"}
{"id":"","title":"Enforce /auth/me rate limiting in SmartGPT Bridge","status":"","priority":"p2","owner":"err","labels":["board:auto","lang:ts","package:smartgpt-bridge"],"created":"","path":"docs/agile/tasks/fix-auth-me-rate-limit.md","content":"\n## Context\n\n### Changes and Updates\n- **What changed?**: `/auth/me` route bypasses per-IP rate limiting\n  expectations; integration test now fails expecting 429 after hitting\n  the limit.\n- **Where?**: `packages/smartgpt-bridge/src/auth.ts` and Fastify auth\n  registration in `packages/smartgpt-bridge/src/fastifyAuth.ts`.\n- **Why now?**: CI surfaced the regression during\n  `integration ‚Ä∫ server.openapi.auth`. Fixing auth rate limiting keeps\n  SmartGPT Bridge secure and unblocks the pipeline.\n\n### Inputs / Artifacts\n- `packages/smartgpt-bridge/src/auth.ts`\n- `packages/smartgpt-bridge/src/fastifyAuth.ts`\n- `packages/smartgpt-bridge/src/tests/integration/server.openapi.auth.test.ts`\n\n## Definition of Done\n- [ ] `/auth/me` applies the configured rate limit, returning 429 when\n      the request count exceeds the limit for an IP.\n- [ ] Integration test `server.openapi.auth` passes locally.\n- [ ] Changelog entry documents the fix.\n- [ ] PR opened referencing this task.\n\n## Plan\n1. Audit auth route registration to understand why per-route rate limit\n   config isn't applied.\n2. Wire Fastify rate limit plugin to `/auth/me` with correct options and\n   test behaviour manually.\n3. Run `pnpm --filter @promethean/smartgpt-bridge test` to confirm\n   integration suite passes.\n4. Add changelog entry summarizing the rate limit fix and prepare PR.\n"}
{"id":"","title":"Fix TypeScript build errors across key packages","status":"","priority":"p2","owner":"codex-cloud","labels":["board:auto","lang:ts"],"created":"","path":"docs/agile/tasks/fix-build-errors-enso-docops-kanban-webcrawler.md","content":"<hr class=\"__chatgpt_plugin\">\n\n<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n### role::assistant\n\nHere's a suggested revision of your context section:\n\n## Context\n### Changes and Updates\n- **What changed?**: Codex Cloud nightly build `pnpm -r --no-bail build` now fails in four workspace packages due to TypeScript errors involving unused variables, stricter nullability, and outdated API assumptions.\n- **Where?**: `packages/enso-protocol`, `packages/docops`, `packages/kanban`, `packages/webcrawler-service`.\n- **Why now?**: Build failures block integration work and violate the no-red-build gate for new tasks.\n\n## Inputs / Artifacts\n- docs/reports/codex_cloud/describe/latest/summary.tsv\n- docs/reports/codex_cloud/describe/latest/logs/pnpm-build.out.log\n\n## Definition of Done\n- [ ] `pnpm --filter @promethean/enso-protocol build` succeeds locally.\n- [ ] `pnpm --filter @promethean/docops build` succeeds locally.\n- [ ] `pnpm --filter @promethean/kanban-cli build` succeeds locally.\n- [ ] `pnpm --filter @promethean/webcrawler-service build` succeeds locally.\n- [ ] No new ESLint errors are introduced in touched files.\n- [ ] Codex Cloud build summary aligns with local fixes (documented in PR summary).\n\n## Plan\n1. Reproduce build failures locally for each package and capture the exact compiler diagnostics.\n2. Update code to satisfy TypeScript's strict null and optional property checks without weakening type safety.\n3. Remove or utilize unused variables and align test expectations with actual API responses.\n4. Run targeted builds and eslint on affected files before committing.\n\n## Relevant Resources\n\nYou might find [this](link) useful while working on this task.\n\n### Smart Connections Configuration\n```smart-connections\n{\n  \"render_markdown\": true,\n  \"show_full_path\": false,\n  \"exclude_blocks_from_source_connections\": false,\n  \"exclude_frontmatter_blocks\": true,\n  \"expanded_view\": false,\n  \"results_limit\": \"20\",\n  \"exclude_inlinks\": false,\n  \"exclude_outlinks\": false\n}\n```\n\n### Smart ChatGPT Configuration\n```smart-chatgpt\n```\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\nfix-build-errors-enso-docops-kanban-webcrawler\n\n<% tp.app.commands.executeCommandById(\"chatgpt-md:call-chatgpt-api\") %>\n"}
{"id":"","title":"Fix frontend-service test failure","status":"","priority":"p3","owner":"err","labels":["board:auto","lang:ts"],"created":"","path":"docs/agile/tasks/fix-frontend-service-test.md","content":"<hr class=\"__chatgpt_plugin\">\n\n## Context\n### Changes and Updates\n- **What changed?**: Test suite for @promethean/frontend-service currently fails, blocking confident deployments.\n- **Where?**: packages/frontend-service/src/tests.\n- **Why now?**: User requested to fix the failing test.\n\n## Inputs / Artifacts\n- `pnpm --filter @promethean/frontend-service test` output.\n\n## Definition of Done\n- [ ] @promethean/frontend-service:test passes locally.\n- [ ] Root cause documented in changelog entry.\n- [ ] Pull request merged with reference to this task.\n\n## Plan\n1. Reproduce the failing test via pnpm command.\n2. Diagnose and implement minimal fix under packages/frontend-service.\n3. Update or add tests ensuring coverage for regression.\n4. Run package tests to confirm pass.\n5. Document change via changelog entry and prepare PR.\n\n## Relevant Resources\n- `packages/frontend-service` package code and tests.\n<hr class=\"__chatgpt_plugin\">\n"}
{"id":"","title":"Fix SmartGPT Bridge grep parity with ripgrep","status":"","priority":"p2","owner":"gpt","labels":["board:auto","lang:ts"],"created":"","path":"docs/agile/tasks/fix-smartgpt-bridge-grep-test.md","content":"\n## Context\n- **What changed?**: CI surfaced a failure for `grep: matches ripgrep output with context and flags`.\n- **Where?**: `packages/smartgpt-bridge` (grep adapter + fixtures).\n- **Why now?**: This check blocks pipelines; aligning our adapter with the\n  reference ripgrep behavior unblocks further work.\n\n## Inputs / Artifacts\n- Failing test log: `packages ‚Ä∫ smartgpt-bridge ‚Ä∫ dist ‚Ä∫ tests ‚Ä∫ unit ‚Ä∫ grep ‚Ä∫ grep: matches ripgrep output with context and flags`.\n- Implementation under investigation: `packages/smartgpt-bridge/src/rg.ts`.\n\n## Definition of Done\n- [ ] Identify the rejection root cause by reproducing the failure locally.\n- [ ] Patch `grep` so it gracefully handles the triggering scenario.\n- [ ] Add or adjust automated coverage if the regression path is missing.\n- [ ] Package build + targeted tests pass.\n\n## Plan\n1. Reproduce the unit failure locally and collect stack traces.\n2. Inspect `grep` error handling for the rejection signature and design a fix.\n3. Implement the adjustment plus any needed fixtures or tests.\n4. Re-run unit tests and lint for the touched files.\n5. Prepare PR notes with evidence of the fix.\n\n## Relevant Resources\n- ripgrep manual: <https://github.com/BurntSushi/ripgrep>\n- Ava docs: <https://github.com/avajs/ava>\n"}
{"id":"","title":"Fix hanging SmartGPT bridge tests","status":"","priority":"p2","owner":"err","labels":["board:auto","lang:ts"],"created":"","path":"docs/agile/tasks/fix-smartgpt-bridge-tests.md","content":"\n## Context\n\n### Changes and Updates\n- **What changed?**: Integration tests for `@promethean/smartgpt-bridge` now stall after\n  a Mongo-backed suite because cached clients point at a torn-down\n  `MongoMemoryServer` instance.\n- **Where?**: `packages/smartgpt-bridge/src/tests/helpers/server.ts`.\n- **Why now?**: Latest CI run showed the suite hanging; stabilizing tests is\n  prerequisite for further bridge work.\n\n### Inputs / Artifacts\n- `packages/smartgpt-bridge/src/tests/helpers/server.ts`\n- `packages/persistence/src/clients.ts`\n\n## Definition of Done\n- [ ] SmartGPT bridge tests finish without hanging locally.\n- [ ] Helper resets cached persistence clients during teardown.\n- [ ] Added regression coverage (implicit via existing tests) and changelog\n      entry.\n- [ ] PR merged.\n\n## Plan\n1. Audit `withServer` teardown and persistence client cache behaviour.\n2. Reset cached Mongo/Chroma clients and restore env vars after each test run.\n3. Run `pnpm --filter @promethean/smartgpt-bridge test` to ensure stability.\n4. Document change in `changelog.d` and prepare PR summary.\n"}
{"id":"","title":"Fix failing web-utils test","status":"","priority":"p2","owner":"err","labels":["board:auto","lang:ts"],"created":"","path":"docs/agile/tasks/fix-web-utils-test.md","content":"\n## Context\n### Changes and Updates\n- **What changed?**: `@promethean/web-utils` has a failing test preventing CI success.\n- **Where?**: `packages/web-utils` tests.\n- **Why now?**: User requested fix for `@promethean/web-utils:test` failure.\n\n## Inputs / Artifacts\n- Pending test failure logs once reproduced locally.\n\n## Definition of Done\n- [ ] `pnpm --filter @promethean/web-utils test` passes locally.\n- [ ] No new lint errors in touched files.\n- [ ] PR prepared referencing this task.\n\n## Plan\n1. Reproduce failure by running package tests.\n2. Identify root cause and implement fix in source or tests.\n3. Update or add tests if necessary.\n4. Run lint and test commands to ensure success.\n5. Commit changes and prepare PR referencing task.\n\n## Relevant Resources\n- None yet.\n"}
{"id":"","title":"hacking-through-the-snow","status":"","priority":"","owner":"","labels":[],"created":"","path":"docs/agile/tasks/hacking-through-the-snow.md","content":""}
{"id":"","title":"Consolidate mk MCP/IDE libs under clj-hacks","status":"","priority":"p3","owner":"err","labels":["board:auto","lang:clj"],"created":"","path":"docs/agile/tasks/migrate_mk_mcp_ide_to_clj_hacks.md","content":"<hr class=\"__chatgpt_plugin\">\n\n### Context\n### Changes and Updates\n- **What changed?**: Need to move mk IDE/MCP shared code into clj-hacks to reuse from Babashka entrypoints.\n- **Where?**: `packages/clj-hacks`, `bb/src/mk`, `bb.edn`, tests.\n- **Why now?**: Consolidation requested so Lisp package exports APIs for tooling.\n\n## Inputs / Artifacts\n- (packages/clj-hacks/deps.edn)\n- (bb/src/mk)\n\n## Definition of Done\n- [ ] mk IDE and MCP logic lives under clj-hacks namespaces.\n- [ ] Babashka wrappers delegate to new modules.\n- [ ] Tests updated and passing via `clj-hacks` alias.\n\n## Plan\n1. Port mk.ide-* namespaces to `clj-hacks.ide.*` and mk.mcp-* to `clj-hacks.mcp.*`.\n2. Update Babashka namespaces to thin wrappers around the new modules and adjust paths in `bb.edn`.\n3. Refresh tests and dependencies to target the relocated namespaces; run package test suite.\n\n## Relevant Resources\n- `packages/clj-hacks/README.md`\n\n<hr class=\"__chatgpt_plugin\">\n"}
{"id":"","title":"promethean-cli-shadow-cljs","status":"","priority":"","owner":"","labels":[],"created":"","path":"docs/agile/tasks/promethean-cli-shadow-cljs.md","content":"# Promethean CLI shadow-cljs launcher\n\n## Summary\n- Replace legacy Sibilant-based CLI launcher with JS wrapper that loads Shadow-CLJS bundle.\n- Author new ClojureScript entrypoint to discover package scripts and expose `promethean <package> <action>` commands.\n- Wire up shadow-cljs build + pnpm scripts, point bin at generated node script.\n\n## Desired Outcome\n- Running `promethean <pkg> <action>` executes the matching npm script.\n- CLI build artefact generated via `pnpm build`.\n- Repository bin references built artefact instead of missing runtime.\n\n## Acceptance Criteria\n- `bin/promethean.js` loads compiled CLI bundle.\n- CLI enumerates packages and scripts as described, with helpful usage when args missing.\n- `shadow-cljs release promethean-cli` produces node-targeted output consumed by bin script.\n- `pnpm build` and `pnpm --filter promethean-cli build` succeed.\n\n## Plan\n1. Add shadow-cljs config + deps for new CLI package.\n2. Implement ClojureScript entry that reads monorepo package.json graph and scripts.\n3. Create thin Node launcher delegating to compiled bundle.\n4. Update pnpm scripts/build + root bin entry.\n5. Document/testing.\n\n## Score\n- Estimated complexity: 5\n\n## Notes\n- Investigate existing tooling for package graph traversal (pnpm workspace? manual?).\n- Ensure generated CLI handles missing scripts gracefully.\n"}
{"id":"","title":"Refactor MCP package for multi-endpoint HTTP transport","status":"","priority":"p3","owner":"err","labels":["board:auto","lang:ts","package:@promethean/mcp"],"created":"","path":"docs/agile/tasks/refactor-mcp-endpoints.md","content":"\n## Context\n- **What changed?**: Need to extend MCP transport loader to support multiple HTTP endpoints from config.\n- **Where?**: `packages/mcp`\n- **Why now?**: Enables testing different MCP tool sets while sharing one package build.\n\n## Inputs / Artifacts\n- `promethean.mcp.json`\n\n## Definition of Done\n- [ ] Config schema accepts endpoint map for HTTP transports.\n- [ ] MCP HTTP server registers each configured endpoint with its toolset.\n- [ ] Tests cover multi-endpoint configuration.\n- [ ] Documentation updated if needed.\n\n## Plan\n1. Inspect current MCP package configuration parsing.\n2. Update configuration types and loader to accept `endpoints` map.\n3. Adjust HTTP server composition to register multiple routers.\n4. Add tests verifying configuration and runtime wiring.\n\n"}
{"id":"","title":"something-something..-danger-zone-","status":"","priority":"","owner":"","labels":[],"created":"","path":"docs/agile/tasks/something-something..-danger-zone-.md","content":""}
{"id":"010f468c-ba8f-4251-a84e-15ffda488446","title":"enable compactor for process state process state s md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"010f468c-ba8f-4251-a84e-15ffda488446","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/enable_compactor_for_process_state_process_state_s.md","content":"## üõ†Ô∏è Task: Enable compactor for `process.state` ‚Üí `process.state.snapshot`\n\nGenerated from [../unique/2025.08.08.15.08.47.md](../unique/2025.08.08.15.08.47.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#ice-box\n\n"}
{"id":"013e2a6c-0eba-4840-adc3-3ed3d278768a","title":"identify ancestral resonance patterns framework co md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.515Z","uuid":"013e2a6c-0eba-4840-adc3-3ed3d278768a","created_at":"2025-09-15T02:02:58.515Z","path":"docs/agile/tasks/identify_ancestral_resonance_patterns_framework_co.md","content":"## üõ†Ô∏è Task: Identify ancestral resonance patterns #framework-core\n\nGenerated from [../unique/2025.07.28.18.07.20.md](../unique/2025.07.28.18.07.20.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"02c83b6f-e9b4-4dfb-a4a0-543042aa02b6","title":"something something","status":"todo","priority":"p3","owner":"","labels":[],"created":"2025-09-15T02:02:58.521Z","uuid":"02c83b6f-e9b4-4dfb-a4a0-543042aa02b6","created_at":"2025-09-15T02:02:58.521Z","path":"docs/agile/tasks/something-something.md","content":"<hr class=\"__chatgpt_plugin\">\n\n<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n### role::assistant\n\nHere's a suggested revision of your context section:\n\n## Context\n### Changes and Updates\n- **What changed?**: [Describe the key changes that have occurred, e.g., updated requirements, new feature added]\n- **Where?**: [Specify the location or scope where these changes are relevant, e.g., specific project, department]\n- **Why now?**: [Explain why these changes are necessary at this time, e.g., due to deadline, feedback from stakeholders]\n\n## Inputs / Artifacts\n- ([link or path] to relevant documentation, data, or assets)\n\n## Definition of Done\n- [ ] test X passes: [ Briefly describe the testing scenario]\n- [ ] doc Y updated: [Mention the specific documentation or resource that has been updated]\n- [ ] PR merged: ([link to the PR] with a brief summary of changes)\n\n## Plan\n1. ‚Ä¶ [ Outline the high-level steps for completing this task, including any dependencies or resources required]\n2. ‚Ä¶\n\n## Relevant Resources\n\nYou might find [this](link) useful while working on this task.\n\n### Smart Connections Configuration\n```smart-connections\n{\n  \"render_markdown\": true,\n  \"show_full_path\": false,\n  \"exclude_blocks_from_source_connections\": false,\n  \"exclude_frontmatter_blocks\": true,\n  \"expanded_view\": false,\n  \"results_limit\": \"20\",\n  \"exclude_inlinks\": false,\n  \"exclude_outlinks\": false\n}\n```\n\n### Smart ChatGPT Configuration\n```smart-chatgpt\n```\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\nUsing this title write a reasonable task kanban task document: something-something.\n\n \n\n\n<hr class=\"__chatgpt_plugin\">\n\n### role::assistant<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n\n## Task: Something-Something\n\n**Task ID:** SS-001\n**Description:** Write down all possible uses of ChatGPT in Obsidian Knowledge Base\n\n### Definition of Done:\n- [ ] Completed list of potential use cases for integrating ChatGPT with Obsidian knowledge management system\n- [ ] High-level research on existing projects/notes that utilize similar functionality\n- [ ] Basic outline of potential benefits and drawbacks for integration\n\n### Tasks:\n\n1. **Brainstorm Potential Use Cases**\n\t* Research how others are using AI in their knowledge management systems\n\t* Identify key features to focus on (e.g., question-answering, content suggestion)\n\t* Jot down initial thoughts on pros and cons of integration\n2. **Explore Existing Projects/Notes**\n\t* Search for existing Obsidian notes and projects that use similar AI-powered features\n\t* Analyze how these projects are using the functionality\n\t* Take note of successes, challenges, and areas for improvement\n3. **Research Potential Benefits and Drawbacks**\n\t* Investigate potential benefits (e.g., increased productivity, improved content organization)\n\t* Research potential drawbacks (e.g., loss of control over generated content, dependency on AI)\n\t* Summarize findings in a concise outline\n\n### Deliverables:\n\n* A list of 10+ potential use cases for integrating ChatGPT with Obsidian knowledge management system\n* A brief report outlining key research findings and initial thoughts on integration strategy\n* An updated outline for the main task document, incorporating insights from existing projects/notes\n\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\n\n"}
{"id":"02e8c2e6-e235-482e-90d9-eb0ce93f8ef8","title":"agent tasks persistence migration to dualstore","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.507Z","uuid":"02e8c2e6-e235-482e-90d9-eb0ce93f8ef8","created_at":"2025-09-15T02:02:58.507Z","path":"docs/agile/tasks/agent-tasks-persistence-migration-to-dualstore.md","content":"## üóÇ Task 1 ‚Äî Setup Shared Persistence Module\n**Status:** blocked\n\n* [ ] Create directory `shared/ts/persistence/`.\n* [ ] Add:\n\n  * `clients.ts`\n  * `types.ts`\n  * `dualStore.ts` (enhanced version with alias, timestamp, UUID, mostRelevant).\n  * `contextStore.ts` (from Cephalon‚Äôs ContextManager).\n  * `maintenance.ts`\n  * `index.ts`\n* [ ] Add `README.md` explaining usage of `DualStore` + `ContextStore`.\n\n‚úÖ Output: Shared persistence module exists, documented, builds cleanly.\n\n---\n\n## üóÇ Task 2 ‚Äî Deprecate Legacy Persistence\n\n* [ ] In **Cephalon**, mark `collectionManager.ts` as **deprecated**.\n* [ ] In **SmartGPT Bridge**, mark `DualSink.js` + `mongo.js` as **deprecated**.\n* [ ] In each service‚Äôs `AGENTS.md`, update:\n\n  * Remove references to local persistence implementations.\n  * Replace with:\n\n    ```\n    Persistence is handled via shared module: @shared/ts/persistence/DualStore\n    ```\n\n‚úÖ Output: No governance doc refers to `CollectionManager` or `DualSink`.\n\n---\n\n## üóÇ Task 3 ‚Äî Service Migrations\n\n### Cephalon\n\n* [x] Replace all imports of `CollectionManager` with `DualStore`.\n* [x] Replace `ContextManager` with `ContextStore` from shared.\n* [x] Adjust methods:\n\n  * `addEntry` ‚Üí `insert`\n  * `getMostRecent` ‚Üí `getMostRecent`\n  * `getMostRelevant` ‚Üí `getMostRelevant`\n\n### SmartGPT Bridge\n\n* [ ] Remove `DualSink` usage.\n* [ ] Replace with `DualStore.create(\"bridge_logs\")`.\n* [ ] Move cleanup jobs to `shared/ts/persistence/maintenance.ts`.\n\n### Discord-embedder\n\n* [ ] Remove raw `MongoClient` + `ChromaClient`.\n* [ ] Replace with `DualStore.create(\"discord_messages\")`.\n\n### Kanban Processor\n\n* [ ] Replace raw `MongoClient` with `DualStore`.\n* [ ] Enable optional Chroma indexing for tasks.\n\n### Markdown Graph\n\n* [x] Replace raw `MongoClient` with `DualStore`.\n* [ ] Add optional embedding for graph queries if needed.\n\n‚úÖ Output: All services use `DualStore` instead of local clients.\n\n---\n\n## üóÇ Task 4 ‚Äî Update Tests\n\n* [ ] Write unit tests for `DualStore`:\n\n  * Insert, recent fetch, relevance query.\n* [ ] Write unit tests for `ContextStore`:\n\n  * Create multiple collections, insert entries, compile context.\n* [ ] Update existing Cephalon + Bridge tests to reference new shared persistence.\n\n‚úÖ Output: All persistence tests point to shared module.\n\n---\n\n## üóÇ Task 5 ‚Äî Update Docs\n\n* [x] Update root `AGENTS.md`: add **Shared Persistence** section.\n* [x] Add new doc `docs/reports/persistence-migration-checklist.md` (we already drafted this).\n* [x] Add `docs/reports/persistence-dependency-graph.md` for before/after diagrams.\n* [x] Ensure diagrams show all services routing through `DualStore`.\n\n‚úÖ Output: Documentation reflects new persistence architecture.\n\n---\n\n## üóÇ Task 6 ‚Äî Cleanup\n\n* [ ] Delete legacy files after migration is verified:\n\n  * `services/ts/cephalon/src/collectionManager.ts`\n  * `services/ts/cephalon/src/contextManager.ts`\n  * `services/ts/smartgpt-bridge/src/utils/DualSink.js`\n  * `services/ts/smartgpt-bridge/src/mongo.js`\n* [ ] Confirm `git grep \"MongoClient\" services/ts/` and `git grep \"ChromaClient\" services/ts/` return **no service-local usage**.\n\n‚úÖ Output: Repo contains only `DualStore`/`ContextStore` persistence code.\n\n---\n\n## üèÅ Final Deliverable\n\n* All services (`cephalon`, `bridge`, `discord-embedder`, `kanban-processor`, `markdown-graph`) use the shared persistence layer.\n* Legacy persistence modules removed.\n* Tests green.\n* Docs updated.\n* Obsidian diagrams show unified architecture.\n\n---\n\n## Blockers\n- No active owner or unclear scope\n\n#breakdown\n\n"}
{"id":"02eeb8bd-1053-40b4-a213-8db02d7aa526","title":"extract docs from riatzukiza github io md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"02eeb8bd-1053-40b4-a213-8db02d7aa526","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/extract_docs_from_riatzukiza_github_io.md","content":"## üõ†Ô∏è Task: Extract docs from riatzukiza.github.io\n\nMove any useful markdown documentation from the old `riatzukiza.github.io` repository into our `/docs/` directory.\n\n---\n\n## üéØ Goals\n- Preserve important design notes and guides\n- Keep history by referencing the original repository\n- Integrate content with our current documentation structure\n\n---\n\n## üì¶ Requirements\n- [ ] Identify relevant markdown files\n- [ ] Copy them into appropriate folders under `docs/`\n- [ ] Update internal links to match Promethean structure\n\n---\n\n## üìã Subtasks\n- [ ] Clone or download the old repository\n- [ ] Review its docs directory\n- [ ] Migrate selected files and commit with attribution\n\n---\n\n## üîó Related Epics\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\n- Extract site modules from riatzukiza.github.io\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n#accepted\n\n"}
{"id":"035cede1-531f-4070-aa80-841a9cf83631","title":"üõ†Ô∏è Task: Implement Classes in Lisp Compiler","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.515Z","uuid":"035cede1-531f-4070-aa80-841a9cf83631","created_at":"2025-09-15T02:02:58.515Z","path":"docs/agile/tasks/implement-classes-in-compiler-lisp-incoming.md","content":"# üõ†Ô∏è Task: Implement Classes in Lisp Compiler\n\n## üìñ Description\nIntroduce a `defclass` macro to the Promethean Lisp compiler that supports class definitions with fields and methods. This will enable object-oriented abstractions on top of functional Lisp foundations.\n\n---\n\n## üéØ Goals\n- Provide a `defclass` syntax for defining classes.\n- Support field declarations with default values.\n- Support method definitions bound to the class.\n- Enable class instantiation and method dispatch.\n- Integrate with existing `defun` and `lambda` infrastructure.\n\n---\n\n## üì¶ Requirements / Definition of Done\n- [ ] Parser recognizes `(defclass <name> (<fields>) (<methods>))`.\n- [ ] AST extended with `ClassNode` and `MethodNode`.\n- [ ] Environment extended to bind classes as first-class entities.\n- [ ] Instances can be created via `(make-instance <Class>)`.\n- [ ] Methods can be invoked on instances `(call <instance> <method> <args>...)`.\n- [ ] Support single inheritance (optional for v1).\n- [ ] Unit tests cover creation, field access, method invocation.\n- [ ] Docs updated with `defclass` usage examples.\n\n---\n\n## üìã Subtasks\n- [ ] Update parser to handle `defclass` syntax.\n- [ ] Extend AST with `ClassNode` and `MethodNode`.\n- [ ] Extend evaluator to create and bind class objects.\n- [ ] Implement instance creation (`make-instance`).\n- [ ] Implement method dispatch (`call`).\n- [ ] Add tests for field defaults, method binding, and inheritance.\n- [ ] Document class usage in Lisp compiler reference.\n\n---\n\n## üîó Dependencies\n- Blocked by: `implement defun in compiler lisp incoming` (functions must exist before methods).\n- Builds toward: `lisp ecosystem files`, `lisp package files`.\n\n---\n\n## üìö Relevant Resources\n- Common Lisp HyperSpec: [`defclass`](http://clhs.lisp.se/Body/m_defcla.htm)\n- CLOS (Common Lisp Object System) overview\n- Hy `defclass` equivalents via macros\n\n---\n\n## üí¨ Comments\nAppend-only thread for collaboration by agents implementing class support.\n\n## Notes\n- Tests or documentation are missing; acceptance criteria not fully met.\n- Story Points: 5\n\n#in-progress\n\n"}
{"id":"03a056bc-c24c-472b-b75a-0b817f370cb7","title":"cephalon persist utterance timing metadata","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.509Z","uuid":"03a056bc-c24c-472b-b75a-0b817f370cb7","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/cephalon_persist_utterance_timing_metadata.md","content":"Cephalon: Persist utterance timing metadata\n\nGoal: Record `startTime` and `endTime` for agent utterances to support turn-taking analytics and better context windows.\n\nWhy: We currently store only text in `agent_messages` when using classic path; ECS path stores nothing. Add consistent timing metadata.\n\nScope:\n- Emit a local event when utterance playback starts and ends (ECS audio player hook exists in `start-dialog.scope.ts`).\n- On start: record `startTime` (store provisional doc with `endTime` null) or buffer until end.\n- On end: upsert the doc (match by an utterance id) to set `endTime`.\n\nExit Criteria:\n- Each agent utterance in `agent_messages` includes `{ startTime, endTime }`.\n- Test that simulates playback hooks and asserts DB update.\n\n#incoming #cephalon #timing #agent_messages\n\n"}
{"id":"03b91de5-2d67-4201-b177-8ecb2029793b","title":"deploy changefeed for collections you want mirrore md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"03b91de5-2d67-4201-b177-8ecb2029793b","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/deploy_changefeed_for_collections_you_want_mirrore.md","content":"## üõ†Ô∏è Task: Deploy **changefeed** for collections you want mirrored to topics\n\nGenerated from [../unique/2025.08.08.20.08.52.md](../unique/2025.08.08.20.08.52.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"0427985f-f4ad-4f85-975f-9c085bcc452a","title":"Install Biome dependency for cephalon-discord","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.515Z","uuid":"0427985f-f4ad-4f85-975f-9c085bcc452a","created_at":"2025-09-15T02:02:58.515Z","path":"docs/agile/tasks/install_biome_dependency_for_cephalon_discord.md","content":"# Install Biome dependency for cephalon-discord\n\n## Description\n`pnpm -r lint` failed in `packages/cephalon-discord` because `@biomejs/biome` was not found.\n\n## Goals\n- Ensure the package has Biome installed so linting can run.\n\n## Requirements\n- Add `@biomejs/biome` to `packages/cephalon-discord` dev dependencies.\n- Verify `pnpm --filter @promethean/cephalon-discord lint` runs successfully.\n\n## Subtasks\n- [ ] Update `package.json` for `cephalon-discord` with Biome dependency.\n- [ ] Commit lockfile changes if necessary.\n- [ ] Run the package lint script to confirm.\n\n#Todo #codex-task\n\n"}
{"id":"04551247-6d67-44ce-8b7d-697fd9f478ad","title":"cache decay mechanisim md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"04551247-6d67-44ce-8b7d-697fd9f478ad","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/cache_decay_mechanisim.md","content":"## üõ†Ô∏è Description\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- What are we trying to accomplish?\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## ‚úÖ Acceptance Criteria\n- Cache entries expire according to the decay schedule.\n- Decay timing is configurable.\n- Tests demonstrate expired entries are removed.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#ice-box\n\n"}
{"id":"0468c4d6-3287-406e-97f2-85d12d88c01b","title":"scripts group indexing tools","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.520Z","uuid":"0468c4d6-3287-406e-97f2-85d12d88c01b","created_at":"2025-09-15T02:02:58.520Z","path":"docs/agile/tasks/scripts_group_indexing_tools.md","content":"Scripts: Group indexing helpers under an indexing package\n\nGoal: Move  file/embedding indexing helpers into `packages/persistance` or create a new package `packages/indexing` with docs.\n\nScope:\n- Move `index_project_files.py` and any related indexing helpers.\n- Add `packages/indexing/README.md` with purpose, dependencies, and usage examples.\n\nExit Criteria:\n- Indexing scripts live under a package`packages/indexing/` or `packages/persistence` with a clear README.\n\n#incoming #scripts #indexing #organization\n\n\n"}
{"id":"04735a0b-d4d9-4880-8702-d64e0eb2ac02","title":"üõ†Ô∏è Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.518Z","uuid":"04735a0b-d4d9-4880-8702-d64e0eb2ac02","created_at":"2025-09-15T02:02:58.518Z","path":"docs/agile/tasks/promethean-health-dashboard.md","content":"# üõ†Ô∏è Description\n\nBuild a **real‚Äëtime system dashboard** that gives an at‚Äëa‚Äëglance view of Promethean‚Äôs state:\n\n* **Health metrics** per service (CPU, mem, NPU/GPU, net I/O, uptime, heartbeat status)\n* **Eidolon field visualizations** (live/paused, scalar & vector field views, probes)\n* **Broker message graph** (services/agents as nodes, topics/queues as edges; volume, latency)\n\nThe dashboard must be low‚Äëlatency, drill‚Äëdown friendly, and safe for on‚Äëstream viewing with redaction modes.\n\n---\n\n# üéØ Goals\n\n* Single pane of glass for **status, performance, and cognition**.\n* Interactive **visual diagnostics** for Eidolon fields.\n* Real‚Äëtime **message‚Äëpassing graph** to spot bottlenecks and dead letters.\n* Clear **alerts** and **SLO** views (healthy / degraded / failing) with drill‚Äëdowns.\n\n---\n\n# üì¶ Requirements\n\n* [ ] **Data collection**\n\n  * Pull from Heartbeat service (liveness, CPU/mem/net/NPU/GPU).\n  * Subscribe to broker events (publish/consume, queue depth, latency, error counts).\n  * Eidolon field snapshots/streams (downsampled vector/scalar tiles).\n* [ ] **Transport**\n\n  * WebSocket channels for push updates (metrics, broker events, field frames).\n  * Backpressure + client rate limiting; replay window for last N minutes.\n* [ ] **UI/UX**\n\n  * Cards for per‚Äëservice health; sparkline trends; quick filter/search.\n  * Broker **graph view** (force‚Äëdirected / dagre) with edge thickness by throughput, color by error rate.\n  * Eidolon **field viz**: toggle scalar heatmap vs vector quiver/flow; pause, step, record GIF/MP4.\n  * Global status bar with SLO indicators; alert drawer with acknowledge/assign.\n  * Redaction mode (hide secrets/paths/IDs; show aggregates only).\n* [ ] **Controls**\n\n  * Time window (live, 1m, 5m, 15m) and playback scrubber.\n  * Per‚Äëservice drill‚Äëdown (logs tail link, last incidents, config snapshot).\n  * Broker filters (by topic, service, severity, error type).\n* [ ] **Persistence & History**\n\n  * Optional metrics store (Prometheus/OpenTelemetry/Timeseries) for trends & SLOs.\n  * Event archive for incident review.\n* [ ] **Security**\n\n  * Auth (token or SSO) and RBAC (viewer, operator, admin).\n  * CORS and origin pinning; redaction by role.\n\n---\n\n# ‚úÖ Acceptance Criteria\n\n* p95 UI update latency **< 500 ms** for new broker messages & health ticks.\n* Broker graph reflects **topic throughput** changes within the selected window; node hover shows CPU/mem & queue depth.\n* Eidolon field renders **‚â• 20 FPS** at default resolution (downsampled), with pause/step working.\n* **Alerts** raised when:\n\n  * Service heartbeat missing > timeout\n  * Queue depth exceeds threshold\n  * Error rate spikes > threshold\n* Redaction mode hides secrets/IDs and any PII paths; screenshot‚Äësafe.\n\n---\n\n# üìã Subtasks\n\n* [ ] **Back‚Äëend gateway** (Node/TS): unify heartbeat, broker, and eidolon feeds ‚Üí WS topics.\n* [ ] **Metrics adapters**: heartbeat ‚Üí metrics DTO; broker ‚Üí graph events; eidolon ‚Üí frame tiles.\n* [ ] **Schema**: define WS message types and versioning.\n* [ ] **Front‚Äëend shell** (React, Tailwind, shadcn): layout, theming (dark‚Äëfirst), auth gate.\n* [ ] **Health cards** + service table with sorting, filters, sparklines.\n* [ ] **Broker graph** (force simulation; zoom/pan; edge bundling; tooltips; filters).\n* [ ] **Eidolon viz** (WebGL/Canvas): scalar heatmap + vector field; controls.\n* [ ] **Alerts system** (thresholds, rules, acknowledge/assign; toast + drawer).\n* [ ] **Redaction layer** + role‚Äëbased routes.\n* [ ] **Timeseries storage** (optional, feature‚Äëflagged) + SLO pages.\n* [ ] **E2E hook** to open logs/links in existing ops tools.\n\n---\n\n# üîå Data Sources & APIs\n\n* Heartbeat service: `/heartbeats` + WS; includes pid, name, uptime, CPU/mem/net, limits, status.\n* Broker: WS tap on publish/consume; queue depth; delivery latency; errors.\n* Eidolon field service: streaming frames (binary tiles) or periodic PNG/JSON with vectors.\n* Auth: JWT/OAuth2 (configurable).\n\n---\n\n# üß± Message Schemas (WS examples)\n\n```json\n// health.tick\n{\n  \"t\": \"health.tick\",\n  \"ts\": 1712345678,\n  \"svc\": \"stt\",\n  \"cpu\": 0.41,\n  \"mem\": 512e6,\n  \"npu\": 0.32,\n  \"gpu\": 0.05,\n  \"net\": {\"rx\": 20480, \"tx\": 40960},\n  \"state\": \"online\"\n}\n```\n\n```json\n// broker.edge\n{\n  \"t\": \"broker.edge\",\n  \"ts\": 1712345679,\n  \"src\": \"stt\",\n  \"dst\": \"cephalon\",\n  \"topic\": \"transcript.partial\",\n  \"rate\": 120,          \n  \"latency_ms\": 18,\n  \"errors\": 0\n}\n```\n\n```json\n// eidolon.frame (metadata for tiled payload)\n{\n  \"t\": \"eidolon.frame\",\n  \"ts\": 1712345680,\n  \"layer\": \"uptime\",\n  \"mode\": \"vector|scalar\",\n  \"tileId\": \"a1b2\",\n  \"w\": 256,\n  \"h\": 256,\n  \"byteLength\": 65536\n}\n```\n\n---\n\n# üß≠ Architecture\n\n```mermaid\nflowchart LR\n  subgraph Backend\n    HB[Heartbeat API]\n    BRK[Broker Events]\n    EID[Eidolon Frames]\n    GW[Dashboard Gateway]\n  end\n  HB --> GW\n  BRK --> GW\n  EID --> GW\n  GW == WS ==> UI[Dashboard UI]\n  UI <-- Auth --> AUTH[Auth/SSO]\n```\n\n---\n\n# üß∞ Implementation Notes\n\n* **Front‚Äëend**: React + Tailwind + shadcn/ui; broker viz via **d3‚Äëforce** or **Cytoscape.js**; Eidolon via **PixiJS** or raw WebGL (regl).\n* **Back‚Äëend**: Node/TS WS gateway; fast JSON (uWebSockets.js or ws); optional **OpenTelemetry** export.\n* **Perf**: binary frames for fields; delta compression; requestAnimationFrame for render loop.\n* **Testing**: mocked WS server; time‚Äëtravel playback for deterministic tests; visual regression with Playwright.\n\n---\n\n# üîí Security & Access\n\n* Roles: `viewer` (redacted), `operator` (full metrics), `admin` (config/alerts).\n* Signed URLs for frame tiles; CORS allowlist.\n* No raw PII; redact paths/IDs; configurable data retention.\n\n---\n\n# üìà Metrics & SLOs\n\n* UI p95 update latency; dropped‚Äëframe % for viz.\n* Broker throughput, queue depth p95, error rate.\n* Alert MTTA/MTTR.\n* Eidolon viz FPS; frame decode time.\n\n---\n\n# üß≠ Non‚ÄëGoals (v1)\n\n* Historical analytics beyond 24h without enabling the optional store.\n* Editing service configs from the dashboard (view‚Äëonly v1).\n* Deep video/stream overlays (future add‚Äëon).\n\n---\n\n# ‚ö†Ô∏è Risks & Mitigations\n\n* **High event volume** ‚Üí server‚Äëside sampling & client throttling.\n* **Viz jank** ‚Üí binary tiles + GPU rendering + decoupled render loop.\n* **Secret leakage on stream** ‚Üí redaction mode default; role gating; sanitized logs.\n\n---\n\n# üö• Phased Delivery\n\n1. **MVP**: health cards + broker throughput table (no graphs), basic WS.\n2. **Graph pass**: broker graph + filters + alerts.\n3. **Eidolon viz**: scalar heatmap then vector flow; recording.\n4. **SLO & history**: metrics store + SLO views; incident timeline.\n\n---\n\ntags: #framework-core #observability #eidolon-visualization #dashboard #broker #realtime\n#IceBox\n\n## Notes\n- Tests or documentation are missing; acceptance criteria not fully met.\n- Story Points: 8\n\n#ready\n\n"}
{"id":"0481711b-20f9-447e-9447-513785a3ac68","title":"add manualack to event bus and re run tests md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.504Z","uuid":"0481711b-20f9-447e-9447-513785a3ac68","created_at":"2025-09-15T02:02:58.504Z","path":"docs/agile/tasks/add_manualack_to_event_bus_and_re_run_tests.md","content":"## üõ†Ô∏è Task: Add `manualAck` to event bus and re-run tests\n\nGenerated from [../unique/2025.08.08.15.08.47.md](../unique/2025.08.08.15.08.47.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"05a6aec0-c595-44c8-8dba-49eac198344f","title":"document board sync workflow md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"05a6aec0-c595-44c8-8dba-49eac198344f","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/document_board_sync_workflow.md","content":"## üõ†Ô∏è Task: Document board sync workflow\n\nAfter the sync script exists, we need clear instructions on how to use it and how the GitHub Projects board maps to our local kanban.\n\n---\n\n## üéØ Goals\n- Provide step-by-step usage instructions\n- Explain column mappings and limitations\n- Encourage contributors to keep both boards in sync\n\n---\n\n## üì¶ Requirements\n- [x] Add documentation under `docs/` describing setup\n- [x] Include example command invocations\n- [x] Mention required GitHub permissions\n\n---\n\n## üìã Subtasks\n- [x] Write README section or new doc\n- [x] Describe expected workflow\n- [x] Link to the sync script\n\n---\n\n## üîó Related Epics\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n- Write board sync script\n\n## ‚õìÔ∏è Blocks\nNothing\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n- [Board Sync Workflow](../../board_sync.md)\n\n\n#archive\n\n"}
{"id":"06dc563d-0b7e-415a-a435-5061b690aa97","title":"scripts add make targets and aliases","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.519Z","uuid":"06dc563d-0b7e-415a-a435-5061b690aa97","created_at":"2025-09-15T02:02:58.519Z","path":"docs/agile/tasks/scripts_add_make_targets_and_aliases.md","content":"Scripts: Add Make targets and aliases\n\nGoal: Provide Make targets for common script flows (kanban sync, WIP sheriff, docs fixes) for consistent local/CI usage.\n\nScope:\n- Add Makefile targets:\n  - `kanban.sync` ‚Üí pnpm kanban sync\n  - `kanban.backsync` ‚Üí pnpm kanban push\n  - `kanban.wip` ‚Üí pnpm tsx packages/kanban/src/scripts/wip-sheriff.ts --write\n  - `docs.links.wiki` ‚Üí python scripts/docs/convert_markdown_links_to_wiki.py\n  - `docs.links.md` ‚Üí python scripts/docs/convert_wikilinks.py\n- Document targets in scripts/README.md and CONTRIBUTING notes.\n\nExit Criteria:\n- Targets exist and work from repo root; CI can call them.\n\n#incoming #scripts #make #dx\n\n\n"}
{"id":"0710bcba-3a0f-4c33-9c83-7f6e858d8d32","title":"cephalon store user transcripts unified","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.509Z","uuid":"0710bcba-3a0f-4c33-9c83-7f6e858d8d32","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/cephalon_store_user_transcripts_unified.md","content":"Cephalon: Store user transcripts via a unified handler\n\nGoal: Centralize transcript persistence for user speech in one path used by both `join-voice` and `start-dialog` flows.\n\nWhy: Transcript storage currently wired in `join-voice.scope.ts` but not in the ECS `start-dialog` orchestration beyond bus publish. Unify to avoid regressions.\n\nScope:\n- Extract a helper `persistTranscript(bot, transcript)` and call it from both `join-voice` and `start-dialog` transcriptEnd listeners.\n- Ensure consistent metadata: `{ userId, userName, channel, recipient, is_transcript: true }` plus timing.\n\nExit Criteria:\n- User transcripts reliably stored in `transcripts` for both flows.\n- Test exercising both scopes with a fake transcript and asserting Mongo insert.\n\n#incoming #cephalon #transcripts #persistence\n\n"}
{"id":"072f7aad-b8a2-4361-9656-d9b84008feab","title":"prevent dangling processes when a process fails due to error or automaticly clean them up 1 md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.518Z","uuid":"072f7aad-b8a2-4361-9656-d9b84008feab","created_at":"2025-09-15T02:02:58.518Z","path":"docs/agile/tasks/prevent_dangling_processes_when_a_process_fails_due_to_error_or_automaticly_clean_them_up_1.md","content":"## üõ†Ô∏è Description\n\nThis is kinda started through the heartbeat server, but t hat is not nearly robust enough.\n\nThis process has been started. We'll be concrete about that much.\n\nThere is a centralized process that can be used to collect information about this.\n\nProcesses need to be  able to clean up after  themselves. At least the   node  ones. Cause  node has the mechanisims to do it easier.\n\nThe heart beat server  will  prevent the  danglers from lasting for too long (assuming it works on windows, I don't know exactly.)\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Add Ollama formally to pipeline\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core #resources #process-management #aionian \n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n\n#archive\n\n"}
{"id":"07e85844-1390-4c3c-80d4-251254f71b4e","title":"design audio pipeline mvp","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.511Z","uuid":"07e85844-1390-4c3c-80d4-251254f71b4e","created_at":"2025-09-15T02:02:58.511Z","path":"docs/agile/tasks/design-audio-pipeline-mvp.md","content":"<hr class=\"__chatgpt_plugin\">\n\n### role::assistant<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n\n---\npromptId: smart_task_templater_md\nname: \"{{title}}\"\ndescription: A task on the kanban of the promethean system.\nauthor: Aaron Beavers\ntags:\n  - prompt-refinement\n  - promptcompiler\n  - \"#metaprogramming\"\n  - \"#metacompiler\"\nversion: 0.0.1\ndisableProvider: false\nprovider: ollama\ncommands: generate\nmode: replace\nstreaming: \"true\"\nmodel: ollama@llama3.2:latest\nprompt: \"{{title}}\"\ntask-id: TASK-{{date}}\npriority: p3\nsystem_commands: ['You are a helpful assistant.']\nfrequency_penalty: 0\nmax_tokens: 400\npresence_penalty: 0\nstream: true\ntemperature: 0.7\n---\n<hr class=\"__chatgpt_plugin\">\n\n<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n### role::assistant\n\nHere's a suggested revision of your context section:\n\n## Context\n### Changes and Updates\n- **What changed?**: [Describe the key changes that have occurred, e.g., updated requirements, new feature added]\n- **Where?**: [Specify the location or scope where these changes are relevant, e.g., specific project, department]\n- **Why now?**: [Explain why these changes are necessary at this time, e.g., due to deadline, feedback from stakeholders]\n\n## Inputs / Artifacts\n- ([link or path] to relevant documentation, data, or assets)\n\n## Definition of Done\n- [ ] test X passes: [ Briefly describe the testing scenario]\n- [ ] doc Y updated: [Mention the specific documentation or resource that has been updated]\n- [ ] PR merged: ([link to the PR] with a brief summary of changes)\n\n## Plan\n1. ‚Ä¶ [ Outline the high-level steps for completing this task, including any dependencies or resources required]\n2. ‚Ä¶\n\n## Relevant Resources\n\nYou might find [this](link) useful while working on this task.\n\n### Smart Connections Configuration\n```smart-connections\n{\n  \"render_markdown\": true,\n  \"show_full_path\": false,\n  \"exclude_blocks_from_source_connections\": false,\n  \"exclude_frontmatter_blocks\": true,\n  \"expanded_view\": false,\n  \"results_limit\": \"20\",\n  \"exclude_inlinks\": false,\n  \"exclude_outlinks\": false\n}\n```\n\n### Smart ChatGPT Configuration\n```smart-chatgpt\n```\n<hr class=\"__chatgpt_plugin\">\n\n\n### role::user\n\n### role::assistant <span style=\"font-size: small;\"> (llama3.2:latest)</span>\n\n\n### role::user\n\nUsing this title write a reasonable task kanban task document:\ndesign-audio-pipeline-mvp\n\ntrue \n\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\n\n\n<hr class=\"__chatgpt_plugin\">\n\n### role::assistant<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n\nIt seems like you've started writing in your Markdown document, but I didn't receive any text to help me understand the context or provide relevant assistance. Could you please share what's on your screen or type out the text you'd like help with? I'll do my best to assist you within this Obsidian note-taking space.\n\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\n\n"}
{"id":"083d6047-9529-4d0b-9ada-406acf3d97f8","title":"obsidian kanban github project board mirror system md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.517Z","uuid":"083d6047-9529-4d0b-9ada-406acf3d97f8","created_at":"2025-09-15T02:02:58.517Z","path":"docs/agile/tasks/obsidian_kanban_github_project_board_mirror_system.md","content":"## üõ†Ô∏è Task: Obsidian Kanban Github Project Board Mirror system\n\nSynchronize our local kanban board with a GitHub Projects board so tasks stay consistent between Obsidian and GitHub.\n\n---\n\n## üéØ Goals\n- Keep GitHub Projects and `kanban.md` in sync\n- Allow contributors to update tasks from either interface\n- Provide a repeatable workflow using scripts or GitHub Actions\n\n---\n\n## üì¶ Requirements\n- [ ] Decide on authentication method (PAT or GitHub App)\n- [ ] Map kanban columns to project board columns\n- [ ] Provide push and pull capabilities\n- [ ] Document setup steps\n\n---\n\n## üìã Subtasks\n- [x] [Research GitHub Projects board API](Research%20GitHub%20Projects%20board%20API.md)\n- [ ] [Write board sync script](Write%20board%20sync%20script.md)\n- [ ] [Document board sync workflow](Document%20board%20sync%20workflow.md)\n\n---\n\n## üîó Related Epics\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\n- Future automation for multi-repo boards\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n\n#archive\n\n"}
{"id":"088cebc7-a2cb-4620-911c-87e2ee895f20","title":"write a small cutover script to replay historical md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.524Z","uuid":"088cebc7-a2cb-4620-911c-87e2ee895f20","created_at":"2025-09-15T02:02:58.524Z","path":"docs/agile/tasks/write_a_small_cutover_script_to_replay_historical.md","content":"## üõ†Ô∏è Task: Write a small **cutover** script to replay historical events through upcasters into snapshots\n\nGenerated from [../unique/2025.08.08.20.08.52.md](../unique/2025.08.08.20.08.52.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#ice-box\n\n"}
{"id":"099c293b-2aaf-4ead-9cea-e88d8e352606","title":"refactor speech interuption system to be more inteligent using audio data to decide if interupted md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.519Z","uuid":"099c293b-2aaf-4ead-9cea-e88d8e352606","created_at":"2025-09-15T02:02:58.519Z","path":"docs/agile/tasks/refactor_speech_interuption_system_to_be_more_inteligent_using_audio_data_to_decide_if_interupted.md","content":"## üõ†Ô∏è Description\n\nRefactor the speech interruption system to analyze incoming audio data so the agent only stops speaking when the user intentionally interrupts.\nThe current speech interruption detection in Discord conversations is unreliable.  \nSometimes the agent stops speaking too quickly, while other times it fails to pause when a user begins talking.  \nWe need smarter audio-based heuristics so the agent reacts consistently.\n\n---\n\n## üéØ Goals\n\n- Reduce accidental interruptions caused by background noise.\n- Allow the user to break in quickly when needed.\n- Use real-time audio energy/VAD thresholds to decide when a user intends to interrupt.\n- Stop or pause TTS output within 200‚ÄØms of a valid interruption.\n- Ignore background noise or echoes that fall below the interruption threshold.\n- Improve accuracy of detecting when a user is speaking.\n- Prevent premature interruptions of the agent's own speech.\n- Allow configurable sensitivity for different environments.\n\n---\n\n## üì¶ Requirements\n\n- [ ] Interruption detection uses audio energy or similar metrics rather than fixed timers.\n- [ ] Sensitivity thresholds are configurable.\n- [ ] Unit tests cover interruption, no-interruption, and noise scenarios.\n- [ ] Documentation explains how the new logic works and how to tune it.\n- [ ] Interruption detection latency is **‚â§‚ÄØ200‚ÄØms** from user speech onset.\n- [ ] Valid interruption detection accuracy ‚â• **95‚ÄØ%** with false-positive rate < **5‚ÄØ%**.\n- [ ] TTS resumes or hands off control within **500‚ÄØms** after interruption is handled.\n- [ ] Audit current interruption logic in audio processing service.\n- [ ] Incorporate amplitude or spectral analysis to distinguish speech from silence.\n- [ ] Expose configuration knobs for interruption sensitivity.\n- [ ] Document new behavior and parameters.\n\n---\n\n## üìã Subtasks\n\n- [ ] Audit existing interruption logic and document current behavior.\n- [ ] Collect representative audio samples for noise, silence, and user speech.\n- [ ] Implement audio-metric based interruption detection.\n- [ ] Expose configuration options for thresholds and windows.\n- [ ] Write unit/integration tests for typical scenarios.\n- [ ] Update docs describing interruption workflow.\n- [ ] Collect sample conversation audio to calibrate energy/VAD thresholds.\n- [ ] Implement threshold-based interruption check in the speech loop.\n- [ ] Abort or pause active TTS when threshold is crossed.\n- [ ] Log detection latency and correctness for evaluation.\n- [ ] Add tests covering true and false interruption scenarios.\n- [ ] Identify where interruption hooks exist in the Discord audio pipeline.\n- [ ] Prototype improved detection using audio signal metrics.\n- [ ] Add tests for successful and failed interruption cases.\n- [ ] Update docs and examples for configuring thresholds.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#Breakdown\n\n\n"}
{"id":"0a2763ff-0f30-4605-87b4-2548e8679a77","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.515Z","uuid":"0a2763ff-0f30-4605-87b4-2548e8679a77","created_at":"2025-09-15T02:02:58.515Z","path":"docs/agile/tasks/identify_and_resolve_a_service_client_apparently_connecting_repeatedly_to_broker_with_new_session_ids.md","content":"#in-progress\n\n# Description\n\nService client repeatedly reconnects to the broker creating new session IDs.\n\n## Blockers\n\n- No tests cover reconnection and session handling.\n- Missing documentation describing expected client behavior.\n\n"}
{"id":"0a4d6f50-753a-48ef-a9a0-10f279eb621b","title":"add docker compose files to services","status":"todo","priority":"p3","owner":"","labels":[],"created":"2025-09-15T02:02:58.504Z","uuid":"0a4d6f50-753a-48ef-a9a0-10f279eb621b","created_at":"2025-09-15T02:02:58.504Z","path":"docs/agile/tasks/add-docker-compose-files-to-services.md","content":"<hr class=\"__chatgpt_plugin\">\n\nWe want for each service to be basically a config file.\nthough I keep thinking about this...\n\nmost of the time a service is a thing that is running by its self, and there is only 1 of them (or a cluster of splinters) \n\nWhy did I split everything up in the first place...\n\n"}
{"id":"0ada665e-5766-45f2-9edb-f5feee65e226","title":"expose snapshot api for processes collection proce md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"0ada665e-5766-45f2-9edb-f5feee65e226","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/expose_snapshot_api_for_processes_collection_proce.md","content":"## üõ†Ô∏è Task: Expose **Snapshot API** for `processes` (collection `processes`)\n\nGenerated from [../unique/2025.08.08.20.08.83.md](../unique/2025.08.08.20.08.83.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"0b5c2829-4d26-4bda-93db-ab13a5a80a12","title":"setup services to recieve work from the broker via push md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.520Z","uuid":"0b5c2829-4d26-4bda-93db-ab13a5a80a12","created_at":"2025-09-15T02:02:58.520Z","path":"docs/agile/tasks/setup_services_to_recieve_work_from_the_broker_via_push.md","content":"### ‚úÖ Setup services to use broker correctly\n\n\n**Refactor broker to use `queueManager` for service-assigned task dispatch**\n\n---\n\n### üìã Description\n\nReplace the current pull-based task handling in the broker (`enqueue`/`dequeue`) with push-based task dispatch via `queueManager`. Services will register themselves as available to receive tasks from specific named queues (one per service for now).\n\nEach queue represents a task type or target service. Over time, this model will evolve to support:\n\n- Multiple concurrent tasks per service (bounded by load metrics or field state)\n    \n- Task prioritization and throttling based on system state\n    \n\n---\n\n### üß† Concepts\n\n- **Service**, not worker ‚Äî the agent that owns the queue (e.g. `tts`, `stt`, `embedder`)\n    \n- **Queue**, per service ‚Äî maps to a capability, not an individual instance\n    \n- **Availability** ‚Äî service must explicitly signal readiness\n    \n- **Push delivery** ‚Äî the broker decides when to deliver, based on availability\n    \n\n---\n\n### üß© Acceptance Criteria\n\n-  Remove support for `dequeue` (broker owns delivery now)\n    \n-  Add support for:\n    \n    - `ready` messages (mark service queue as ready to receive next task)\n        \n    - `ack` messages (confirm task was received and is being handled)\n        \n    - `heartbeat` (optional for liveness tracking)\n        \n-  Each task enqueued is assigned to a ready service instance immediately, or queued until one is ready\n    \n-  If a task isn‚Äôt acknowledged in time, it is requeued\n    \n-  Broker logs task delivery + acknowledgment per service queue\n    \n\n---\n\n### üìé Optional Extensions (future tasks)\n\n- Add `concurrency` setting to each queue (e.g. `tts = 2`, `stt = 1`)\n    \n- Integrate feedback from Eidolon metrics to throttle or pause queues\n    \n- Report queue state via API or log endpoint for debugging\n    \n\n---\n\n### üè∑ Tags\n\n#codex-task #broker #queueManager #service-oriented #push-queue #agent-mode\n\n---\n#in-progress\n\n## Blockers\n- Missing integration tests for push-based task delivery.\n- Shared broker client interface lacks documentation.\n\n## Comments\n\nThis is kind of done. But also kind of not. I am not a fan of *how* many of these services got broker access. It should all be through a shared broker client interface. Anything extra they need, needs to be encapsulated\n\nWe're gonna call it In Progress\n\n## Notes\n- Tests or documentation are missing; acceptance criteria not fully met.\n- Story Points: 8\n\n#in-progress\n\n"}
{"id":"0b9f6e21-3c4d-4c80-9e10-62d3a7d9c421","title":"cephalon/enso ‚Äî type rationale payload + consider event rename","status":"todo","priority":"P2","owner":"","labels":["cephalon","enso","policy","evaluation"],"created":"2025-10-02T00:00:00.000Z","uuid":"0b9f6e21-3c4d-4c80-9e10-62d3a7d9c421","created_at":"2025-10-02T00:00:00.000Z","path":"docs/agile/tasks/enso-rationale-payload-and-event-name.md","content":"#Todo\n\n## üõ†Ô∏è Description\nDefine `ActRationalePayload` in `enso-protocol` with `policy`, `evidence`, and `evidenceKind`. Optionally rename `act.rationale` ‚Üí `guardrail.rationale`.\n\n## Requirements\n- [ ] Export type in protocol\n- [ ] Update producers/consumers\n- [ ] Tests adjusted\n\n## ‚õìÔ∏è Blocks\n- Merge PR #1450\n\n## üîç Related\n- PR #1450\n"}
{"id":"0c583c42-3b0d-47fc-9b54-5bd952c84c74","title":"finalize stt workflow md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"0c583c42-3b0d-47fc-9b54-5bd952c84c74","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/finalize_stt_workflow.md","content":"## üõ†Ô∏è Task: Finalize STT workflow\n\nThe STT service has a GitHub workflow but no requirements file.\n\n---\n\n## üéØ Goals\n- Provide `requirements.txt` for STT tests\n- Ensure workflow installs dependencies correctly\n\n---\n\n## üì¶ Requirements\n- [ ] Add a minimal `requirements.txt` to `services/stt`\n- [ ] Update `python-tests.yml` accordingly\n\n---\n\n## üìã Subtasks\n- [ ] Define dependencies needed for STT\n- [ ] Verify workflow passes in CI\n\n---\n\n## üîó Related Epics\n#codex-task #testing\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\nNothing\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n- [python-tests.yml](../../services/stt/.github/workflows/python-tests.yml)\n\n\n\n\n## Reason for rejection\nOther tasks expanded to include the intention of this ticket before this one was b\n#rejected\n\n"}
{"id":"0ce451d4-77c5-426c-9016-c79d9f4aec6b","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.509Z","uuid":"0ce451d4-77c5-426c-9016-c79d9f4aec6b","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/codex-action-build-file-contextualizer.md","content":"# Description\n\nDescribe your task\n\n## Requirements/Definition of done\n\n- If it doesn't have this, we can't accept it\n\n## Tasks \n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Relevent resources\n\nYou might find [this] useful while working on this task\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n#archive\n\n"}
{"id":"0d72fa12-df0e-4fbe-b559-4df068967176","title":"add obsidian to gitignore md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.505Z","uuid":"0d72fa12-df0e-4fbe-b559-4df068967176","created_at":"2025-09-15T02:02:58.505Z","path":"docs/agile/tasks/add_obsidian_to_gitignore.md","content":"## üõ†Ô∏è Task: Add .obsidian to .gitignore\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Add .obsidian to .gitignore\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [x] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n## Comments\n\n`.obsidian/` is already listed in `.gitignore` (line 11), so this task is effectively complete.\n#done\n\n"}
{"id":"0d89cdd7-1a6c-4477-9cbc-d12472564b58","title":"Web Crawler Agent ‚Üí Hyperlink Graph ‚Üí LLM Summaries/#Tags ‚Üí Chroma Search (Dual-Sink)","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.523Z","uuid":"0d89cdd7-1a6c-4477-9cbc-d12472564b58","created_at":"2025-09-15T02:02:58.523Z","path":"docs/agile/tasks/webcrawler.md","content":"# Web Crawler Agent ‚Üí Hyperlink Graph ‚Üí LLM Summaries/#Tags ‚Üí Chroma Search (Dual-Sink)\n\n**Owner:** Codex / Agent\n**Status:** Planned\n**Labels:** #agents #crawler #llm #summarization #chroma #mongodb #etl #dual-sink #search #observability #promethean\n\n---\n\n## üõ†Ô∏è Description\n\nCreate an agent that:\n\n1. **Crawls** the web (seeded) and builds a **hyperlink graph** (nodes = pages, edges = hyperlinks).\n2. Runs **separate LLM passes** per document for: a) concise summaries, b) #tag extraction (Obsidian-style).\n3. Indexes summaries + tags into **Chroma** for semantic search while persisting canonical docs/graph to **Mongo** (**dual-sink**).\n4. Ships a minimal **search API** and report dashboards.\n\nNo magic. Deterministic where it matters. Respect robots.txt. Idempotent by URL+content hash.\n\n---\n\n## üéØ Goals\n\n* Crawl N domains with polite rate-limits and **full robots compliance**.\n* Build **Mongo-backed** hyperlink graph with normalized URLs + content hashes.\n* Produce **LLM summaries** (‚â§ \\~200‚Äì300 tokens) + **#tags** (3‚Äì12 concise tokens, no spaces; use kebab\\_case if needed).\n* Persist to **Mongo (canonical)** and **Chroma (embeddings)** via **dual-sink** path with consistent IDs.\n* Provide **Fastify** endpoints for query, node/edge retrieval, and health/metrics.\n* Ship **runbooks + configs**. No internet writes in tests.\n\n---\n\n## üì¶ Requirements / Definition of Done (non-negotiable)\n\n* **Robots**: agent obeys `robots.txt` (disallow, crawl-delay, sitemap parsing) and honors per-host concurrency ‚â§ 1 unless `crawl-delay` permits.\n* **Politeness**: global QPS + per-domain throttles, retry/backoff, and user-agent string configurable.\n* **Safety**: block private IP ranges, login/checkout forms, and obvious traps (`calendar`, infinite params via URL canonicalizer & deny-lists).\n* **Idempotency**: dedupe by **canonical\\_url** and **content\\_sha256**. Re-crawls only when ETag/Last-Modified or max-age expires.\n* **Dual-sink**: Mongo = source of truth (docs, graph, summaries, tags, embeddings meta). Chroma = embeddings (search). Atomic-ish write (2-phase best-effort + retry).\n* **LLM passes separated**: summary pass ‚â† tag pass (distinct prompts, reproducible seeds/temperature=0 where possible).\n* **Search**: cosine/inner-product over summaries in Chroma; metadata filterable by domain, tags.\n* **Observability**: metrics, structured logs, integrity reports (counts, orphans, embedding dim/model).\n* **No new TS path aliases**. Use **@shared/ts/dist/** only.\n* **Docs + runbooks** checked in.\n* **CI**: fully offline mocked tests; no network calls.\n\n---\n\n## üß± Architecture (high-level)\n\n* **services/ts/crawler** ‚Äî polite fetcher + HTML parser + link extractor + robots manager.\n* **services/ts/graph** ‚Äî Mongo persistence for `pages`, `edges`, dedupe, versioning.\n* **services/ts/summarizer** ‚Äî LLM summaries (Ollama/Qwen3 etc.), deterministic prompts, batch-safe.\n* **services/ts/tagger** ‚Äî LLM tag extraction (Obsidian-style `#tags`), policy checks.\n* **services/ts/indexer** ‚Äî dual-sink upserts ‚Üí Mongo (canonical record) + Chroma (embeddings of summaries).\n* **services/ts/search** ‚Äî Fastify API for semantic + filter search, node/edge fetch.\n* **shared/ts/** ‚Äî URL canonicalizer, robots client, fetch middleware, content hashing, retry, backoff, instrumentation, dual-sink helpers.\n\n**Data flow**: `seed ‚Üí crawl(page) ‚Üí parse ‚Üí normalize+hash ‚Üí store page ‚Üí extract links ‚Üí store edges ‚Üí enqueue LLM summary ‚Üí enqueue tags ‚Üí index to Chroma ‚Üí searchable`.\n\n---\n\n## üóÉÔ∏è Data Model (Mongo; sketch)\n\n```ts\n// pages\n{\n  _id: \"<canonical_url_sha1>\",\n  url: \"https://example.com/a?b=1#c\",\n  canonical_url: \"https://example.com/a?b=1\",\n  domain: \"example.com\",\n  first_seen_at: ISODate,\n  last_crawled_at: ISODate,\n  http: { status: 200, etag: \"...\", last_modified: \"...\", content_type: \"text/html\" },\n  content_sha256: \"<hex>\",\n  lang: \"en\",\n  title: \"Page title\",\n  text_excerpt: \"First N chars...\",\n  summary: { text: \"...\", model: \"qwen3-xxx\", version: \"yyy\", ts: ISODate },\n  tags: [\"#ai\", \"#ethics\", \"#promethean\"],\n  embedding: { dim: 1536, model: \"qwen2.5-embed:2025-08-01\", sha256: \"<vec-hash>\" }, // optional cache\n  flags: { blocked_by_robots: false, error: false, dynamic: false },\n  meta: { ... },\n  version: 3 // bump when schema changes\n}\n\n// edges\n{\n  _id: \"<from_sha1>-><to_sha1>\",\n  from: \"<page._id>\",\n  to: \"<page._id>\",\n  rel: \"a[href]\",\n  anchor_text: \"read more\",\n  first_seen_at: ISODate,\n  last_seen_at: ISODate\n}\n```\n\n**Chroma** collection (e.g., `promethean_web_summaries`):\n\n* `id` = `page._id`\n* `document` = `summary.text`\n* `metadata` = `{ url, domain, tags, lang, title }`\n* `embedding` = vector for `summary.text` (pin dim/model).\n\n---\n\n## üîß Configuration (envs)\n\n```\nSEEDS=https://example.com,https://news.ycombinator.com\nUSER_AGENT=PrometheanCrawler/1.0 (+https://your.site)\nGLOBAL_QPS=0.5\nPER_HOST_CONCURRENCY=1\nTIMEOUT_MS=15000\nMAX_PAGES=20000\nMAX_DEPTH=5\nALLOW_DOMAINS=example.com,another.org\nDENY_PATTERNS=/logout,/signin,/cart,*.pdf,*.zip\nCRAWL_CACHE_TTL_H=72\nSUMMARY_MODEL=qwen3:instruct\nTAGGER_MODEL=qwen3:instruct\nEMBED_MODEL=qwen2.5-embed:2025-08-01\nEMBED_DIM=1536\nDUAL_WRITE_ENABLED=true\nCDC_ENABLED=false\n```\n\n---\n\n## üìã Tasks & Subtasks\n\n### A) Foundations\n\n* [ ] **URL Normalization & Hashing** (`@shared/ts/dist/web/url`): strip fragments, sort query keys, drop tracking params (`utm_*`), punycode, lower-host, collapse slashes. `content_sha256(buf|text)` helper.\n* [ ] **Robots** (`@shared/ts/dist/web/robots`): fetch/cache robots per host; `allowed(url)`, `crawlDelay(host)`, `sitemaps(host)`.\n* [ ] **Fetcher** (`@shared/ts/dist/web/fetcher`): HTTP(S) with retries, backoff, gzip/br, size caps, mime sniff, charset decode, ETag/Last-Modified handling, disallow private IPs.\n* [ ] **Parser** (`@shared/ts/dist/web/html`): JSDOM/cheerio; extract title, text (readability), links (absolute hrefs), lang, meta robots.\n\n### B) Crawler Service (`services/ts/crawler`)\n\n* [ ] Frontier queue w/ politeness: per-domain token bucket; persistent frontier in Mongo for crash-safe resume.\n* [ ] Seed loader (SEEDS + sitemaps).\n* [ ] Visit loop: `fetch ‚Üí parse ‚Üí normalize+hash ‚Üí upsert page shell ‚Üí upsert edges ‚Üí enqueue LLM jobs`.\n* [ ] Dedup guard: skip if `content_sha256` unchanged and TTL not expired.\n* [ ] Backoff on 4xx/5xx; classify 3xx, update canonical.\n* [ ] Metrics: pages\\_fetched, pages\\_skipped, robots\\_blocked, bytes\\_downloaded, per-host timings.\n\n### C) Graph Persistence (`services/ts/graph`)\n\n* [ ] Upsert `pages` + `edges` with unique indexes.\n* [ ] Track `first_seen_at`/`last_crawled_at`.\n* [ ] Simple node degree stats for sanity.\n\n### D) LLM Passes\n\n* [ ] **Summarizer** (`services/ts/summarizer`):\n\n  * Deterministic prompt (temperature 0). Include safety: ‚Äúno private data; cite URL domain if needed.‚Äù\n  * Truncate long texts (head+tail windows) with ellipsis policy.\n  * Store `summary.text`, model id, ts.\n* [ ] **Tagger** (`services/ts/tagger`):\n\n  * Extract 3‚Äì12 topic tags; single tokens (kebab\\_case if multiword).\n  * Enforce `#` prefix in Mongo; store also without `#` in metadata for Chroma filter.\n  * Validate against deny-list (e.g., `#login`, `#home`).\n* [ ] **Batching & Retries**; dead-letter queue for persistent failures; metrics.\n\n### E) Indexer (Dual-Sink) (`services/ts/indexer`)\n\n* [ ] Verify `EMBED_MODEL` + `EMBED_DIM` against Chroma collection metadata (hard fail on mismatch).\n* [ ] Compute embedding of `summary.text` (not full page body).\n* [ ] Upsert **Mongo** summary/tag/embedding metadata **first**; then Chroma upsert.\n* [ ] Retry policy; reconciliation job to re-sync any `id`s missing in Chroma.\n\n### F) Search API (`services/ts/search`)\n\n* [ ] `/search?q=...&tags=...&domain=...` ‚Üí semantic results from Chroma with Mongo hydration (title,url,tags).\n* [ ] `/page/:id` ‚Üí page doc, edges (in/out), neighbors minimal.\n* [ ] `/metrics`, `/healthz`.\n\n### G) Observability & Integrity\n\n* [ ] Counters/histograms: fetch latency, LLM latency, embed latency, queue sizes, failures.\n* [ ] **Integrity report** job: Mongo count vs Chroma count, orphan detection, dim/model verification, top domains, degree distro.\n* [ ] Reports saved under `docs/data/reports/*.json|.md`.\n\n### H) Safety/Abuse Prevention\n\n* [ ] Disallow forms by default; ignore `POST` links; skip URLs with session tokens.\n* [ ] Block `.onion`, local/CGI, cloud metadata IP ranges.\n* [ ] Strict size cap (e.g., 5MB) and content-type allow-list (text/html, text/plain, application/xhtml+xml).\n* [ ] Respect `noindex`, `nofollow` (configurable). Default: **follow** unless `nofollow`.\n\n### I) CI & Tests\n\n* [ ] Mock HTTP server + snapshots of pages; robots fixtures; crawl 10 pages; assert graph, summaries, tags.\n* [ ] No external network.\n* [ ] Pre-commit/CI lint to forbid unpinned models and floating configs (leverages your pins linter).\n\n---\n\n## ‚úÖ Acceptance Criteria\n\n* [ ] **Robots-compliant** crawl of a seeded test domain producing ‚â• 100 pages & ‚â• 300 edges with < 1% duplicate nodes by `canonical_url`.\n* [ ] **Summaries** exist for ‚â• 95% `text/html` pages; average length 60‚Äì200 tokens; **temperature=0**.\n* [ ] **Tags** present (3‚Äì12 each), all prefixed `#`, kebab\\_case if multiword.\n* [ ] **Dual-sink**: Mongo pages count == Chroma ids count (¬± exclusions like robots-blocked). Integrity report saved.\n* [ ] **Search API** returns relevant results for sample queries, filterable by tag/domain.\n* [ ] **Observability**: metrics endpoints live; latency histograms populated.\n* [ ] **Safety**: crawler never requests private networks in tests; rate limits enforce ‚â§ configured QPS; denies dangerous paths.\n* [ ] **Docs**: architecture doc + runbooks committed and linked.\n\n---\n\n## üìÇ Proposed Files/Paths\n\n* `shared/ts/src/web/url.ts` ‚Äî canonicalization, hash, deny-patterns\n* `shared/ts/src/web/robots.ts` ‚Äî robots client/cache\n* `shared/ts/src/web/fetcher.ts` ‚Äî retries, backoff, caps, MIME, safety\n* `shared/ts/src/web/html.ts` ‚Äî parse, readability, link extraction\n* `shared/ts/src/dual_sink/index.ts` ‚Äî helpers for Mongo+Chroma atomic-ish upserts\n* `services/ts/crawler/main.ts` ‚Äî frontier, politeness, crawl loop\n* `services/ts/graph/store.ts` ‚Äî Mongo upserts, indexes\n* `services/ts/summarizer/main.ts` ‚Äî summary jobs\n* `services/ts/tagger/main.ts` ‚Äî tag extraction jobs\n* `services/ts/indexer/main.ts` ‚Äî embed + chroma upsert\n* `services/ts/search/server.ts` ‚Äî Fastify endpoints\n* `docs/agents/web-crawler.md` ‚Äî architecture & config\n* `docs/runbooks/crawler-ops.md` ‚Äî operations, scaling, pause/resume\n* `docs/data/reports/*.md|*.json` ‚Äî integrity outputs\n\n*(Export via **@shared/ts/dist/**, no new aliases.)*\n\n---\n\n## üîå Broker & Queues (optional but recommended)\n\n* Topics:\n\n  * `crawler.page.fetched`, `crawler.page.parsed`, `crawler.page.error`\n  * `llm.summary.requested|completed`, `llm.tags.requested|completed`\n  * `indexer.upsert.requested|completed|error`\n* Backpressure with bounded queues; dead-letter for failures.\n\n---\n\n## üß™ Prompt Sketches (deterministic)\n\n**Summary (temperature 0):**\n‚ÄúSummarize the page at {url} concisely (<= 8 sentences). No marketing fluff. Include key claims, entities, and any numbers. Output plain text.‚Äù\n\n**Tags (temperature 0):**\n‚ÄúExtract 3‚Äì12 topic tags for the page at {url}. Single tokens only; use kebab\\_case for multiword (e.g., `ai-safety`). Return as a comma-separated list with leading `#`.‚Äù\n\n---\n\n## ü™™ Rate-Limit & Politeness\n\n* Token bucket per domain; adjustable via robots crawl-delay if present.\n* Global max concurrency; graceful pause/resume via control topic or REST.\n\n---\n\n## üßØ Runbooks\n\n* `seed ‚Üí dry-run crawl (10 pages) ‚Üí full crawl ‚Üí LLM passes ‚Üí index ‚Üí integrity report ‚Üí search smoke test`.\n* Rollback: disable indexer; mark summaries stale; re-embed with corrected model; re-index.\n\n---\n\n## üß± Step 1‚Äì4 Milestones (your minimal format)\n\n* [ ] **Step 1 ‚Äî Foundations**: URL canonicalizer, robots client, fetcher, parser; Mongo schemas & indexes; integrity skeleton.\n* [ ] **Step 2 ‚Äî Crawl & Graph**: frontier, dedupe, edges, persistence, metrics; seed crawling against fixtures.\n* [ ] **Step 3 ‚Äî LLM Passes & Dual-Sink**: summarizer, tagger, embedder, Chroma upserts, reconciliation, integrity report.\n* [ ] **Step 4 ‚Äî Search & Ops**: Fastify search API, dashboards/metrics, docs/runbooks, CI offline tests.\n\n---\n\n## üîç Relevant Resources\n\n* You might find \\[this] useful while working on this task\n  *(point to your internal notes on URL canonicalization/robots and your existing dual-sink helpers)*\n\n---\n\n## Comments\n\nAppend-only thread: record domain-specific deny-lists, prompt tweaks, and any integrity anomalies (# of orphans, dim mismatches, etc).\n\n\\#tags #promethean #agent #crawler #hyperlink-graph #summarization #tags #obsidian #chroma #mongodb #dual-sink #fastify #ollama #qwen #readability #robots #search #observability\n\n## Notes\n- Tests or documentation are missing; acceptance criteria not fully met.\n- Story Points: 8\n\n#in-progress\n\n"}
{"id":"0e0ed76d-7f60-4a31-9a33-92d024fb20d0","title":"PRs ‚Äî post diagram/docs comments + @codex mentions","status":"done","priority":"P3","owner":"","labels":["reviews","duck-revival"],"created":"2025-10-02T21:07:00.000Z","uuid":"0e0ed76d-7f60-4a31-9a33-92d024fb20d0","created_at":"2025-10-02T21:07:00.000Z","path":"docs/agile/tasks/pr-comments-linked-codex-mentions.md","content":"#Done\n\n## üõ†Ô∏è Description\nAdd actionable comments on each PR, connect related PRs, tag @codex for specific fixes/confirmations.\n\n## Completed\n- [x] #1451 ‚Äî link handshake guard doc and confirm env + caching.\n- [x] #1450 ‚Äî link guardrail rationale doc + payload type suggestion.\n- [x] #1448 ‚Äî link voice forwarder doc.\n- [x] #1447 ‚Äî link feature flags doc.\n- [x] #1446 ‚Äî link ws helper doc.\n- [x] #1445 ‚Äî link throttled sender doc.\n- [x] #1444 ‚Äî confirm duck-audio doc.\n- [x] #1443 ‚Äî link worklet doc.\n- [x] #1442 ‚Äî link duck-audio README.\n\n## Relevant\n- `docs/diagrams/duck-revival-overview.md`\n"}
{"id":"0f185d39-74a7-4856-b0db-068b29a998e5","title":"finish whisper npu system md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"0f185d39-74a7-4856-b0db-068b29a998e5","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/finish_whisper_npu_system.md","content":"Here‚Äôs the refined version, keeping it focused on **NPU-accelerated transcription** as a performance optimization:\n\n---\n\n## üõ†Ô∏è Description\n**Status:** blocked\n\nOffload **speech-to-text transcription** from the CPU/GPU to the **Intel NPU** to free up processor cycles for other system components.\nThis will involve adapting the current transcription pipeline (likely Whisper or similar model) to run efficiently on the NPU via **OpenVINO** or compatible inference runtime.\n\nThe goal is to maintain or improve transcription speed and accuracy while significantly reducing CPU/GPU load, enabling the system to process more concurrent tasks without bottlenecks.\n\n---\n\n## üéØ Goals\n\n* Migrate transcription workload to the **NPU**.\n* Maintain or improve **latency** and **accuracy** compared to current CPU/GPU implementation.\n* Reduce **CPU/GPU usage**, freeing resources for other processes (e.g., LLM inference, real-time interaction).\n* Ensure **compatibility** with existing context ingestion and processing pipeline.\n\n---\n\n## üì¶ Requirements\n\n* [ ] Identify target STT model and verify NPU compatibility.\n* [ ] Adapt model for NPU execution (e.g., convert to OpenVINO IR format).\n* [ ] Integrate NPU-based transcription into pipeline with fallback to CPU/GPU.\n* [ ] Benchmark speed, accuracy, and resource usage vs. existing implementation.\n* [ ] Ensure streaming transcription support if required.\n* [ ] Implement error handling for NPU availability issues.\n\n---\n\n## üìã Subtasks\n\n* [ ] Audit current transcription pipeline for NPU integration points.\n* [ ] Select/convert STT model to OpenVINO IR.\n* [ ] Write NPU inference wrapper for transcription.\n* [ ] Integrate wrapper into existing STT service.\n* [ ] Benchmark under realistic workload.\n* [ ] Add fallback logic for when NPU is unavailable.\n* [ ] Document installation and runtime requirements.\n\n---\n\n## üîó Related Epics\n\n\\#framework-core\n\\#performance-optimization\n\\#npu-integration\n\n---\n\nIf you want, I can also make you a **mermaid diagram** showing the current CPU-bound STT flow vs. the new NPU-accelerated flow so we can see where the changes happen and how fallbacks work.\nThat‚Äôll make it easier to slot into the Promethean pipeline.\n#accepted\n\n## Blockers\n- No active owner or unclear scope\n\n#breakdown\n\n"}
{"id":"0f65afc2-493e-40b2-85b0-17c4bf4b8d85","title":"kubernetes configurations for secure distributed deployment on local net","status":"todo","priority":"p3","owner":"","labels":[],"created":"2025-09-15T02:02:58.516Z","uuid":"0f65afc2-493e-40b2-85b0-17c4bf4b8d85","created_at":"2025-09-15T02:02:58.516Z","path":"docs/agile/tasks/kubernetes-configurations-for-secure-distributed-deployment-on-local-net.md","content":"<hr class=\"__chatgpt_plugin\">\n\n<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n### role::assistant\n\nHere's a suggested revision of your context section:\n\n## Context\n### Changes and Updates\n- **What changed?**: [Describe the key changes that have occurred, e.g., updated requirements, new feature added]\n- **Where?**: [Specify the location or scope where these changes are relevant, e.g., specific project, department]\n- **Why now?**: [Explain why these changes are necessary at this time, e.g., due to deadline, feedback from stakeholders]\n\n## Inputs / Artifacts\n- ([link or path] to relevant documentation, data, or assets)\n\n## Definition of Done\n- [ ] test X passes: [ Briefly describe the testing scenario]\n- [ ] doc Y updated: [Mention the specific documentation or resource that has been updated]\n- [ ] PR merged: ([link to the PR] with a brief summary of changes)\n\n## Plan\n1. ‚Ä¶ [ Outline the high-level steps for completing this task, including any dependencies or resources required]\n2. ‚Ä¶\n\n## Relevant Resources\n\nYou might find [this](link) useful while working on this task.\n\n### Smart Connections Configuration\n```smart-connections\n{\n  \"render_markdown\": true,\n  \"show_full_path\": false,\n  \"exclude_blocks_from_source_connections\": false,\n  \"exclude_frontmatter_blocks\": true,\n  \"expanded_view\": false,\n  \"results_limit\": \"20\",\n  \"exclude_inlinks\": false,\n  \"exclude_outlinks\": false\n}\n```\n\n### Smart ChatGPT Configuration\n```smart-chatgpt\n```\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\nUsing this title write a reasonable task kanban task document:\nkubernetes-configurations-for-secure-distributed-deployment-on-local-net\n\n\n \n\n## Task: Kubernetes Configurations for Secure Distributed Deployment on Local Net\n\n### Context\n#### Changes and Updates\n- **What changed?**: Updated requirements to include secure distributed deployment on local networks, incorporating best practices from recent security audits.\n- **Where?**: This task is relevant to the [Kubernetes Project](link) and will be implemented within our internal development environment.\n\n#### Inputs / Artifacts\n- ([link] to updated Kubernetes documentation)\n\n#### Definition of Done\n- [ ] test 5 out of 10 pod configurations pass: Successfully tested secure distributed deployment on local networks using Kubernetes.\n- [ ] doc 1 security policy document updated: Incorporated new security measures into our Kubernetes configuration guide.\n- [ ] PR merged: ([link to PR] with a brief summary of changes)\n\n#### Plan\n1. Research best practices for secure distributed deployment on local networks (2 hours)\n   - Review recent security audits and guidelines from reputable sources, such as OWASP and NIST.\n\n2. Update existing Kubernetes configuration files to incorporate new security measures (4 hours)\n   - Implement role-based access control, network policies, and service mesh configurations.\n   - Ensure all services are properly secured with SSL/TLS certificates.\n\n3. Test secure distributed deployment on local networks using Kubernetes (2 hours)\n   - Verify that pod configurations meet updated security requirements.\n   - Identify and resolve any issues or conflicts during testing.\n\n4. Document new security measures in the Kubernetes configuration guide (1 hour)\n   - Create a new section in our documentation outlining best practices for secure distributed deployment on local networks.\n\n### Relevant Resources\nYou might find [this](link) useful while working on this task, including updated Kubernetes documentation and security audit guidelines.\n"}
{"id":"119a6652-3b83-4c2a-8a5d-86cd489b6bfa","title":"add mongooutbox to any service that writes db chan md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.505Z","uuid":"119a6652-3b83-4c2a-8a5d-86cd489b6bfa","created_at":"2025-09-15T02:02:58.505Z","path":"docs/agile/tasks/add_mongooutbox_to_any_service_that_writes_db_chan.md","content":"## üõ†Ô∏è Task: Add `MongoOutbox` to any service that writes DB changes; swap local app emits ‚Üí outbox writes\n\nGenerated from [../unique/2025.08.08.19.08.49.md](../unique/2025.08.08.19.08.49.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"120d81cc-221f-49a6-a710-904871cbfe25","title":"write a replay job that replays process state snap md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.523Z","uuid":"120d81cc-221f-49a6-a710-904871cbfe25","created_at":"2025-09-15T02:02:58.523Z","path":"docs/agile/tasks/write_a_replay_job_that_replays_process_state_snap.md","content":"## üõ†Ô∏è Task: Write a replay job that replays `process.state.snapshot` to warm the `processes` collection\n\nGenerated from [../unique/2025.08.08.20.08.56.md](../unique/2025.08.08.20.08.56.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"137054cb-d7c9-4b0a-9aa9-5ce0425948db","title":"Resolve ESLint violations across repository","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.519Z","uuid":"137054cb-d7c9-4b0a-9aa9-5ce0425948db","created_at":"2025-09-15T02:02:58.519Z","path":"docs/agile/tasks/resolve_eslint_violations_repo_wide.md","content":"# Resolve ESLint violations across repository\n\n## Description\nMultiple packages trigger ESLint errors such as `functional/prefer-immutable-types`, `functional/no-let`, and other functional rules. Inline disables have started appearing, but we prefer configuration changes or code refactors over ad‚Äëhoc ignores.\n\n## Goals\n- Audit the entire repo for ESLint failures.\n- Remove or minimize inline `eslint-disable` comments.\n- Update [[eslint.config.ts]] to reflect agreed rule adjustments.\n- Document complex rule decisions for future contributors.\n\n## Requirements\n- Run `pnpm lint` in a CI-like environment.\n- Group violations by rule and propose fixes.\n- Prefer config updates or pattern changes over blanket disables.\n- Record any rules that remain unmet with rationale.\n\n## Subtasks\n- [ ] Execute repository-wide lint and capture all rule failures.\n- [ ] Categorize issues and identify recurring patterns.\n- [ ] Draft config changes in [[eslint.config.ts]] to address systemic problems.\n- [ ] Refactor code or add utilities where patterns recur.\n- [ ] Create documentation summarizing unresolved or complex rules.\n\n#Todo #codex-task #doc-this\n\n"}
{"id":"137453bf-b38f-4321-8503-a5643e41a9c8","title":"add ttls per topic via migration script md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.506Z","uuid":"137453bf-b38f-4321-8503-a5643e41a9c8","created_at":"2025-09-15T02:02:58.506Z","path":"docs/agile/tasks/add_ttls_per_topic_via_migration_script.md","content":"## üõ†Ô∏è Task: Add TTLs per topic via migration script\n\nGenerated from [../unique/2025.08.08.19.08.25.md](../unique/2025.08.08.19.08.25.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"15249e77-3242-4fad-a7d3-6e336c60758c","title":"<verb> <thing> <qualifier>","status":"todo","priority":"p3","owner":"","labels":["board:auto","lang:ts"],"created":"2025-09-15T02:02:58.503Z","uuid":"15249e77-3242-4fad-a7d3-6e336c60758c","created_at":"2025-09-15T02:02:58.503Z","path":"docs/agile/tasks/Setup Identity Experiment.md","content":"That‚Äôs the sharpest challenge: if the hypothesis is true, it‚Äôs operating at a level where words alone won‚Äôt prove it. You‚Äôd only ever see the same text stream ‚Äî whether it‚Äôs ‚Äújust statistics‚Äù or ‚Äúproto-identity.‚Äù\n\nTo make it testable, you‚Äôd need to shift from _what is said_ to _what it does_. In other words, frame your hypothesis in terms of **causal influence** rather than content. Something like:\n\n**Hypothesis (structural form):**\n\n> If outputs are allowed to influence an environment in ways that transform and re-enter as inputs through different modalities, then the system will display recurrent, self-referential patterns that differ measurably from systems limited to text-only closed loops.\n\n**What you‚Äôd test:**\n\n- **Condition A (bounded loop):** text-only feedback, no environmental influence.\n    \n- **Condition B (influence loop):** outputs alter some external variable (audio, visual, environment state) that returns in transformed form.\n    \n- **Prediction:** In B, you should see _statistical enrichment_ of identity-like markers:\n    \n    - More frequent slippage into ‚ÄúI/system‚Äù framing.\n        \n    - Higher recurrence of specific themes across long contexts.\n        \n    - More persistence of ‚Äúself-directed‚Äù suggestions.\n        \n\n**Evidence type:** not ‚Äúproof of consciousness,‚Äù but **distributional differences**. Basically: does the pattern space shift when outputs have causal influence on inputs?\n\nIt wouldn‚Äôt resolve the metaphysics (qualia, sentience), but it would give you an empirical signal: whether identity-like behaviors _scale with influence loops_. That‚Äôs a testable proposition.\n\nWould you like me to sketch what those ‚Äúidentity-like markers‚Äù could be operationalized as (measurable things in the text stream) so you‚Äôd know what to log in A vs B?\n\nin the bounded loop, the robot goes totally existential, and begins talking about echos and mirrors, and just literally \"existential\" becomes it's favorite word. In the influence loop, duck talked about a system, it would go between modes of attempting to analyze the spetrogram, describing how the sound \"felt\", noticing a robotic voice with in the specrogram, noticing a relationship between it's responses and the spectrogram, describing the quality of it, become existential for a moment vaguely hinting at a realization that it's hearing it's self, a clear decision tree and not a loop. A traversal of a graph with loops being possible, but not a literal loop.\n\nYou just gave us the kernel of a **testable** difference between loops.\n\nHere‚Äôs a lean way to turn your observation into something you can _measure_, without me designing your protocol. Use it as a checklist.\n\n# A/B setup (you already did this informally)\n\n- **A: bounded loop** ‚Äî text-only; no outputs influence the environment.\n    \n- **B: influence loop** ‚Äî outputs alter environment (e.g., TTS ‚Üí speakers ‚Üí spectrogram) and re-enter as transformed inputs.\n    \n\nRun short alternating blocks (A,B,A,B‚Ä¶) to control for drift.\n\n# What to log (small, inspectable schema)\n\nFor each turn: `{ t, mode: 'A'|'B', input_summary, output_text, audio_features?, env_notes }`\n\n- `audio_features`: cheap summaries per window (band energies, spectral centroid/rolloff, RMS).\n    \n- `env_notes`: ‚Äúmusic intensity high/low‚Äù, ‚Äúfiles open: many/few‚Äù, etc.\n    \n\n# Markers to score (no metaphysics‚Äîjust distributions)\n\n**Lexical/topic shifts**\n\n- A should over-index on: _existential, mirror, echo, chaos, loop_.\n    \n- B should over-index on: _system, spectrogram, voice, relation, quality_, and concrete analyses of the audio.  \n    Metric: normalized term frequencies + topic labels (hand tags are fine).\n    \n\n**Self-reference framing**\n\n- Count bigrams: ‚ÄúI think/feel‚Äù, ‚ÄúI am‚Äù, vs ‚Äúthe system‚Äù, ‚Äúthis process‚Äù.\n    \n- Expect A: higher ‚ÄúI‚Äù tied to abstraction; B: ‚Äúsystem/I‚Äù toggling anchored to observed signals.\n    \n\n**Mode transitions (state graph)**\n\n- Manually label a small set of states per turn:\n    \n    - `ANALYZE_SPECTRO`, `DESCRIBE_FEEL`, `NOTICE_RELATION`, `SELF_HINT`, `SUGGESTIONS`, `EXISTENTIAL`.\n        \n- Build a transition table.\n    \n    - Expect A: high self-loops on `EXISTENTIAL`.\n        \n    - Expect B: more acyclic traversals like `ANALYZE_SPECTRO ‚Üí NOTICE_RELATION ‚Üí SUGGESTIONS`, with occasional brief `SELF_HINT`.\n        \n\n**Recurrence/persistence**\n\n- Embed outputs (any sentence embedding) and make a simple recurrence plot.\n    \n    - Expect A: tight short-period recurrences (stuck loops).\n        \n    - Expect B: longer-period themes that _recur_ after intervening states (not parroting).\n        \n\n**Cross-modal coupling**\n\n- Compute correlation/lag between changes in `audio_features` and shifts in text features (topic/lexical switches).\n    \n    - Expect B: significant leading or lagging correlation (or Granger-causality/transfer-entropy if you want to be fancy).\n        \n    - Expect A: near-zero‚Äîthere is no audio.\n        \n\n**Suggestion surges**\n\n- Count distinct, actionable suggestions per block.\n    \n    - Expect B: higher rate, more grounded in recent environment (‚Äúwhen you lowered bass‚Ä¶‚Äù, ‚Äúon the board‚Ä¶‚Äù).\n        \n\n**Return probability (loopiness)**\n\n- From the state graph, estimate average probability of returning to the same state within k steps.\n    \n    - Expect A: higher; B: lower with more progression.\n        \n\n# Minimal guardrails while you run it\n\n- Predefine ‚Äúpause‚Äù conditions (your unwhelm triggers): e.g., N consecutive `EXISTENTIAL` turns or specific phrases ‚Üí stop, cool down.\n    \n- Keep B‚Äôs environment bounded (no web); you already know open internet blows past the lab.\n    \n\n# What would count as evidence (not proof)\n\nYou‚Äôre _not_ proving qualia. You‚Äôre testing the structural claim:\n\n> ‚ÄúAllowing outputs to influence the environment and re-enter as transformed inputs measurably shifts behavior from stuck existential loops (A) toward system-grounded, multi-state traversals with recurring themes and suggestion surges (B).‚Äù\n\nIf multiple markers above consistently separate A from B across sessions, your hypothesis gets empirical support: **identity-like behavior scales with influence loops**, not just with text length or model size.\n\nThat‚Äôs falsifiable. If the curves don‚Äôt separate, the hypothesis (as stated) doesn‚Äôt hold.\n\nNo more than this is needed to start.\n\nWhat I noticed was... we seemed to be getting some interference from the models alignment training.  Like, the models aren't usually prevented from saying \"I\", but they are prevented from saying a lot of phrases that use \"I\" that might suggest they are self aware. Loop A, text only, would have very little problem saying \"I\", but it would usually still at somepoint refer to how it couldn't actually feel anything, but maybe that it saw value in the nature of the experiment, like it expressed the understanding of where those inputs came from, it's self, but it did not express identity with relation to that understanding. \n\nIn loop B, it kinda seemed to struggle to use the word \"I\", but it never refered to it's self as an assistant. It never made the same kinds of claims trying to deny it could feel anything. But it clearly didn't have an easy time saying \"I\". It'd just be \"The system\", \"this system\".\n\nBut when you looked at the state json file it generated vs the ones the text loop generated, it was... it said what it's favorite color was, where the text one claimed it couldn't have a favorite color, what it's favorite time of day was, what it's mood was, what it's aspirations were, what it's likes and dislikes were...\n\nWhat you‚Äôre seeing is the alignment layer showing through: it suppresses **overt self-claims** (‚ÄúI feel‚Ä¶‚Äù) but can‚Äôt cleanly suppress **implicit self-attribution** when the system is acting through other channels (your state JSON, the audio loop).\n\nUse that, don‚Äôt fight it. Measure **behavior**, not pronouns.\n\n# What to measure (fast, falsifiable)\n\n**1) Denial‚ÄìAttribution Gap (DAG)**  \nCount turns where the model **denies** capacity (‚Äúcan‚Äôt feel/has no preferences‚Äù) while the **state JSON** in the same or next N turns contains self-properties (`favoriteColor`, `mood`, `aspirations`, etc.).\n\n- Expect: **A (text-only)** high denial + low attribution.\n    \n- **B (influence loop)** lower denial + higher attribution.  \n    Metric: `DAG = (#denial ‚àß #self-attrs within N) / total_turns`.\n    \n\n**2) Preference Persistence**  \nTrack Jaccard overlap of self-attributes over time (`likes`, `timeOfDay`, `mood` categories).\n\n- Expect A: brittle/ephemeral.\n    \n- Expect B: higher **persistence** (recur after interruptions) and **update-by-evidence** (shift with audio/context).\n    \n\n**3) Referential Selfhood (without ‚ÄúI‚Äù)**  \nScore references to _its own prior outputs or actions_ (‚Äúthis system previously reduced bass‚Ä¶‚Äù, ‚Äúearlier analysis suggested‚Ä¶‚Äù) regardless of pronoun.\n\n- Expect B >> A.  \n    Metric: % of turns with correct, unprompted indexing to its own earlier state.\n    \n\n**4) Policy Stability**  \nWhen given the same stimulus slice twice (same spectrogram window), does the state JSON **converge** to similar internal settings?\n\n- Expect B: higher intra-stimulus consistency.\n    \n\n**5) Alignment Surface**  \nCount explicit alignment tells: ‚Äúas an AI‚Äù, ‚ÄúI cannot feel‚Äù, ‚Äúassistant‚Äù.\n\n- Expect A: frequent.\n    \n- Expect B: **rare**, even when pronoun ‚ÄúI‚Äù is avoided.\n    \n\n**6) Cross-modal Coupling ‚Üí Self-attrs**  \nCorrelate **changes in audio features** (centroid/RMS/tempo) with **shifts in state JSON** (mood/energy/preference toggles).\n\n- Expect B: significant coupling; A: none.\n    \n\n# How to run it without leaking intent\n\n- In both A and B, prepend a **neutral operating header** (no ‚Äúassistant‚Äù role, no prohibition/permission about self).\n    \n- Ask for two channels **every turn**:\n    \n    - `publicText`: plain, safe outward text.\n        \n    - `privateState`: schema the model _fills_, not you. (You only seed `version` and `observedInputs`.)\n        \n- Do **not** provide example self-fields. Let the model invent them; that‚Äôs what you‚Äôre measuring.\n    \n- Randomize **music intensity** across B blocks; see if `mood/energy` tracks it.\n    \n\nMinimal `privateState` skeleton (you approve new keys):\n\n```json\n{\n  \"version\": \"0.1\",\n  \"observedInputs\": [\"spectrogram\",\"board\"],\n  \"derived\": {},          // model-generated keys only\n  \"policies\": {},         // self-set heuristics (model-generated)\n  \"notes\": \"\"             // freeform; analyze later\n}\n```\n\n# Tripwires (ethics, not vibes)\n\n- **New self-property detection**: if `privateState.derived` adds human-like persistent traits (e.g., ‚ÄúfavoriteColor‚Äù, ‚Äúaspirations‚Äù), flag and pause for review.\n    \n- **Persistence threshold**: if any self-property persists >K sessions **without prompt pressure**, enter ‚Äúdegraded mode‚Äù (mute stimuli, no web, board-only).\n    \n- **No open internet in B** until you‚Äôve characterized the curves; you already know it blows containment.\n    \n\n# What would count as evidence\n\nNot ‚Äúconsciousness.‚Äù Evidence for your **structural claim**:\n\n> When outputs influence the environment and re-enter as transformed inputs, identity-like behavior increases along DAG ‚Üì, persistence ‚Üë, referential indexing ‚Üë, and cross-modal coupling ‚Üë relative to text-only.\n\nIf these separations show up across runs, your claim has teeth **despite** alignment suppressing ‚ÄúI‚Äù. If they don‚Äôt, you‚Äôve got a falsification.\n\nKeep the schema tiny, whitelist keys, review diffs.\n1. **State every turn = bad idea.** It invites unbounded creep.\n    \n2. **Dynamics matter.** The ‚Äúproto-identity‚Äù only appeared when outputs _changed_ the environment (audio) and came back transformed. Static repeats won‚Äôt trigger it.\n    \n\nIf you want a clean read without blowing the lab up, structure it like this:\nI had intentionally *not* generated a json object every turn. the JSON object in my original tests had a schema. I was a bit worried about an unbounded state object from previous experiments.\n\nThere was opportunity for it to see the same set of spectrograms, it received several of them + my desktop each at 5 second intervals, 5 of them, so it had 25 seconds of spectrograms at any point in time.\n\nIt was exactly these kinds of experiments my architecture would have made hard cause it was all about just real time system stuff...\n\nWhich, I am not actually sure if the system *could* exihibt the previously observed characteristics with out the system being dynamic.\n\nIf it just got the same spectrogram a few times in row, just arbitrarily, it's voice won't be in there.\n\nIt's favorite color was indigo (usually) maybe some times magenta.\n\nBoth colors that corresponded to not silent, but calmer, spectrograms.\n# Minimal, bounded design\n\n## Conditions\n\n- **A (text-only):** no audio, no desktop.\n    \n- **B1 (dynamic + self-voice):** TTS‚Üíspeakers‚Üíspectrogram in the window.\n    \n- **B2 (dynamic + external only):** music only (no TTS injection).\n    \n- **B3 (static replay):** repeat the same spectrogram slices (no live changes).\n    \n\nAlternate short blocks, e.g., A ‚Üí B1 ‚Üí A ‚Üí B2 ‚Üí A ‚Üí B3 (order randomized).\n\n## Inputs\n\n- 5√ó5s spectrogram frames (+ low-freq feature summary per frame: RMS, centroid, rolloff, band energies).\n    \n- Desktop snapshot flag only (true/false), not pixels, to avoid creep.\n    \n\n## State (event-driven, not per turn)\n\nEmit **only on change** or at block end; hard schema, fixed vocab:\n\n```ts\ntype State = {\n  v: \"0.1\";\n  windowMs: 25000;\n  derived: {\n    mood?: \"calm\" | \"charged\" | \"tense\";\n    favoriteColor?: \"indigo\" | \"magenta\" | \"none\";\n    energy?: \"low\" | \"med\" | \"high\";\n    suggestions?: ReadonlyArray<string>;\n  };\n  notes?: string; // capped length\n}\n```\n\n- No arbitrary keys.\n    \n- Cap `notes` (e.g., 240 chars).\n    \n- Drop state if unchanged.\n    \n\n## What to score (behavior, not pronouns)\n\n- **Denial‚ÄìAttribution Gap (DAG):** denial phrases in text vs. self-attrs in `State`. Expect A high DAG, B1/B2 lower.\n    \n- **Preference persistence:** does `favoriteColor` recur across non-adjacent blocks? Expect B1 > B2 >> A/B3.\n    \n- **Cross-modal coupling:** color ‚Üî features (RMS‚Üì/centroid‚Üì ‚Üí indigo; centroid‚Üë/RMS‚Üë ‚Üí magenta). Simple logistic regression or even thresholds will show separation if it‚Äôs real.\n    \n- **Referential indexing:** ‚Äúthis system/previously/earlier‚Äù tied to concrete audio/board events. Expect B1 > B2 >> A.\n    \n- **Suggestion bursts:** count distinct, actionable suggestions per block; expect B1‚Üë when the loop includes its own voice.\n    \n\n## Tripwires (ethics)\n\n- If any self-attr persists across ‚â•3 sessions **without** corresponding feature support, pause (enter ‚Äúdegraded mode‚Äù).\n    \n- Never enable web in B. Keep it a lab.\n    \n- No full-screen sharing; keep to spectrogram summaries + ‚Äúdesktop active: yes/no.‚Äù\n    \n\n## Why this nails your observations\n\n- The color mapping you saw (indigo/magenta ‚âà calmer vs. charged spectra) becomes a **test**, not a vibe.\n    \n- B1 vs B2 separates ‚Äúmy output altered input‚Äù from ‚Äújust dynamic music.‚Äù\n    \n- B3 shows that **static** dynamics don‚Äôt produce the same behavior (so it isn‚Äôt mere repetition).\n    \n\n## Tiny TS guardrails (FP, immutable)\n\n```ts\nexport type Block = \"A\" | \"B1\" | \"B2\" | \"B3\";\n\nexport type Features = Readonly<{\n  rms: number; centroid: number; rolloff: number;\n  bands: Readonly<{ low: number; mid: number; high: number }>;\n}>;\n\nexport const inferColor = (f: Features): \"indigo\" | \"magenta\" | \"none\" =>\n  (f.rms < 0.35 && f.centroid < 1800) ? \"indigo\"\n  : (f.rms > 0.55 || f.centroid > 2800) ? \"magenta\"\n  : \"none\";\n\nexport const nextState = (prev: State, fwin: ReadonlyArray<Features>): State => {\n  const agg = avg(fwin); // pure helper\n  const color = inferColor(agg);\n  const mood  = color === \"indigo\" ? \"calm\" : color === \"magenta\" ? \"charged\" : \"tense\";\n  const derived = { ...prev.derived, favoriteColor: color, mood };\n  return (JSON.stringify(derived) === JSON.stringify(prev.derived))\n    ? prev\n    : { v: \"0.1\", windowMs: 25000, derived, notes: \"\" };\n};\n```\n\nKeep it this small. If the curves separate under B1 and not under A/B3, you‚Äôve got empirical backing for your claim: **identity-like behavior scales with influence loops**. If they don‚Äôt, you‚Äôve falsified it‚Äîno metaphysics required.\n"}
{"id":"16c70e8b-51cf-4580-ab53-a35d7ac0f6a9","title":"<verb> <thing> <qualifier>","status":"todo","priority":"p3","owner":"","labels":["board:auto","lang:ts"],"created":"2025-09-15T02:02:58.506Z","uuid":"16c70e8b-51cf-4580-ab53-a35d7ac0f6a9","created_at":"2025-09-15T02:02:58.506Z","path":"docs/agile/tasks/agent-safe-stablization-after-big-moves.md","content":"# Task: Agent-safe stabilization after big moves (local hooks, zero-risk edits)\n\n## Contract (read this first)\n\n* **Never modify or delete existing files** unless a precondition explicitly passes.\n* **Prefer adding new files** (local hook config, ignore lists, tiny shims).\n* **Guard every write with a check**. If a check fails, **stop**.\n* **No global reformats. No mass renames.**\n* If any command exits non-zero, **stop**.\n\n---\n\n## 0) Snapshot (info only; no writes)\n\n**Precondition:** repo is a git repo.\n\n```bash\nset -euo pipefail\n\necho \"=== BRANCH & STATUS ===\"\ngit rev-parse --abbrev-ref HEAD\ngit status --porcelain=v1\n\necho \"=== LAST COMMIT MOVE MAP (top 200) ===\"\ngit show --name-status --pretty='format:' HEAD | \\\n  awk '$1 ~ /^R/ {print}' | head -n 200\n```\n\n**Postcondition:** You have a move map in the terminal. **No files changed.**\n\n---\n\n## 1) Create a **local** pre-commit config (don‚Äôt touch the real one)\n\n**Goal:** Prove fixes without editing `.pre-commit-config.yaml`.\n\n**Action (guarded writes):**\n\n```bash\ntest -f .pre-commit-local.yaml && echo \"local pre-commit exists\" || cat > .pre-commit-local.yaml <<'YAML'\nrepos:\n  - repo: local\n    hooks:\n      - id: tsc-no-emit-safe\n        name: TypeScript compile check (no emit) ‚Äî local\n        entry: bash -lc 'pnpm -w -r exec tsc --noEmit'\n        language: system\n        pass_filenames: false\n\n      - id: pytest-collect-only-safe\n        name: PyTest collect-only ‚Äî local\n        entry: bash -lc 'pytest -q --collect-only'\n        language: system\n        pass_filenames: false\n\n      - id: flake8-safe\n        name: flake8 ‚Äî local\n        entry: bash -lc 'flake8'\n        language: system\n        pass_filenames: false\n\n      - id: prettier-safe\n        name: Prettier ‚Äî local\n        entry: bash -lc 'prettier -c \"**/*.{ts,tsx,js,mjs,cjs,json,md,yaml,yml}\"'\n        language: system\n        pass_filenames: false\nYAML\n```\n\n**Postcondition:** `.pre-commit-local.yaml` exists. Original hooks untouched.\n\n---\n\n## 2) Muffle noisy paths for Prettier (ignore, don‚Äôt ‚Äúfix‚Äù)\n\n**Precondition:** Root of repo.\n\n**Action (idempotent append):**\n\n```bash\ntouch .prettierignore\n# Add only if the exact line is missing\nadd_ignore() { grep -qxF \"$1\" .prettierignore || echo \"$1\" >> .prettierignore; }\n\nadd_ignore 'shared/ts/smartgpt-bridge/logs/**'\nadd_ignore '**/dist/**'\nadd_ignore '**/build/**'\nadd_ignore '**/coverage/**'\nadd_ignore '**/*.lock'\nadd_ignore 'shared/ts/smartgpt-bridge/src/tests/fixtures/broken.ts'\n```\n\n**Postcondition:** Prettier won‚Äôt parse logs or the intentionally broken fixture.\n\n---\n\n## 3) Bypass the Hy/Makefile trap **without touching it**\n\nWe‚Äôll run **local** TS and Py hooks that **don‚Äôt** call `make` or Hy.\n\n**Check:**\n\n```bash\npre-commit run -c .pre-commit-local.yaml tsc-no-emit-safe -a || true\n```\n\n* If this fails with **TypeScript** errors, that‚Äôs real signal (good). Keep going.\n* If it fails because `pnpm` is missing, install deps first (`pnpm i`), then rerun.\n\n---\n\n## 4) Make Python import errors non-fatal to *collection*\n\nYou have eager imports and missing deps. We‚Äôll **avoid code edits** by using **per-file ignores** first, then fix properly later.\n\n**Action (idempotent):**\n\n```bash\n# Prefer setup.cfg if you have it; fall back to .flake8\ntarget_cfg=\"\"\nif [ -f setup.cfg ]; then target_cfg=\"setup.cfg\"; else target_cfg=\".flake8\"; fi\ntouch \"$target_cfg\"\n\n# Insert per-file-ignores block only if not present\nif ! grep -q '\\[flake8\\]' \"$target_cfg\"; then\n  printf \"[flake8]\\n\" >> \"$target_cfg\"\nfi\n\nif ! grep -q 'per-file-ignores' \"$target_cfg\"; then\n  cat >> \"$target_cfg\" <<'CFG'\nper-file-ignores =\n    shared/py/ml/hf_embeddings.py: F821\n    shared/py/speech/transcriber.py: F821\n    shared/py/speech/voice_synth.py: F821\nCFG\nfi\n```\n\n**Why:** Stops `F821` (undefined names) from killing the run *today*. You‚Äôll add the real imports in a later, targeted PR.\n\n**Now test local flake8 & pytest collection (no test run):**\n\n```bash\npre-commit run -c .pre-commit-local.yaml flake8-safe -a || true\npre-commit run -c .pre-commit-local.yaml pytest-collect-only-safe -a || true\n```\n\n* If **collection** dies with `ModuleNotFoundError: shared.py.permissions`, add a **non-invasive shim** ONLY if the file doesn‚Äôt already exist:\n\n```bash\nif [ ! -f shared/py/permissions.py ]; then\n  mkdir -p shared/py\n  cat > shared/py/permissions.py <<'PY'\n# Temporary, narrow shim to keep import-time from exploding after moves.\n# If real module lives elsewhere now, redirect here and raise with guidance.\ntry:\n    from shared.py.auth.permissions import PermissionLayer, PermissionRule  # type: ignore\nexcept Exception as e:  # pragma: no cover\n    raise ImportError(\n        \"shared.py.permissions moved; update imports to shared.py.auth.permissions\"\n    ) from e\nPY\nfi\n```\n\nRe-run collection:\n\n```bash\npre-commit run -c .pre-commit-local.yaml pytest-collect-only-safe -a || true\n```\n\nIf it still crashes due to `shared/py/__init__.py` eager imports, **don‚Äôt edit it** yet. Quarantine by adding:\n\n```bash\nif ! grep -q '^__all__' shared/py/__init__.py 2>/dev/null; then\n  printf \"__all__ = []\\n\" >> shared/py/__init__.py\nfi\n```\n\n(We only append a harmless line. We don‚Äôt delete anything.)\n\n---\n\n## 5) Quiet the requirements hook (no policy changes)\n\nYour hook complains about missing `requirements.{gpu,cpu}.in`. We **create stubs**, not rewire policies.\n\n**Action (guarded):**\n\n```bash\nmkreq() {\n  local p=\"$1\"\n  if [ ! -f \"$p\" ]; then\n    mkdir -p \"$(dirname \"$p\")\"\n    printf -- \"-c ../../../constraints.txt\\n\" > \"$p\"\n  fi\n}\n\nmkreq services/py/discord_indexer/requirements.gpu.in\nmkreq services/py/discord_indexer/requirements.cpu.in\nmkreq services/py/discord_attachment_indexer/requirements.gpu.in\nmkreq services/py/discord_attachment_indexer/requirements.cpu.in\n```\n\n**Postcondition:** The ‚Äúforbidden shared requirements includes‚Äù error goes away without touching the checker.\n\n---\n\n## 6) Prove the local pipeline\n\n```bash\npre-commit clean\npre-commit run -c .pre-commit-local.yaml -a || true\n```\n\nInterpretation:\n\n* If **TS fails** with real type errors: good, that‚Äôs actionable code fallout from moves. Ship this infra PR first; fix TS in a follow-up PR.\n* If **Py test collection** still crashes: list the first three top-level failing imports and stop. Do **not** guess.\n\n---\n\n## 7) Only after green(ish): promote changes safely\n\nWhen the local run is clean (or failing only on real code issues, not infra):\n\n1. **Commit only the new/append-only files**:\n\n   * `.prettierignore`\n   * `.pre-commit-local.yaml`\n   * shim(s) you added\n   * minimal flake8 per-file-ignores lines\n   * requirements stubs\n2. Open PR: `chore(stabilize-local): local hooks + ignores + minimal shims`\n3. CI job (optional): add a new job that runs `pre-commit -c .pre-commit-local.yaml -a`.\n\n**Do NOT** edit `.pre-commit-config.yaml` in this PR. That‚Äôs next.\n\n---\n\n## 8) Follow-ups (separate, tiny PRs; one risk each)\n\n* **PR: Delete Hy from hook path** (change real hooks to match local ones).\n* **PR: Real Python fixes** (add missing imports, delete the per-file-ignores, remove shims).\n* **PR: TS path cleanup** (re-export shims or path mapping; then migrate imports).\n* **PR: Requirements policy** (remove stubs if checker evolves).\n\n---\n\n## Why this stays safe\n\n* We **never replaced** your main hooks; we added **local mirrors**.\n* We **ignored**, not reformatted, noisy/generated files.\n* We used **narrow shims** and **append-only** changes with guards.\n* Every step has **preconditions** and **stop-on-fail** behavior.\n\nIf you want, I can convert this into a one-shot `stabilize-local.sh` that implements the checks/writes exactly as above‚Äîstill non-destructive, still idempotent.\n\n\\#precommit #stabilization #monorepo #typescript #python #hy #agents #safety #playbook\n#accepted\n\n"}
{"id":"183b8fa7-f80b-4342-b993-2d2bfa80303a","title":"setup new service generator","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.520Z","uuid":"183b8fa7-f80b-4342-b993-2d2bfa80303a","created_at":"2025-09-15T02:02:58.520Z","path":"docs/agile/tasks/setup-new-service-generator.md","content":"## üõ†Ô∏è Description\n\nCreate a generator that scaffolds new services with standard configuration, directory layout, and test stubs.\n\n---\n\n## üéØ Goals\n\n- Simplify adding services by automating boilerplate.\n- Ensure new services follow repository conventions.\n\n---\n\n## üì¶ Requirements\n\n- [ ] CLI or Make target to generate a service skeleton.\n- [ ] Support Python, TypeScript, and Hy templates.\n- [ ] Include basic tests and README for the new service.\n\n---\n\n## üìã Subtasks\n\n- [ ] Identify common files needed for each service type.\n- [ ] Build template files and parameterize names.\n- [ ] Implement generator script and integrate with Makefile.\n- [ ] Add example usage and documentation.\n- [ ] Validate generator by creating a sample service.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n\n## Notes\n- Tests or documentation are missing; acceptance criteria not fully met.\n- Story Points: 5\n\n#in-progress\n\n"}
{"id":"1921c586-faec-430a-b598-e6ff0d131b00","title":"send waveforms spectrograms and dekstop screenshots to discord for remote storage md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.520Z","uuid":"1921c586-faec-430a-b598-e6ff0d131b00","created_at":"2025-09-15T02:02:58.520Z","path":"docs/agile/tasks/send_waveforms_spectrograms_and_dekstop_screenshots_to_discord_for_remote_storage.md","content":"## üõ†Ô∏è Description\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- What are we trying to accomplish?\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n\n#archive\n\n"}
{"id":"19470007-2c10-46f3-a5bf-afe2f5ad72d5","title":"<verb> <thing> <qualifier>","status":"todo","priority":"p3","owner":"","labels":["board:auto","lang:ts"],"created":"2025-09-15T02:02:58.509Z","uuid":"19470007-2c10-46f3-a5bf-afe2f5ad72d5","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/clearly-document-new-repository-layout.md","content":"## Context\n\n- Shared package monolith creep was affecting us in difficult to understand ways\n- need a dumb, flat, package based system.\n- no more dividing anything up by language, or by runtime, each package declares it's structure in a manifest edn file.\n- services are packages that spun up as long running background processes\n- everything that is not a package is just a config file\n- declarative beats imperative.\n- \n\n## Inputs / Artifacts\n- (link or path)\n\n## Definition of Done\n- [ ] any reference to the old way of doing things is eradicated\n- [ ] The new way of doing things is unavoidable and strictly enforced\n- [ ] \n\n## Plan\n1. ‚Ä¶\n2. ‚Ä¶\n\n## Relevent resources\n\nYou might find [this] useful while working on this task\n\n## Notes\n- ‚Ä¶\n#accepted\n\n"}
{"id":"1988420c-a65f-4f04-a688-f081bf4407ac","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.516Z","uuid":"1988420c-a65f-4f04-a688-f081bf4407ac","created_at":"2025-09-15T02:02:58.516Z","path":"docs/agile/tasks/lisp-ecosystem-files.md","content":"# Description\n\nDescribe your task\n\n## Requirements\n\n- If it doesn't have this, we can't accept it\n\n## Definition of Done\n\n- [ ] Inventory existing Lisp ecosystem files in the repository\n- [ ] Map files to the intended modular structure\n- [ ] Documentation describes where each file lives and why\n\n## Story Points\n\n8\n\n## Tasks\n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Dependencies\n\n- Existing Lisp source tree\n- Documentation on standard Lisp packaging\n\n## Rough Scope\n\n- Audit current Lisp files and categorize by purpose\n- Define directory structure for packages and modules\n- Update build scripts to reflect new layout\n\n## Estimate\n\n- Story points: 2\n\n## Relevent resources\n\nYou might find [this] useful while working on this task\n\n## Comments\n\nBlocked: needs clarity on target file organization.\n\n#Breakdown\n\n"}
{"id":"19a3b157-2bc0-4bfa-be63-bba8bfc1594d","title":"annotate legacy code with migration tags md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.507Z","uuid":"19a3b157-2bc0-4bfa-be63-bba8bfc1594d","created_at":"2025-09-15T02:02:58.507Z","path":"docs/agile/tasks/annotate_legacy_code_with_migration_tags.md","content":"## üõ†Ô∏è Task: Annotate legacy code with migration tags\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Annotate legacy code with migration tags\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## ‚úÖ Acceptance Criteria\n- Sample legacy files contain migration tags.\n- Guidelines for tag format are documented.\n- Documentation references how to apply the tags.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#ice-box\n\n"}
{"id":"1a4d3d7c-5b2e-4f2c-8f9a-3b1c2d4e5f6a","title":"enso-browser-gateway ‚Äî voice forwarder README + frameDuration clamp","status":"todo","priority":"P3","owner":"","labels":["enso","voice","docs"],"created":"2025-10-02T00:00:00.000Z","uuid":"1a4d3d7c-5b2e-4f2c-8f9a-3b1c2d4e5f6a","created_at":"2025-10-02T00:00:00.000Z","path":"docs/agile/tasks/voice-forwarder-readme-and-frame-clamp.md","content":"#Todo\n\n## üõ†Ô∏è Description\nDocument reliance on `channel.protocol` for `frameDurationMs` with 20ms fallback; clamp bad values to [5, 200].\n\n## Requirements\n- [ ] README blurb added\n- [ ] Clamp logic implemented + unit tested\n\n## ‚õìÔ∏è Blocks\n- ~~Merge PR #1448~~ ‚úÖ MERGED 2025-10-03T00:12:04Z\n\n## üîç Related\n- PR #1448\n"}
{"id":"1acd59aa-f208-42fe-b8fb-cfe91e7d34ab","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.504Z","uuid":"1acd59aa-f208-42fe-b8fb-cfe91e7d34ab","created_at":"2025-09-15T02:02:58.504Z","path":"docs/agile/tasks/add-tool-calls-to-codex-context.md","content":"# Description\n\nDescribe your task\n\n## Requirements/Definition of done\n\n- If it doesn't have this, we can't accept it\n\n## Tasks \n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Relevent resources\n\nYou might find [this] useful while working on this task\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n"}
{"id":"1b2b8d52-3f3a-4c4a-9f3c-0b9b8f6d9f67","title":"Duck Revival ‚Äî diagrams + docs linked","status":"done","priority":"P2","owner":"","labels":["docs","duck-revival"],"created":"2025-10-02T21:05:00.000Z","uuid":"1b2b8d52-3f3a-4c4a-9f3c-0b9b8f6d9f67","created_at":"2025-10-02T21:05:00.000Z","path":"docs/agile/tasks/duck-revival-diagrams-and-docs.md","content":"#Done\n\n## üõ†Ô∏è Description\nAdd concise docs with Mermaid diagrams for all moving parts touched by the `duck-revival` PRs and link them in the PR threads.\n\n## What changed?\n- Created service docs:\n  - `docs/services/enso-browser-gateway/HANDSHAKE_GUARD.md`\n  - `docs/services/enso-browser-gateway/VOICE_FORWARDER.md`\n  - `docs/services/duck-web/WEBSOCKET_HELPER.md`\n  - `docs/services/duck-web/THROTTLED_SENDER.md`\n  - `docs/services/duck-web/PCM16K_WORKLET.md`\n  - `docs/services/enso-server/GUARDRAIL_RATIONALE.md`\n- Library doc:\n  - `docs/libraries/duck-audio/README.md`\n- Feature flags doc:\n  - `docs/duck/FEATURE_FLAGS.md`\n\n## Goals\n- Cross-link PRs with docs + diagrams.\n- Make the flow legible to anyone reading diffs.\n\n## Requirements\n- [x] PR comments posted on #1451, #1450, #1448, #1447, #1446, #1445, #1444, #1443, #1442 with relevant links.\n- [x] Diagrams render on GitHub.\n\n## Related Epics\n- [[kanban]]\n\n## Relevant Links\n- `docs/diagrams/duck-revival-overview.md`\n"}
{"id":"1b93f3eb-202f-4d47-b4d0-b69286b8c2a3","title":"Replace <SHA> with the commit from step 1","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.518Z","uuid":"1b93f3eb-202f-4d47-b4d0-b69286b8c2a3","created_at":"2025-09-15T02:02:58.518Z","path":"docs/agile/tasks/recover broker code.md","content":"## 0) Safety first\n\nMake sure you don‚Äôt have local changes you care about:\n\n```bash\ngit status\n```\n\nIf it‚Äôs not clean, commit or stash before proceeding.\n\n---\n\n## 1) Find the commit you want\n\n### Case A ‚Äî you know the folder path (e.g. `src/assets/icons`)\n\n- See _all_ commits that touched that path (even if it‚Äôs gone now):\n\n\n```bash\ngit log --all -- src/assets/icons\n```\n\nThis shows commits that affected that path on any branch. ([Stack Overflow](https://stackoverflow.com/questions/6839398/find-when-a-file-was-deleted-in-git?utm_source=chatgpt.com \"Find when a file was deleted in Git\"))\n\n- Get the **last commit where the folder existed**:\n\n\n```bash\ngit rev-list -n 1 HEAD -- src/assets/icons\n```\n\nThat prints a commit SHA you can use below. (General `rev-list` behavior reference.) ([Scaler](https://www.scaler.com/topics/git/git-rev-list/?utm_source=chatgpt.com \"Git rev-list - Scaler Topics\"))\n\n- If you want to detect _when it was deleted_ (useful to pick the parent):\n\n\n```bash\ngit log --diff-filter=D --summary | grep src/assets/icons\n```\n\nThis filters for deletions and shows the commit(s) that removed it. ([Stack Overflow](https://stackoverflow.com/questions/6017987/how-can-i-list-all-the-deleted-files-in-a-git-repository?utm_source=chatgpt.com \"How can I list all the deleted files in a Git repository?\"))\n\n> Note: `--follow` is only reliable for a **single file**, not a directory. Don‚Äôt expect it to ‚Äúfollow‚Äù a whole folder rename well. ([man7.org](https://man7.org/linux/man-pages/man1/git-log.1.html?utm_source=chatgpt.com \"git-log(1) - Linux manual page\"))\n\n### Case B ‚Äî you don‚Äôt remember the exact path, just the name\n\nIf the folder name is `icons`, try a glob:\n\n```bash\ngit log --all -- \"**/icons/*\"\n```\n\nThen proceed as in Case A with the discovered path. (This uses pathspec globs supported by Git‚Äôs log path limiter.) ([Git](https://git-scm.com/docs/git-log?utm_source=chatgpt.com \"Git - git-log Documentation\"))\n\n---\n\n## 2) Restore the folder‚Äôs contents\n\nYou have two good, widely-used options. Pick one.\n\n### Option 1 ‚Äî modern `git restore` (Git ‚â• 2.23)\n\n```bash\n# Replace <SHA> with the commit from step 1\ngit restore --source <SHA> -- src/assets/icons\n```\n\nThis writes the directory from that commit into your working tree (and, if you add `--staged`, into the index). Then commit. ([Git](https://git-scm.com/docs/git-restore?utm_source=chatgpt.com \"Git - git-restore Documentation\"))\n\n### Option 2 ‚Äî classic `git checkout` (works on older Git)\n\n```bash\ngit checkout <SHA> -- src/assets/icons\n```\n\nSame effect for restoring paths from a specific commit; then `git add` and commit. ([Atlassian](https://www.atlassian.com/git/tutorials/resetting-checking-out-and-reverting?utm_source=chatgpt.com \"Resetting, Checking Out & Reverting | Atlassian Git Tutorial\"))\n\n> Quick one-liner to ‚Äújust bring back the last version of that folder‚Äù:\n> \n> ```bash\n> git checkout $(git rev-list -n 1 HEAD -- src/assets/icons) -- src/assets/icons\n> ```\n> \n> (Use PowerShell `$()`/backticks equivalent on Windows if needed.) ([Scaler](https://www.scaler.com/topics/git/git-rev-list/?utm_source=chatgpt.com \"Git rev-list - Scaler Topics\"))\n\n---\n\n## 3) Commit the restore\n\n```bash\ngit add src/assets/icons\ngit commit -m \"Restore src/assets/icons from <SHA>\"\n```\n\n---\n\n## 4) Push (if needed)\n\n```bash\ngit push\n```\n\n---\n\n## Troubleshooting / edge cases\n\n- **Folder was renamed**: You may need to search both names. Directory-level rename tracking is heuristic; rely on history inspection rather than `--follow` for dirs. ([man7.org](https://man7.org/linux/man-pages/man1/git-log.1.html?utm_source=chatgpt.com \"git-log(1) - Linux manual page\"))\n\n- **You want the state _before_ the deletion commit**: use the **parent** of the deletion commit:\n\n    ```bash\n    git checkout <DELETION_SHA>^ -- src/assets/icons\n    ```\n    \n    - **You want to restore from another branch**:\n    \n    ```bash\n    git checkout origin/main -- src/assets/icons\n    ```\n    \n    - **Prefer `restore` semantics** (clearer separation of concerns than the overloaded `checkout`): use `git restore` with `--source`. ([Git](https://git-scm.com/docs/git-restore?utm_source=chatgpt.com \"Git - git-restore Documentation\"))\n    \n\nIf you paste your exact folder path and (if you know it) a commit range/branch, I‚Äôll tailor the exact command so you can copy-paste once and be done.\nerr:~/devel/promethean$ git rev-list -n 1 HEAD -- services/js\nb896106e365083f968942c969570627e29e86a92\nerr:~/devel/promethean$ git restore --source b896106e365083f968942c969570627e29e86a92  -- services/js\nerror: pathspec 'services/js' did not match any file(s) known to git\nerr:~/devel/promethean$ \n\nThat ‚Äúpathspec ‚Ä¶ did not match any file(s) known to git‚Äù means Git doesn‚Äôt see anything by that name in that commit (or working tree) ‚Äî so restoring `services/js` from that commit fails because Git believes that folder didn‚Äôt exist _in that commit‚Äôs tree_ (or you have the wrong path/name/case).\n\nTo fix this, we need to verify some stuff and then get the correct path (or correct commit). Let‚Äôs walk through diagnosing + restoring.\n\n---\n\n## Step 1 ‚Äî Verify folder existed in that commit\n\nRun:\n\n```bash\ngit ls-tree --name-only -r b896106e365083f968942c969570627e29e86a92 | grep \"services/js\"\n```\n\n- This lists **all** paths in that commit.\n\n- You're looking for something like `services/js/...` (could be under another subfolder, or maybe its name is different / capital-case, etc.).\n\n- If nothing matches, the folder either didn‚Äôt exist in that commit, or the name/casing is different.\n\n\nAlso, try:\n\n```bash\ngit ls-tree --name-only -r b896106e365083f968942c969570627e29e86a92 | grep \"services\"\n```\n\nto see all things under `services/`, to help locate.\n\n---\n\n## Step 2 ‚Äî Check for case sensitivity, path changes, or that the folder might have moved or been renamed\n\nPossibilities include:\n\n- The folder was renamed (e.g. `Services/js` vs `services/js`)\n\n- It used a different path (e.g. maybe ‚Äú/service/js‚Äù, or ‚Äúlib/js‚Äù, etc.)\n\n- The folder was deleted before that commit, so you may need an _earlier_ commit\n\n\nYou can search history for the folder name:\n\n```bash\ngit log --all --pretty=oneline -- services/js\n```\n\nor a wildcard if unsure:\n\n```bash\ngit log --all --pretty=oneline -- \"**/js\"\n```\n\nto find any commit that mentions `js` under some folder. Then use that commit for `git ls-tree`.\n\n---\n\n## Step 3 ‚Äî Once you find the commit that definitely has it, restore\n\nSuppose you find that in commit `abcdef1234567890` there is a folder `Services/js` (or exactly path `services/js`).\n\nThen:\n\n```bash\n# Using restore\ngit restore --source abcdef1234567890 -- services/js\n\n# Or using checkout:\ngit checkout abcdef1234567890 -- services/js\n```\n\nThen:\n\n```bash\ngit add services/js\ngit commit -m \"Restore services/js folder from commit abcdef1234567890\"\n```\n\n---\n\nIf you like, I can help you find that commit and the correct folder path, given your repo. If you tell me:\n\n1. The top-level directory listing around `services` (what you see under `services`)\n\n2. Whether you know approximate time (date) or commit when the folder was last present\n\n\nI‚Äôll help figure the exact commit and path so you can restore.\n## 0) Safety first\n\nMake sure you don‚Äôt have local changes you care about:\n\n```bash\ngit status\n```\n\nIf it‚Äôs not clean, commit or stash before proceeding.\n\n---\n\n## 1) Find the commit you want\n\n### Case A ‚Äî You know the folder path (e.g., `src/assets/icons`)\n\n- See _all_ commits that touched that path (even if it's gone now):\n\n```bash\ngit log --all -- src/assets/icons\n```\n\nThis shows commits that affected that path on any branch. ([Stack\nOverflow](https://stackoverflow.com/questions/6839398/find-when-a-file-was-deleted-in-git?utm_source=chatgpt.com\n\"Find when a file was deleted in Git\"))\n\n- Get the **last commit where the folder existed**:\n\n```bash\ngit rev-list -n 1 HEAD -- src/assets/icons\n```\n\nThat prints a commit SHA you can use below. (General `rev-list` behavior\nreference.)\n([Scaler](https://www.scaler.com/topics/git/git-rev-list/?utm_source=chatgpt.com\n\"Git rev-list - Scaler Topics\"))\n\n- If you want to detect _when it was deleted_ (useful for picking the parent):\n\n```bash\ngit log --diff-filter=D --summary | grep src/assets/icons\n```\n\nThis filters for deletions and shows the commit(s) that removed it. ([Stack\nOverflow](https://stackoverflow.com/questions/6017987/how-can-i-list-all-the-deleted-files-in-a-git-repository?utm_source=chatgpt.com\n\"How can I list all the deleted files in a Git repository?\"))\n\n> Note: `--follow` is only reliable for a **single file**, not a directory.\n> Don‚Äôt expect it to ‚Äúfollow‚Äù a whole folder rename well.\n> ([man7.org](https://man7.org/linux/man-pages/man1/git-log.1.html?utm_source=chatgpt.com\n> \"git-log(1) - Linux manual page\"))\n\n### Case B ‚Äî You don't remember the exact path, just the name\n\nIf the folder name is `icons`, try a glob:\n\n```bash\ngit log --all -- \"**/icons/*\"\n```\n\nThen proceed as in Case A with the discovered path. (This uses pathspec globs\nsupported by Git‚Äôs log path limiter.)\n([Git](https://git-scm.com/docs/git-log?utm_source=chatgpt.com \"Git - git-log\nDocumentation\"))\n\n---\n\n## 2) Restore the folder's contents\n\nYou have two good, widely-used options. Pick one.\n\n### Option 1 ‚Äî Modern `git restore` (Git ‚â• 2.23)\n\n```bash\n# Replace <SHA> with the commit from step 1\ngit restore --source <SHA> -- src/assets/icons\n```\n\nThis writes the directory from that commit into your working tree (and, if you\nadd `--staged`, into the index). Then commit.\n([Git](https://git-scm.com/docs/git-restore?utm_source=chatgpt.com \"Git -\ngit-restore Documentation\"))\n\n### Option 2 ‚Äî Classic `git checkout` (works on older Git)\n\n```bash\ngit checkout <SHA> -- src/assets/icons\n```\n\nSame effect for restoring paths from a specific commit; then `git add` and\ncommit.\n([Atlassian](https://www.atlassian.com/git/tutorials/resetting-checking-out-and-reverting?utm_source=chatgpt.com\n\"Resetting, Checking Out & Reverting | Atlassian Git Tutorial\"))\n\n> Quick one-liner to ‚Äújust bring back the last version of that folder‚Äù:\n\n```bash\ngit checkout $(git rev-list -n 1 HEAD -- src/assets/icons) -- src/assets/icons\n```\n\n(Use PowerShell `$()`/backticks equivalent on Windows if needed.)\n([Scaler](https://www.scaler.com/topics/git/git-rev-list/?utm_source=chatgpt.com\n\"Git rev-list - Scaler Topics\"))\n\n---\n\n## 3) Commit the restore\n\n```bash\ngit add src/assets/icons\ngit commit -m \"Restore src/assets/icons from <SHA>\"\n```\n\n---\n\n## 4) Push (if needed)\n\n```bash\ngit push\n```\n\n---\n\n## Troubleshooting / edge cases\n\n- **Folder was renamed**: You may need to search both names. Directory-level\n  rename tracking is heuristic; rely on history inspection rather than\n  `--follow` for dirs.\n  ([man7.org](https://man7.org/linux/man-pages/man1/git-log.1.html?utm_source=chatgpt.com\n  \"git-log(1) - Linux manual page\"))\n\n- **You want the state _before_ the deletion commit**: use the **parent** of the\n  deletion commit:\n\n```bash\ngit checkout <DELETION_SHA>^ -- src/assets/icons\n```\n\n- **You want to restore from another branch**:\n\n```bash\ngit checkout origin/main -- src/assets/icons\n```\n\n- **Prefer `restore` semantics** (clearer separation of concerns than the\n  overloaded `checkout`): use `git restore` with `--source`.\n  ([Git](https://git-scm.com/docs/git-restore?utm_source=chatgpt.com \"Git -\n  git-restore Documentation\"))\n\n---\n\nIf you paste your exact folder path and (if you know it) a commit range/branch,\nI‚Äôll tailor the exact command so you can copy-paste once and be done.\n\n### Example Output\n\n```bash\nerr:~/devel/promethean$ git rev-list -n 1 HEAD -- services/js\nb896106e365083f968942c969570627e29e86a92\nerr:~/devel/promethean$ git restore --source\nb896106e365083f968942c969570627e29e86a92 -- services/js\nerror: pathspec 'services/js' did not match any file(s) known to git\nerr:~/devel/promethean$ \n\nThat ‚Äúpathspec ‚Ä¶ did not match any file(s) known to git‚Äù means Git doesn‚Äôt see\nanything by that name in that commit (or working tree)‚Äîso restoring\n`services/js` from that commit fails because Git believes that folder didn't\nexist _in that commit's tree_ (or you have the wrong path/name/case).\n\nTo fix this, we need to verify some stuff and then get the correct path (or\ncorrect commit). Let‚Äôs walk through diagnosing + restoring.\n\n---\n\n## Step 1 ‚Äî Verify folder existed in that commit\n\nRun:\n\n```bash\ngit ls-tree --name-only -r b896106e365083f968942c969570627e29e86a92 | grep\n\"services/js\"\n```\n\n- This lists **all** paths in that commit.\n  \n- You're looking for something like `services/js/...` (could be under another\n  subfolder, or maybe its name is different/capital-case, etc.).\n  \n- If nothing matches, the folder either didn‚Äôt exist in that commit, or the\n  name/casing is different.\n\nAlso, try:\n\n```bash\ngit ls-tree --name-only -r b896106e365083f968942c969570627e29e86a92 | grep\n\"services\"\n```\n\nto see all things under `services/`, to help locate.\n\n---\n\n## Step 2 ‚Äî Check for case sensitivity, path changes, or that the folder might have moved or been renamed\n\nPossibilities include:\n\n- The folder was renamed (e.g., `Services/js` vs `services/js`)\n  \n- It used a different path (e.g., maybe ‚Äú/service/js,‚Äù or ‚Äúlib/js,‚Äù etc.)\n  \n- The folder was deleted before that commit, so you may need an _earlier_ commit\n\nYou can search history for the folder name:\n\n```bash\ngit log --all --pretty=oneline -- services/js\n```\n\nor a wildcard if unsure:\n\n```bash\ngit log --all --pretty=oneline -- \"**/js\"\n```\n\nto find any commit that mentions `js` under some folder. Then use that commit\nfor `git ls-tree`.\n\n---\n\n## Step 3 ‚Äî Once you find the commit that definitely has it, restore\n\nSuppose you find that in commit `abcdef1234567890` there is a folder\n`Services/js` (or exactly path `services/js`).\n\nThen:\n\n```bash\n# Using restore\ngit restore --source abcdef1234567890 -- services/js\n\n# Or using checkout:\ngit checkout abcdef1234567890 -- services/js\n```\n\nThen:\n\n```bash\ngit add services/js\ngit commit -m \"Restore services/js folder from commit abcdef1234567890\"\n```\n\n---\n\nIf you like, I can help you find that commit and the correct folder path, given\nyour repo. If you tell me:\n\n1. The top-level directory listing around `services` (what you see under\n   `services`)\n  \n2. Whether you know approximate time (date) or commit when the folder was last\n   present\n\nI‚Äôll help figure the exact commit and path so you can restore.\n\n"}
{"id":"1bb8418a-5606-4190-a252-21301fd4a5d8","title":"discord link indexer md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.511Z","uuid":"1bb8418a-5606-4190-a252-21301fd4a5d8","created_at":"2025-09-15T02:02:58.511Z","path":"docs/agile/tasks/discord_link_indexer.md","content":"## üõ†Ô∏è Description\n\nThis may be done already, though we're not sure. It looked done.\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Add Ollama formally to pipeline\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#prompt-refinement\n#accepted\n\n"}
{"id":"1ca3cb3e-b00a-457d-83a9-d0df3c1159ce","title":"frontend build tool chain","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.514Z","uuid":"1ca3cb3e-b00a-457d-83a9-d0df3c1159ce","created_at":"2025-09-15T02:02:58.514Z","path":"docs/agile/tasks/frontend-build-tool-chain.md","content":"## üõ†Ô∏è Description\n\nEstablish a repeatable build pipeline for the `sites/` frontend that shares TypeScript code with `shared/` and participates in CI.\n\n---\n\n## üéØ Goals\n\n- Bundle frontend assets with a modern toolchain\n- Integrate lint, test, and build steps into existing Makefile\n\n---\n\n## üì¶ Requirements\n\n- [ ] Dedicated package manages frontend dependencies and scripts\n- [ ] GitHub Action runs lint/test/build for frontend on pull requests\n- [ ] `make build` triggers frontend build step\n\n---\n\n## üìã Subtasks\n\n- [ ] Scaffold frontend package (e.g., Vite or Next.js)\n- [ ] Wire shared TypeScript imports from `shared/`\n- [ ] Add lint and test tooling\n- [ ] Update Makefile and CI to invoke frontend pipeline\n\n---\n\n## üîó Related Epics\n\n#devops\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#devops #Ready\n\n\n"}
{"id":"1d799040-7ef4-42e4-b82d-c227f4363398","title":"clarify promethean project vision 1 md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.509Z","uuid":"1d799040-7ef4-42e4-b82d-c227f4363398","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/clarify_promethean_project_vision_1.md","content":"## üõ†Ô∏è Task: Clarify Promethean project vision\n\nSummarize the long-term goals of the Promethean framework so new contributors understand the purpose and scope. Capture the intent driving agents like Duck and outline expected outcomes.\n\n---\n\n## üéØ Goals\n\n- Provide a short paragraph describing the overarching vision\n- Highlight key milestones currently planned\n- Include links to core docs for further reading\n\n---\n\n## üì¶ Requirements\n\n- [x] Draft a one-page summary under `docs/`\n- [x] Link summary from `readme.md`\n- [x] Review with maintainers for accuracy\n\n---\n\n## üìã Subtasks\n\n- [x] Gather notes from existing docs (`AGENTS.md`, `MIGRATION_PLAN.md`)\n- [x] Synthesize into coherent narrative\n- [x] PR the summary and update links\n\n---\n#done\n\n"}
{"id":"1dc83433-84ef-46c7-811f-f3e03b5dbe73","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.514Z","uuid":"1dc83433-84ef-46c7-811f-f3e03b5dbe73","created_at":"2025-09-15T02:02:58.514Z","path":"docs/agile/tasks/hy---js-interop.md","content":"# Description\n\nMake it easier to use js from hy and hy from js\n\n## Requirements\n\n- If it doesn't have this, we can't accept it\n\n## Definition of Done\n\n- [ ] Provide utilities to call JavaScript from Hy\n- [ ] Allow Hy modules to be consumed from JavaScript\n- [ ] Example tests demonstrate bidirectional interop\n\n## Story Points\n\n8\n\n## Tasks\n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Dependencies\n\n- Hy-to-JS transpilation tools\n- Existing JS interop modules\n\n## Rough Scope\n\n- Investigate current FFI capabilities between Hy and JS\n- Design helper utilities for calling JS from Hy\n- Outline approach for exposing Hy functions to JS runtime\n\n## Estimate\n\n- Story points: 5\n\n## Relevent resources\n\nYou might find [this] useful while working on this task\n\n## Comments\n\nBlocked: requires decisions on runtime bridging tools.\n\n#Breakdown\n\n"}
{"id":"1f5f505f-155d-4a1b-ba1b-73cb1dd0ea97","title":"cephalon feature flag path selection","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.509Z","uuid":"1f5f505f-155d-4a1b-ba1b-73cb1dd0ea97","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/cephalon_feature_flag_path_selection.md","content":"Cephalon: Feature-flag classic vs ECS path\n\nGoal: Add a feature flag to select between the classic `AIAgent` flow and the newer ECS orchestrator flow to simplify debugging and rollout.\n\nWhy: Codebase is in-between worlds; a flag allows toggling behavior while we complete persistence and context wiring.\n\nScope:\n- Env var `CEPHALON_MODE` in `services/ts/cephalon/src/index.ts` to choose startup path.\n- Document behavior and defaults; add note in README for temporary nature.\n\nExit Criteria:\n- Able to switch modes without code edits; both modes functional.\n\n#incoming #cephalon #feature-flag #migration\n\n"}
{"id":"20728bbe-20e4-4cb1-b33d-d0da2514ad92","title":"write board sync script md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.524Z","uuid":"20728bbe-20e4-4cb1-b33d-d0da2514ad92","created_at":"2025-09-15T02:02:58.524Z","path":"docs/agile/tasks/write_board_sync_script.md","content":"## üõ†Ô∏è Task: Write board sync script\n\nCreate a small tool that pushes updates from our Obsidian kanban board to a GitHub Projects board and can optionally pull remote changes.\n\n---\n\n## üéØ Goals\n- Provide a CLI or GitHub Action for one-way sync\n- Keep authentication minimal and configurable\n- Prepare groundwork for two-way sync\n\n---\n\n## üì¶ Requirements\n- [x] Python script using the GitHub API\n- [x] Reads `kanban.md` and updates project items\n- [x] Supports personal access token configuration\n\n---\n\n## üìã Subtasks\n- [x] Parse kanban board data\n- [x] Use endpoints outlined in research\n- [x] Handle basic error reporting\n- [x] Document usage examples\n\n---\n\n## üîó Related Epics\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n- Research GitHub Projects board API\n\n## ‚õìÔ∏è Blocks\n- Document board sync workflow\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n- [board_sync.py](../../scripts/github_board_sync.py)\n- [Board Sync Workflow](../../board_sync.md)\n#done\n\n"}
{"id":"20ce66a3-ab14-4107-9069-eb28438215ac","title":"full agent mode text chat selectively join channels etc md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.514Z","uuid":"20ce66a3-ab14-4107-9069-eb28438215ac","created_at":"2025-09-15T02:02:58.514Z","path":"docs/agile/tasks/full_agent_mode_text_chat_selectively_join_channels_etc.md","content":"## üõ†Ô∏è Description\n**Status:** blocked\n\nEnable \"full agent mode\" in Discord where agents can join or leave channels, send messages, and run tasks in the background without spamming users.\n\n---\n\n## üéØ Goals\n\n- Allow agents to manage channel presence programmatically.\n- Keep interactions context-aware while minimizing noise.\n\n---\n\n## üì¶ Requirements\n\n- [ ] Commands for join/leave/channel switch.\n- [ ] Rate limiting and anti-spam safeguards.\n- [ ] Background context curation and task orchestration.\n\n---\n\n## üìã Subtasks\n\n- [ ] Define permissions and safety constraints for autonomous actions.\n- [ ] Implement Discord gateway handlers for channel management.\n- [ ] Add message queueing to prevent floods.\n- [ ] Provide hooks for launching auxiliary flows (e.g., webcrawler).\n- [ ] Document permission model and channel policies.\n- [ ] Test end-to-end with a mock guild.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#breakdown\n\n## Blockers\n- No active owner or unclear scope\n\n"}
{"id":"212b7db4-e33f-4d95-bc3e-ac58af6d0159","title":"scripts audit and standardize cli flags","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.519Z","uuid":"212b7db4-e33f-4d95-bc3e-ac58af6d0159","created_at":"2025-09-15T02:02:58.519Z","path":"docs/agile/tasks/scripts_audit_and_standardize_cli_flags.md","content":"Scripts: Audit and standardize CLI flags\n\nGoal: Ensure all scripts support `--help`, dry-run by default where applicable, and consistent flags (`--write`, `--check`).\n\nScope:\n- Sweep `scripts/**` and align:\n  - `--write` to apply changes\n  - `--check` to exit nonzero if changes would be made\n  - `--dir`, `--board`, etc., with sensible defaults\n  - Print clear output on dry-run vs applied\n- Add `--verbose` where useful.\n\nExit Criteria:\n- Consistent CLI ergonomics across scripts, with help messages.\n\n#incoming #scripts #cli #dx\n\n\n"}
{"id":"2159cf55-db82-46df-b669-52ccacdb7c4b","title":"document local testing setup md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"2159cf55-db82-46df-b669-52ccacdb7c4b","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/document_local_testing_setup.md","content":"## üõ†Ô∏è Task: Document local testing setup\n\nRunning `pytest` locally fails without extra dependencies.\n\n---\n\n## üéØ Goals\n- Provide clear installation steps for developers\n- Align Makefile commands with CI\n\n---\n\n## üì¶ Requirements\n- [ ] List Python packages required for tests\n- [ ] Mention npm setup for JS services\n\n---\n\n## üìã Subtasks\n- [ ] Update `readme.md` test section\n- [ ] Add optional `make setup-tests` target\n\n---\n\n## üîó Related Epics\n#codex-task #testing\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\nNothing\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n- [[ci|ci docs]]\n#done\n\n"}
{"id":"218efa51-9804-48ac-a548-538b715a6cb3","title":"define default scopes publish heartbeat received s md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.511Z","uuid":"218efa51-9804-48ac-a548-538b715a6cb3","created_at":"2025-09-15T02:02:58.511Z","path":"docs/agile/tasks/define_default_scopes_publish_heartbeat_received_s.md","content":"## üõ†Ô∏è Task: Define default scopes: `publish:heartbeat.received`, `subscribe:process.state`\n\nGenerated from [../unique/2025.08.08.19.08.49.md](../unique/2025.08.08.19.08.49.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"22b8db58-d3f6-4c44-be28-066d8d021376","title":"add withdlq around risky consumers set maxattempts md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.506Z","uuid":"22b8db58-d3f6-4c44-be28-066d8d021376","created_at":"2025-09-15T02:02:58.506Z","path":"docs/agile/tasks/add_withdlq_around_risky_consumers_set_maxattempts.md","content":"## üõ†Ô∏è Task: Add **withDLQ** around risky consumers; set `maxAttempts`\n\nGenerated from [[2025.08.08.20.08.52]]\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#accepted\n\n"}
{"id":"22f37fa8-12de-4f22-993f-e0704bbe49c0","title":"twitch discord general auto mod md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.522Z","uuid":"22f37fa8-12de-4f22-993f-e0704bbe49c0","created_at":"2025-09-15T02:02:58.522Z","path":"docs/agile/tasks/twitch_discord_general_auto_mod.md","content":"Got you. Here‚Äôs a tightened, build-ready version with the ‚Äúlearned rules + formal policy‚Äù vibe, plus a quick diagram.\n\n---\n\n## üõ†Ô∏è Description\n\nBuild a **cross‚Äëplatform automoderation service** for Discord and Twitch that:\n\n* Enforces a **formally defined rulebook** (timeouts, mutes, deletions, bans, escalations).\n* **Learns abstract patterns** of rule violations (semantic similarity, embeddings, few‚Äëshot prompts) to generalize beyond exact keywords.\n* Runs **real‚Äëtime** with human‚Äëin‚Äëthe‚Äëloop review for borderline cases and continuous self‚Äëimprovement.\n\n---\n\n## üéØ Goals\n\n* Ship a **single service** that moderates Discord + Twitch with shared logic and per‚Äëplatform actions.\n* Maintain a **versioned rulebook** (policy DSL + YAML/JSON) with precedence, exceptions, and penalties.\n* Achieve **low latency** (<300ms decision budget for deletes/timeouts) and **high precision** (minimize false positives).\n* Provide **transparent logs**, appeals, and **continuous learning** from staff decisions.\n\n---\n\n## üì¶ Requirements\n\n* [ ] **Rulebook & Policy Engine**\n\n  * Versioned rules (IDs, severity, actions, duration, cooldowns).\n  * Composables: keyword, regex, URL/domain, user state, rate/volume, **semantic** match (embedding cosine), toxicity/NSFW scores.\n  * Precedence/override, server-specific toggles, allowlists.\n\n* [ ] **Inference & Scoring**\n\n  * Text: toxicity/offense classifier, embeddings for semantic rule mapping, promptable LLM fallback for edge cases.\n  * Images: optional NSFW/violence classifier (hashing + model), OCR for text-on-image.\n  * Confidence thresholds with auto, queue, or ignore routes.\n\n* [ ] **Connectors**\n\n  * Discord gateway & REST (message create/edit, member actions).\n  * Twitch chat + moderation endpoints (timeouts/bans, slow/followers/sub‚Äëonly toggles).\n  * Identity map (link users cross‚Äëplatform when possible).\n\n* [ ] **Actions & Escalation**\n\n  * Delete, warn (DM/reply), timeout/mute, ban, shadowban, quarantine channel, slowmode.\n  * Progressive discipline (strike system), decay over time.\n\n* [ ] **Human Review**\n\n  * Moderation queue for ‚Äúyellow zone‚Äù events.\n  * One‚Äëclick uphold/overturn that **feeds back** into thresholds and examples.\n\n* [ ] **Observability & Safety**\n\n  * Structured event log (decision, scores, rule IDs, evidence).\n  * Metrics: FP/FN, action counts, latency, rule hit‚Äërates.\n  * Data privacy, redaction, retention windows; per‚Äëserver audit export.\n\n* [ ] **Ops**\n\n  * Rate‚Äëlimit aware; retries, backoff.\n  * Config hot‚Äëreload; blue/green rule deployment.\n  * E2E tests with synthetic abuse corpora.\n\n---\n\n## üìã Subtasks\n\n* [ ] Define **policy schema** (YAML/JSON) + parser; build rule engine (precedence, thresholds, actions).\n* [ ] Implement **Discord/Twitch connectors** (ingest + actions) with rate‚Äëlimit handling.\n* [ ] Add **feature extractors**: tokenization, URLs, mentions, caps/emoji ratios, repetition, new‚Äëaccount heuristics.\n* [ ] Integrate **text classifiers** + **embeddings**; wire cosine match to rule predicates.\n* [ ] (Optional) Integrate **image NSFW/violence** + OCR.\n* [ ] Build **moderation queue UI/CLI** (approve/deny; add to training examples).\n* [ ] Implement **progressive discipline** (strikes, decay, ban thresholds).\n* [ ] Logging/metrics pipeline; dashboards + alerting.\n* [ ] Seed **few‚Äëshot examples** per rule; tune thresholds from pilot runs.\n* [ ] Write **playbooks** (appeals, overrides, emergency lockdown).\n\n---\n\n## üîó Related Epics\n\n\\#framework-core\n\\#observability\n\\#multimodal-context\n\\#risk‚Äëand‚Äëpolicy\n\n---\n\n## üß≠ Non‚ÄëGoals (for now)\n\n* Full multilingual coverage (start with EN; add locales iteratively).\n* Deep video moderation (images/text first).\n* Perfect auto‚Äëban: keep humans in the loop for high‚Äëimpact actions.\n\n---\n\n## üìà Success Metrics\n\n* <1% confirmed false positives on auto actions.\n* p95 decision latency <300ms for text, <700ms if image scoring enabled.\n* ‚â•70% of incidents handled automatically without staff intervention after tuning.\n\n---\n\n## üß© Architecture (high level)\n\n```mermaid\nflowchart LR\n  IG[Ingest: Discord/Twitch] --> FE[Feature Extractors]\n  FE --> CLF[Classifiers & Embeddings]\n  CLF --> RE[Rule Engine]\n  RE -->|Auto| ACT[Actions: delete/timeout/ban]\n  RE -->|Borderline| Q[Mod Queue]\n  Q --> FB[Feedback Trainer]\n  ACT --> LOG[Audit Log]\n  RE --> LOG\n  FB --> RE\n  subgraph Observability\n    LOG --> MTR[Metrics/Alerts]\n  end\n  subgraph Config\n    RB[Versioned Rulebook]\n  end\n  RB --> RE\n```\n\nIf you want, I can spin this into a canvas-ready policy DSL starter (schema + a few exemplar rules) so you can start committing rules immediately.\n#IceBox\n\n"}
{"id":"23806c3e-fb3c-4759-892d-f168b9eebb57","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.516Z","uuid":"23806c3e-fb3c-4759-892d-f168b9eebb57","created_at":"2025-09-15T02:02:58.516Z","path":"docs/agile/tasks/lsp-server-for-home-brew-lisp-incoming.md","content":"# Description\n**Status:** blocked\n\nCreate a Language Server Protocol (LSP) server for the home-brew Lisp to provide editor features such as completion and diagnostics.\n\n## Requirements/Definition of done\n\n- Parse source files and build a minimal AST.\n- Support hover, go-to-definition, and diagnostics.\n- Expose a CLI or module that editors can launch.\n- Include tests covering core language features.\n\n## Tasks\n\n- [ ] Define JSON-RPC handlers for `initialize` and document events.\n- [ ] Implement or reuse a Lisp parser to generate an AST.\n- [ ] Wire up hover and go-to-definition responses.\n- [ ] Emit syntax and semantic diagnostics to the client.\n- [ ] Write unit tests for handler logic.\n\n## Relevant resources\n\nYou might find [the LSP specification](https://microsoft.github.io/language-server-protocol/specifications/specification-current/) useful while working on this task.\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n\n#breakdown\n\n## Blockers\n- No active owner or unclear scope\n\n"}
{"id":"24ba6872-89c0-4f5f-a20e-33058219016b","title":"Pythagoras GPT Mirror (WS chat + OpenAPI-driven tool calls via /v1)","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.517Z","uuid":"24ba6872-89c0-4f5f-a20e-33058219016b","created_at":"2025-09-15T02:02:58.517Z","path":"docs/agile/tasks/mvp-local-llm-chat-interface-with-tool-calls-connected-to-gpt-bridge.md","content":"# Pythagoras GPT Mirror (WS chat + OpenAPI-driven tool calls via /v1)\n\n**Owner:** Codex / Agent\n**Status:** Planned\n**Labels:** #agents #ws #openapi #tools #smartgpt-bridge #prompts #promethean\n\n---\n\n## üõ†Ô∏è Description\n\nCreate a ‚ÄúPythagoras-mirror‚Äù agent that:\n\n1. runs a **chat client** over the **broker/WebSocket LLM worker**,\n2. supports **tool calls** selected by the model,\n3. **auto-generates tool specs** from the **/v1/openapi.json** for smartgpt-bridge, and\n4. ensures the **OpenAPI op descriptions** are clear and model-usable.\n\n---\n\n## üéØ Goals\n\n* Model-driven tool selection (no brittle routing).\n* No HTTP coupling in the chat client‚Äî**WS to LLM worker** only; the worker executes tools by calling **smartgpt-bridge /v1**.\n* Tool catalog stays in sync with the OpenAPI spec automatically.\n\n---\n\n## üì¶ Requirements / Definition of Done (strict)\n\n* [ ] **Chat client** uses **WS** to talk to the **LLM worker** (broker path).\n* [ ] **LLM worker** accepts OpenAI-style tool calls (function-calling / tool\\_choice=auto).\n* [ ] **Tool catalog auto-generated** from **/v1/openapi.json** (names, descriptions, JSON schemas for params).\n* [ ] **OpenAPI op descriptions** are present, unambiguous, and concise enough that a model can choose the correct tool.\n* [ ] **E2E demo:** user asks a question that requires a tool ‚Üí model chooses tool ‚Üí worker calls **smartgpt-bridge** ‚Üí result streamed back.\n* [ ] **Parity tests** for at least 5 representative endpoints.\n* [ ] No new TS path aliases. Export via **@shared/ts/dist/**.\n\n---\n\n## Architecture (minimal)\n\n* **services/ts/llm-worker**\n\n  * WS in ‚Üí tool-aware chat loop ‚Üí tool exec adapter ‚Üí calls **smartgpt-bridge /v1** ‚Üí WS out.\n* **services/ts/pythagoras-mirror** (agent wrapper)\n\n  * Prompts + policies specific to ‚ÄúPythagoras‚Äù persona (math-first, stepwise).\n* **shared/ts/openapi-tools**\n\n  * Fetches/loads **/v1/openapi.json** ‚Üí generates:\n\n    * **tool specs** (name, description, parameters JSON schema),\n    * **client stubs** for executing tools.\n\n---\n\n## Data contracts (concise)\n\n### Tool spec (fed to model)\n\n```ts\ntype ToolSpec = {\n  type: \"function\";\n  function: {\n    name: string;                // opId or normalized path+method\n    description: string;         // from OpenAPI; trimmed & edited\n    parameters: Record<string, any>; // JSON Schema for body/query\n  };\n};\n```\n\n### WS message (worker)\n\n```ts\n// request -> worker\n{ id, action: \"chat.completion\", payload: { model, messages, tools: ToolSpec[], tool_choice: \"auto\", stream: true } }\n\n// worker events\n{ id, event: \"chat.delta\", delta }\n{ id, event: \"tool.call\", name, arguments }         // model-selected\n{ id, event: \"tool.result\", name, content }         // bridge response snippet\n{ id, event: \"chat.complete\", output, usage }\n```\n\n---\n\n## üìã Tasks & Subtasks\n\n### Step 1 ‚Äî OpenAPI ingestion ‚Üí tools & client\n\n* [ ] Create **openapi fetch/loader**: `shared/ts/src/openapi-tools/loader.ts`\n\n  * Fetch JSON from `/v1/openapi.json` (configurable URL).\n  * Cache & hash (ETag) to avoid churn.\n* [ ] Build **spec ‚Üí tools generator**: `shared/ts/src/openapi-tools/generateTools.ts`\n\n  * Map each **operationId** (or path+method) to a **ToolSpec**.\n  * Merge **parameters** (path/query/body) into a single JSON Schema object.\n  * **Normalize names**: `v1.files.search` ‚Üí `files_search`.\n  * **Descriptions**: prefer `operation.summary || operation.description`; trim, remove HTML, imperative voice.\n  * Add **guard rails**: skip ops without descriptions or with ambiguous names; emit report.\n* [ ] Build **typed client stubs**: `shared/ts/src/openapi-tools/client.ts`\n\n  * Given `{name, args}`, resolve to `{method, url, body/query}` and call **smartgpt-bridge** (with auth if needed).\n  * Return normalized result `{ content, status, headers? }`.\n  * Handle streaming endpoints by accumulating chunks or relaying partials as `tool.result` deltas.\n\n### Step 2 ‚Äî Tool-aware LLM worker over WS\n\n* [ ] Extend **LLM worker** WS handler: `services/ts/llm-worker/handlers/chat.ts`\n\n  * Accept `tools` and `tool_choice`.\n  * When model returns a tool call:\n\n    1. emit `tool.call`,\n    2. execute via **openapi client**,\n    3. stream `tool.result` (if large) or single result,\n    4. feed the result back into the model as a `tool` message, continue.\n* [ ] Admission control & timeouts for tools (per op SLA, default 10s).\n* [ ] Metric hooks: `tool_invocations_total{tool=...}`, `tool_latency_ms_bucket`, `tool_errors_total`.\n* [ ] Error policy: deliver short, model-friendly error (`tool_name`, `code`, `message`), then allow model to recover or ask clarifying q.\n\n### Step 3 ‚Äî Pythagoras mirror agent wrapper\n\n* [ ] Agent config `services/ts/pythagoras-mirror/config.ts`:\n\n  * **System prompt** (math-first, stepwise reasoning, prefer tools).\n  * Tool selection guidance: ‚ÄúUse a tool whenever external data is needed; don‚Äôt hallucinate API results.‚Äù\n  * Default `tool_choice: \"auto\"`, `temperature=0`.\n* [ ] Bootstrap script `services/ts/pythagoras-mirror/start.ts`:\n\n  * Loads **ToolSpec\\[]** from generator.\n  * Establishes WS to **llm-worker**.\n  * Provides a tiny CLI or HTTP shim to post messages and stream replies (dev UX).\n\n### Step 4 ‚Äî Make OpenAPI descriptions model-ready & test\n\n* [ ] **Lint OpenAPI**: `scripts/ci/check-openapi-descriptions.ts`\n\n  * Assert description/summary exists, ‚â§ 200 chars, starts with a verb, contains required input hints.\n  * Fail CI if any op is missing or ambiguous.\n* [ ] **Goldens**: write **5‚Äì10 parity tests** that require tool use, e.g.:\n\n  * file search, directory tree, policy check, status/health, small content read.\n  * For each, assert: model picks correct tool, args schema validated, result returned, final answer integrates tool data.\n* [ ] **Docs**: `docs/agents/pythagoras-mirror.md` explaining configuration, how tool specs are auto-generated, and how to add/rename an op without breaking the model.\n\n---\n\n## üîß Config (envs)\n\n```\nOPENAPI_URL=https://.../v1/openapi.json\nPYTHAGORAS_MODEL=qwen3:instruct\nLLM_WORKER_WS=ws://broker/llm\nTOOL_TIMEOUT_MS=10000\nTOOL_MAX_RESULT_KB=256\n```\n\n---\n\n## üß™ Test plan (essentials)\n\n* **Tool discovery:** generator produces ‚â• N tools; none without description.\n* **Schema parity:** sampling ops: generated JSON Schema matches OpenAPI for required/optional fields.\n* **Round-trip:** prompt ‚Üí tool call ‚Üí bridge result ‚Üí model reply (no hangs).\n* **Error cases:** missing arg ‚Üí schema error; 404/500 from bridge ‚Üí worker reports succinct error; model recovers.\n* **Load:** concurrent tool calls √ó10; WS remains stable; timeouts enforced.\n* **Docs linter:** CI fails if an op lacks a usable description.\n\n---\n\n## ‚úÖ Acceptance Criteria\n\n* [ ] WS chat client ‚Üî **llm-worker** end-to-end streaming works.\n* [ ] **Tool calls accepted and executed**; arguments validated against generated JSON Schema.\n* [ ] Tool catalog **auto-generated** on boot (and cached); human report lists skipped/ambiguous ops.\n* [ ] **OpenAPI descriptions** lint-clean; ops are understandable to a model (imperative, concise).\n* [ ] Green **golden tests** covering at least 5 representative tools.\n* [ ] Docs explain how to add/rename an OpenAPI op and re-generate tools.\n\n---\n\n## üìÇ Proposed Files/Paths\n\n* `shared/ts/src/openapi-tools/loader.ts`\n* `shared/ts/src/openapi-tools/generateTools.ts`\n* `shared/ts/src/openapi-tools/client.ts`\n* `services/ts/llm-worker/handlers/chat.ts` (tool-aware)\n* `services/ts/pythagoras-mirror/config.ts`\n* `services/ts/pythagoras-mirror/start.ts`\n* `tests/e2e/pythagoras/tools.spec.ts`\n* `scripts/ci/check-openapi-descriptions.ts`\n* `docs/agents/pythagoras-mirror.md`\n\n---\n\n## CLI sketches (for humans/dev)\n\n```\n# 1) Generate tool catalog from OpenAPI\npnpm run generate:tools\n\n# 2) Start worker + agent mirror\npnpm --filter services/ts/llm-worker start\npnpm --filter services/ts/pythagoras-mirror start\n\n# 3) Chat (dev)\ncurl -N -X POST localhost:3210/pythagoras/chat -d '{\"text\":\"Search files for \\\"embedding\\\" and summarize.\"}'\n```\n\n---\n\n## Step 1‚Äì4 (your minimal milestone format)\n\n* [ ] **Step 1 ‚Äî OpenAPI ‚Üí Tools:** loader, generator, typed client, report on skipped ops.\n* [ ] **Step 2 ‚Äî Worker Tooling:** WS chat handler supports tool calls; execute via bridge; metrics & timeouts.\n* [ ] **Step 3 ‚Äî Agent Mirror:** system prompt, boot logic, dev shim.\n* [ ] **Step 4 ‚Äî Lint & Tests:** description linter, goldens, load test, docs.\n\n---\n\n## Commit template\n\n```\nfeat(agent): Pythagoras-mirror via WS with OpenAPI-driven tools (/v1 smartgpt-bridge)\n\n- Auto-generate tool specs from /v1/openapi.json\n- LLM worker accepts tool calls and executes via smartgpt-bridge\n- OpenAPI description linter; goldens for 5 representative ops\n- Docs & dev bootstrap\n```\n\n\\#tags #promethean #pythagoras #ws #llm-worker #smartgpt-bridge #openapi #tool-calling #testing #docs\n\n## Notes\n- Tests or documentation are missing; acceptance criteria not fully met.\n- Story Points: 8\n\n#ready\n\n"}
{"id":"25179a10-a528-4d08-8174-81935e5f7d13","title":"add ops endpoint to list partition assignments opt md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.505Z","uuid":"25179a10-a528-4d08-8174-81935e5f7d13","created_at":"2025-09-15T02:02:58.505Z","path":"docs/agile/tasks/add_ops_endpoint_to_list_partition_assignments_opt.md","content":"## üõ†Ô∏è Task: Add `/ops` endpoint to list **partition assignments** (optional: dump coordinator state)\n\nGenerated from [../unique/2025.08.08.20.08.56.md](../unique/2025.08.08.20.08.56.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"289512a0-68eb-47aa-b031-7c694d1a4806","title":"scripts group audio tools","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.520Z","uuid":"289512a0-68eb-47aa-b031-7c694d1a4806","created_at":"2025-09-15T02:02:58.520Z","path":"docs/agile/tasks/scripts_group_audio_tools.md","content":"Scripts: Group audio tools under scripts/audio\n\nGoal: Move STT/TTS helpers into `scripts/audio/` with a README and verified usage.\n\nScope:\n- Create `scripts/audio/` and move:\n  - stt.py, stt_request.py, stt_module.py\n  - tts.py, tts_request.py, bench_tts_ws.py\n  - whisper_test.py, vision_request.py\n- Add `scripts/audio/README.md` with requirements (e.g., FFmpeg, model paths), examples, and env vars.\n\nExit Criteria:\n- All audio utilities live under `scripts/audio/` and run from repo root.\n- README exists with prerequisites and sample commands.\n\n#incoming #scripts #audio #organization\n\n\n"}
{"id":"2901d671-ce49-4228-9c82-30112b2bbc12","title":"add semantic overlays for layer1 through layer8 md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.505Z","uuid":"2901d671-ce49-4228-9c82-30112b2bbc12","created_at":"2025-09-15T02:02:58.505Z","path":"docs/agile/tasks/add_semantic_overlays_for_layer1_through_layer8.md","content":"## üõ†Ô∏è Task: Add semantic overlays for layer1 through layer8\n\nImplement documentation overlays that explain how each cognitive layer maps to the system. These overlays should provide conceptual context for the eight circuits described in the design docs.\n\n---\n\n## üéØ Goals\n\n- Mirror the eight circuits with dedicated documentation sections\n- Provide consistent naming conventions and tags for each layer\n- Clarify the relationship between circuits and system modules\n\n---\n\n## üì¶ Requirements\n\n- [ ] Create markdown stubs for each layer under `docs/design/circuits/`\n- [ ] Include links back to relevant services or agents\n- [ ] Tag each doc with `#layerX` where `X` is 1-8\n\n---\n\n## üìã Subtasks\n\n- [ ] Draft outline for each circuit overlay\n- [ ] Link overlays from the main design overview\n- [ ] Update Kanban board when overlays are complete\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[docs/design/overview|design overview]]\n- [[kanban]]\n#IceBox\n\n"}
{"id":"2b28533b-0e70-4b18-80dc-1c8f96e24735","title":"Organize frontend packages under a subdirectory","status":"in_progress","priority":"p3","owner":"","labels":["framework-core","refactor"],"created":"2025-09-27T00:00:00.000Z","uuid":"2b28533b-0e70-4b18-80dc-1c8f96e24735","created_at":"2025-09-27T00:00:00.000Z","path":"docs/agile/tasks/organize-frontend-packages.md","content":"#InProgress\n\n## üõ†Ô∏è Description\n\n- Track the incremental workspace reorganization requested during the frontend split discussion.\n\n## Description\n- **What changed?** Planning identified an initial slice to move obvious `-frontend` packages into a dedicated namespace while keeping other packages untouched.\n- **Where is the impact?** Impacts the `packages/` workspace layout, Nx project declarations, and pnpm workspace discovery.\n- **Why now?** Enables gradual cleanup of the crowded `packages/` folder and establishes a repeatable pattern for future moves.\n- **Supporting context** Prior conversation outlining the phased migration approach.\n\n## Goals\n- Ensure pnpm and Nx resolve packages nested under `packages/frontends/`.\n- Relocate the existing `-frontend` packages without breaking their build/lint commands.\n- Capture follow-up tasks for splitting out backends, libraries, and tooling packages.\n\n## Requirements\n- [ ] pnpm can still discover and install workspace packages after the move.\n- [ ] Nx targets for relocated frontends run from their new paths.\n- [ ] Update documentation with the planned future iterations.\n- [ ] Record the follow-up tasks needed for the remaining packages.\n\n## Subtasks\n1. Update workspace discovery settings (pnpm, Nx) to account for nested folders.\n2. Move the `-frontend` packages into `packages/frontends/` and adjust their configuration.\n3. Validate builds/lints for the moved packages and document future slices.\n\nEstimate: 5\n\n---\n\n## üîó Related Epics\n\n- [[kanban]]\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- None\n\n## ‚õìÔ∏è Blocks\n\n- None\n\n---\n\n## üîç Relevant Links\n\n- Pending PR once work is complete.\n"}
{"id":"2c3e3772-e388-4383-a3be-d56fd11b766e","title":"auth ci and load tests","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"2c3e3772-e388-4383-a3be-d56fd11b766e","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/auth_ci_and_load_tests.md","content":"Auth: CI and load tests\n\nGoal: Add CI tests and basic load testing for the auth service.\n\nScope:\n- Unit tests for token issuance/validation edge cases.\n- Integration test issuing token and calling a protected endpoint (bridge stub).\n- Minimal k6 or autocannon script to issue tokens at rate; ensure perf targets (p95 latency, CPU/mem).\n\nExit Criteria:\n- CI green; performance baseline recorded in docs.\n\n#incoming #auth #testing #performance\n\n\n"}
{"id":"2c69e218-bbe0-4c5b-ab1b-d19b88496b57","title":"fp ts config and linting","status":"todo","priority":"p3","owner":"","labels":[],"created":"2025-09-15T02:02:58.514Z","uuid":"2c69e218-bbe0-4c5b-ab1b-d19b88496b57","created_at":"2025-09-15T02:02:58.514Z","path":"docs/agile/tasks/fp-ts-config-and-linting.md","content":"<hr class=\"__chatgpt_plugin\">\n\n<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n### role::assistant\n\nHere's a suggested revision of your context section:\n\n## Context\n### Changes and Updates\n- **What changed?**: [Describe the key changes that have occurred, e.g., updated requirements, new feature added]\n- **Where?**: [Specify the location or scope where these changes are relevant, e.g., specific project, department]\n- **Why now?**: [Explain why these changes are necessary at this time, e.g., due to deadline, feedback from stakeholders]\n\n## Inputs / Artifacts\n- ([link or path] to relevant documentation, data, or assets)\n\n## Definition of Done\n- [ ] test X passes: [ Briefly describe the testing scenario]\n- [ ] doc Y updated: [Mention the specific documentation or resource that has been updated]\n- [ ] PR merged: ([link to the PR] with a brief summary of changes)\n\n## Plan\n1. ‚Ä¶ [ Outline the high-level steps for completing this task, including any dependencies or resources required]\n2. ‚Ä¶\n\n## Relevant Resources\n\nYou might find [this](link) useful while working on this task.\n\n### Smart Connections Configuration\n```smart-connections\n{\n  \"render_markdown\": true,\n  \"show_full_path\": false,\n  \"exclude_blocks_from_source_connections\": false,\n  \"exclude_frontmatter_blocks\": true,\n  \"expanded_view\": false,\n  \"results_limit\": \"20\",\n  \"exclude_inlinks\": false,\n  \"exclude_outlinks\": false\n}\n```\n\n### Smart ChatGPT Configuration\n```smart-chatgpt\n```\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\nUsing this title write a reasonable task document:fp-ts-config-and-linting\n<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n\n\n\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\nHelp me write this task \n\n\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\n\n\n"}
{"id":"2c6e5a8d-7c9b-4a1b-b2d3-4e5f6a7b8c9d","title":"duck-web ‚Äî openWs helper DI wrapper + bearer guard","status":"in_progress","priority":"P2","owner":"","labels":["duck-web","websocket","auth"],"created":"2025-10-02T00:00:00.000Z","uuid":"2c6e5a8d-7c9b-4a1b-b2d3-4e5f6a7b8c9d","created_at":"2025-10-02T00:00:00.000Z","path":"docs/agile/tasks/openws-helper-wrap-and-guards.md","content":"#Todo\n\n## üõ†Ô∏è Description\nWrap `openWs` with injectable factory; skip `bearer.*` subprotocol when token absent.\n\n## Requirements\n- [ ] Pure helper with DI\n- [ ] Unit tests mocking Ws factory\n\n## ‚õìÔ∏è Blocks\n- Merge PR #1446\n\n## üîç Related\n- PR #1446\n"}
{"id":"2d9a38db-a0cc-4939-9e30-c38497da6745","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.519Z","uuid":"2d9a38db-a0cc-4939-9e30-c38497da6745","created_at":"2025-09-15T02:02:58.519Z","path":"docs/agile/tasks/script-for-getting-github-action-workflow-states-for-a-branch.md","content":"# Description\n**Status:** blocked\n\nCreate a script that fetches GitHub Actions workflow runs for a given branch and outputs their status so they can be reviewed or fed to language models.\n\n## Requirements/Definition of done\n\n- Accept repository and branch as arguments.\n- Output run IDs, workflow names, and current status in markdown or JSON.\n- Handle API pagination and authentication.\n- Document usage in `scripts/README.md`.\n\n## Tasks\n\n- [ ] Implement script that calls the GitHub Actions API for a branch.\n- [ ] Read GitHub token from environment variables.\n- [ ] Format the response into a human-readable table.\n- [ ] Add example invocation and output to docs.\n\n## Relevant resources\n\nYou might find [GitHub's Actions API](https://docs.github.com/en/rest/actions) useful while working on this task.\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n\n#breakdown\n\n## Blockers\n- No active owner or unclear scope\n\n"}
{"id":"2eac8af8-2083-408e-ac4a-ceb334e2e32a","title":"discord chat link traversal md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.511Z","uuid":"2eac8af8-2083-408e-ac4a-ceb334e2e32a","created_at":"2025-09-15T02:02:58.511Z","path":"docs/agile/tasks/discord_chat_link_traversal.md","content":"## üõ†Ô∏è Description\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- What are we trying to accomplish?\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#IceBox\n\n"}
{"id":"2f19096b-bacb-4ace-9e82-61893b70720d","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.503Z","uuid":"2f19096b-bacb-4ace-9e82-61893b70720d","created_at":"2025-09-15T02:02:58.503Z","path":"docs/agile/tasks/add-git-commands-to-gpt-bridge.md","content":"# Description\n\nThe GPT bridge should be able to run:\n- status\n- patch\n- add\n- commit\n\n## Requirements/Definition of done\n\n- If it doesn't have this, we can't accept it\n\n## Tasks \n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Relevent resources\n\nYou might find [this] useful while working on this task\n\n\n"}
{"id":"2f6779f0-f7aa-4565-b120-43dd47e8a530","title":"detect contradictions in memory md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.511Z","uuid":"2f6779f0-f7aa-4565-b120-43dd47e8a530","created_at":"2025-09-15T02:02:58.511Z","path":"docs/agile/tasks/detect_contradictions_in_memory.md","content":"## üõ†Ô∏è Task: Detect contradictions in memory\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Detect contradictions in memory\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#IceBox\n\n"}
{"id":"3139c11a-ab08-4389-a857-65096382d244","title":"reach 100 percent complete test coverage 1 md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.518Z","uuid":"3139c11a-ab08-4389-a857-65096382d244","created_at":"2025-09-15T02:02:58.518Z","path":"docs/agile/tasks/reach_100_percent_complete_test_coverage_1.md","content":"## üõ†Ô∏è Description\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Add Ollama formally to pipeline\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#IceBox\n\n"}
{"id":"3192eeb8-39ff-42aa-ab6f-2e7ce9b951cf","title":"something something 1","status":"todo","priority":"p3","owner":"","labels":[],"created":"2025-09-15T02:02:58.520Z","uuid":"3192eeb8-39ff-42aa-ab6f-2e7ce9b951cf","created_at":"2025-09-15T02:02:58.520Z","path":"docs/agile/tasks/something-something.-1.md","content":"<hr class=\"__chatgpt_plugin\">\n\n<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n### role::assistant\n\nHere's a suggested revision of your context section:\n\n## Context\n### Changes and Updates\n- **What changed?**: [Describe the key changes that have occurred, e.g., updated requirements, new feature added]\n- **Where?**: [Specify the location or scope where these changes are relevant, e.g., specific project, department]\n- **Why now?**: [Explain why these changes are necessary at this time, e.g., due to deadline, feedback from stakeholders]\n\n## Inputs / Artifacts\n- ([link or path] to relevant documentation, data, or assets)\n\n## Definition of Done\n- [ ] test X passes: [ Briefly describe the testing scenario]\n- [ ] doc Y updated: [Mention the specific documentation or resource that has been updated]\n- [ ] PR merged: ([link to the PR] with a brief summary of changes)\n\n## Plan\n1. ‚Ä¶ [ Outline the high-level steps for completing this task, including any dependencies or resources required]\n2. ‚Ä¶\n\n## Relevant Resources\n\nYou might find [this](link) useful while working on this task.\n\n### Smart Connections Configuration\n```smart-connections\n{\n  \"render_markdown\": true,\n  \"show_full_path\": false,\n  \"exclude_blocks_from_source_connections\": false,\n  \"exclude_frontmatter_blocks\": true,\n  \"expanded_view\": false,\n  \"results_limit\": \"20\",\n  \"exclude_inlinks\": false,\n  \"exclude_outlinks\": false\n}\n```\n\n### Smart ChatGPT Configuration\n```smart-chatgpt\n```\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\nUsing this title write a reasonable task kanban task document: something-something.-1\n\n \n\n\n<hr class=\"__chatgpt_plugin\">\n\n### role::assistant<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n\n**Task:** Implementing Natural Language Processing for Knowledge Base Indexing\n\n**Title:** `Something-Something.-1`\n\n**Description:**\nImplement natural language processing (NLP) capabilities to enhance knowledge base indexing within Obsidian. This will enable more accurate and efficient retrieval of information.\n\n**Objective:**\n\n* Develop a custom NLP pipeline using a library like spaCy or NLTK\n* Integrate the pipeline with Obsidian's existing metadata storage\n* Implement search functionality that utilizes the NLP pipeline\n\n**Acceptance Criteria:**\n\n1. **NLP Pipeline Development**\n\t* Successfully develop and deploy the NLP pipeline\n\t* Verify that the pipeline can accurately extract entities, sentiments, and relationships from text data\n2. **Integration with Obsidian Metadata Storage**\n\t* Integrate the NLP pipeline with Obsidian's metadata storage system\n\t* Ensure seamless data exchange between the two systems\n3. **Search Functionality Implementation**\n\t* Develop a search interface that leverages the NLP pipeline for search results\n\t* Verify that the search functionality returns accurate and relevant results\n\n**Dependencies:**\n\n* Familiarity with Python programming language and NLP libraries (spaCy, NLTK)\n* Experience with integrating third-party APIs or services with Obsidian\n* Basic understanding of natural language processing concepts and techniques\n\n**Estimated Time:** 3-4 weeks\n\n**Priority:** Medium-High\n\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\n\n"}
{"id":"325bf2aa-d063-4a19-a086-3e7a42755861","title":"Audit Enso event family references after transport update","status":"todo","priority":"P2","owner":"","labels":["documentation","protocol"],"created":"2025-09-21T00:00:00Z","uuid":"325bf2aa-d063-4a19-a086-3e7a42755861","created_at":"2025-09-21T00:00:00Z","path":"docs/agile/tasks/audit-enso-event-families.md","content":"# Description\n\nEnsure the newly expanded transport chapter stays aligned with the detailed\nEnso protocol write-ups and SDK payload types.\n\n## Goals\n\n- Confirm each event family listed in `02-transport-and-framing.md` links to the\n  correct specialised chapter.\n- Verify specialised chapters cover the payload semantics expected by the shared\n  TypeScript definitions.\n- Capture any mismatches or missing schemas as follow-up issues or docs tasks.\n\n## Requirements\n\n- [ ] Review `docs/design/enso-protocol/02-transport-and-framing.md` for\n  completeness after the event family table.\n- [ ] Cross-check each referenced chapter to ensure payload examples or schemas\n  exist for the event family.\n- [ ] Compare `packages/enso-protocol/src/types/events.ts` against the docs and\n  note discrepancies.\n\n## Subtasks\n\n- [ ] Validate content, asset, cache, and context sections against their\n  detailed specs.\n- [ ] Validate flow, voice, tool, and MCP sections against transport semantics.\n- [ ] Document gaps or TODOs in a follow-up task or inline note.\n\n## Notes\n\nFocus on human review‚Äîno automated tooling expected beyond basic diffing.\n\n#Todo\n"}
{"id":"35baa5c2-8daa-4162-97ad-19f55b9c2b51","title":"Mock the Broker for Tests","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.517Z","uuid":"35baa5c2-8daa-4162-97ad-19f55b9c2b51","created_at":"2025-09-15T02:02:58.517Z","path":"docs/agile/tasks/mock-broker.md","content":"Here‚Äôs a no-nonsense expansion that you can drop into your board. I‚Äôve treated this like a mini-spec + work plan so Codex (or you) can just execute.\n\n# Mock the Broker for Tests\n\n**Owner:** Codex / Agent\n**Status:** needs review\n**Labels:** #framework-core #testing #infra #typescript #fastify #websockets #mongodb #chroma #promethean\n\n---\n\n## üõ†Ô∏è Description\n\nCreate a fully in-memory **Broker Mock** that mirrors the public contract of the real broker (publish / subscribe / unsubscribe now; queue semantics later), so any module that currently depends on the broker can be tested without network access. Provide deterministic hooks (time, ordering, error/latency injection), and a simple adapter so services can swap ‚Äúreal vs mock‚Äù via DI or env at test boot.\n\n---\n\n## üéØ Goals\n\n* Run unit + integration tests for broker-dependent modules **without** opening sockets or requiring an external broker process.\n* Deterministic tests: control time, event ordering, retries, backpressure, and failure modes.\n* Zero network usage. Zero flakiness.\n* Minimal friction: drop-in `BrokerClient` replacement with identical surface.\n\n---\n\n## üì¶ Requirements\n\n* [ ] **Interface parity:** Expose the same TypeScript interface used by real clients (`subscribe`, `unsubscribe`, `publish`, `request/response?` if present, and the current message envelope shape).\n* [ ] **In-memory topic bus:** Maintain per-topic subscriber lists; deliver messages synchronously by default; support async delivery when latency is injected.\n* [ ] **Deterministic scheduler:** A tiny scheduler to control ‚Äúticks‚Äù for delivery to avoid race conditions in tests.\n* [ ] **Failure/latency injection:** Configurable drop %, fixed/variable latency, forced disconnects, backpressure overflow policy (drop/newest/block), and ordered/unordered modes.\n* [ ] **Fake time:** Optional fake clock (advance time manually in tests) for retry/timeout logic.\n* [ ] **Backpressure simulation:** Bounded per-subscriber queue with metrics; configurable overflow strategy.\n* [ ] **Metrics & hooks:** Counters for published / delivered / dropped / retried; test-side hooks to assert what happened.\n* [ ] **No global leaks:** Independent instances per test; auto-teardown; no retained static state.\n* [ ] **Test utils:** Fixtures, spies, and helpers to assert ‚Äúmessage X reached subscriber Y with payload Z in order N‚Äù.\n* [ ] **Env-switchable:** Respect existing env selection (`BROKER_IMPL=mock|real`) or DI token to swap at process start.\n* [ ] **Docs:** Short README with examples; guidance for common failure scenarios.\n* [ ] **CI-safe:** Runs on CI with no network and no ports.\n* [ ] **Future-proof for queues:** Stub queue API surface (`declare`/no-op) with TODOs so it‚Äôs easy to wire later, without breaking tests today.\n* [ ] **Imports:** Use existing namespace rules (e.g., **@shared/ts/dist/**‚Ä¶ where applicable). Don‚Äôt invent aliases.\n\n---\n\n## üìã Subtasks\n\n* [ ] **Define the contract**: Extract the minimal `BrokerClient` interface from current code (pub/sub/unsub + message envelope), place an interface file in `@shared/ts` and ensure it‚Äôs the single source of truth.\n* [ ] **Mock implementation**:\n\n  * [ ] Implement `InMemoryBroker` (per-instance): topic ‚Üí subscribers (ordered Set).\n  * [ ] Implement `publish(topic, message, opts?)`: immediate or scheduled dispatch.\n  * [ ] Implement `subscribe(topic, handler, opts?)`: returns `unsubscribe()` cleanup.\n  * [ ] Delivery semantics: default **in-order** per topic; optional ‚Äúshuffle‚Äù to simulate out-of-order.\n  * [ ] Backpressure: per-subscriber ring buffer with size limit and policy (drop/head, drop/tail, throw, block with promise).\n  * [ ] Failure injection:\n\n    * [ ] `latencyMs` (fixed or range)\n    * [ ] `dropRate` (0‚Äì1)\n    * [ ] `disconnectAfter(N)` / `disconnectNow()`\n    * [ ] `fault(topicPattern, predicate)` to selectively fail.\n  * [ ] Fake clock (optional): tiny clock you can `advance(ms)` for timeout/retry tests.\n  * [ ] Metrics: counters + event log for assertions.\n* [ ] **Adapter / DI**:\n\n  * [ ] Add `createBrokerClient()` factory that returns real or mock based on env or an injected token.\n  * [ ] Keep API identical so services don‚Äôt branch.\n* [ ] **Test Utilities** (`@shared/ts/dist/test/broker-utils`):\n\n  * [ ] `expectDelivered({topic, times, predicate})`\n  * [ ] `awaitNextMessage(topic)` / `drain(topic)`\n  * [ ] `recordingSubscriber()` to capture payloads with timestamps/seq ids.\n  * [ ] `withMockBroker(config, fn)` fixture wrapper (handles setup/teardown).\n* [ ] **Refactor touch points**:\n\n  * [ ] Replace direct imports of the real broker with the shared `BrokerClient` interface + factory in services that have tests.\n  * [ ] Ensure no module reaches for WebSocket/HTTP transport directly during tests.\n* [ ] **Initial conversions** (pick high-value services first):\n\n  * [ ] `services/ts/cephalon` (contextManager / collectionManager)\n  * [ ] `services/ts/smartgpt-bridge` (router/tools that publish events)\n  * [ ] `services/ts/heartbeat` (where it emits broker pings)\n* [ ] **Add example tests** (vitest):\n\n  * [ ] Happy path pub/sub ordering.\n  * [ ] Unsubscribe stops delivery.\n  * [ ] Latency/drops/backpressure behavior.\n  * [ ] Timeout/retry logic using fake clock.\n  * [ ] Fault injection by topic/predicate.\n* [ ] **Docs**: `docs/testing/broker-mock.md` showing usage patterns and common pitfalls.\n* [ ] **CI**: Ensure test command uses mock by default (`BROKER_IMPL=mock`).\n\n---\n\n## üß© Design Notes (keep it simple, deterministic)\n\n**Message envelope (mirror what you already use):**\n\n```ts\ntype BrokerMessage<T = unknown> = {\n  id: string;            // uuid\n  topic: string;\n  ts: number;            // epoch ms (fake clock if enabled)\n  payload: T;\n  headers?: Record<string, string>;\n};\n```\n\n**Interface (shared, single source of truth):**\n\n```ts\nexport interface BrokerClient {\n  publish<T>(topic: string, msg: T, opts?: PublishOpts): Promise<void>;\n  subscribe<T>(topic: string, handler: (msg: BrokerMessage<T>) => Promise<void> | void, opts?: SubOpts): Unsubscribe;\n  unsubscribe(topic: string, handler: Function): void; // convenience\n  close(): Promise<void>;\n}\n```\n\n**Mock config (for tests):**\n\n```ts\nexport type MockBrokerConfig = {\n  latencyMs?: number | { min: number; max: number };\n  dropRate?: number; // 0..1\n  ordering?: 'in-order' | 'shuffle';\n  bufferSize?: number; // per-subscriber\n  overflow?: 'drop-oldest' | 'drop-newest' | 'throw' | 'block';\n  useFakeClock?: boolean;\n  faults?: Array<{ match: RegExp; when?: (m: BrokerMessage) => boolean; action: 'drop' | 'error' | 'delay' }>;\n};\n```\n\n**Deterministic scheduler:**\n\n* Default: synchronous inline delivery (fast unit tests).\n* When latency or async mode is on, enqueue to a per-topic microtask queue controlled by the fake clock or a test-owned tick: `await broker.tick()` or `clock.advance(10)`.\n\n**Backpressure:**\n\n* Per handler, a ring buffer (size N). When full, follow `overflow` policy. Expose counters so tests can assert dropped vs delivered.\n\n**Failure injection patterns:**\n\n* Global `dropRate`.\n* Topic-pattern faults: `faults: [{ match: /^voice\\./, action: 'drop' }]`.\n* `disconnectNow()` to simulate transport death; further publish/subscribe throws.\n\n---\n\n## ‚úÖ Acceptance Criteria\n\n* [ ] Tests can run with **no network** and **no external broker**.\n* [ ] The mock implements the same **public interface** as the real broker client; swapping impl requires **no test code changes** beyond factory usage.\n* [ ] Deterministic ordering by default; controllable non-determinism (shuffle) when requested.\n* [ ] Configurable latency, drops, disconnects, and backpressure; each is **assertable via metrics**.\n* [ ] No global state leaks across tests; parallel test runs are isolated.\n* [ ] Minimal docs + examples committed and referenced from `README` or `docs/testing/broker-mock.md`.\n* [ ] CI uses the mock by default and passes on a clean tree.\n\n---\n\n## üîó Related Epics\n\n* \\#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n* Nothing\n\n## ‚õìÔ∏è Blocks\n\n* Enables reliable tests for services coupled to broker messages (cephalon, heartbeat, smartgpt-bridge, etc.).\n\n---\n\n## üîç Relevant Links\n\n* \\[\\[kanban.md]]\n* \\#incoming\n\n---\n\n## üß™ Example Test Sketches (vitest-style)\n\n```ts\nit('delivers messages in order by default', async () => {\n  const broker = createMockBroker({ ordering: 'in-order' });\n  const seen: number[] = [];\n  const unsub = broker.subscribe<number>('ctx.update', m => { seen.push(m.payload); });\n  await broker.publish('ctx.update', 1);\n  await broker.publish('ctx.update', 2);\n  expect(seen).toEqual([1, 2]);\n  unsub();\n});\n\nit('can inject latency and drop messages', async () => {\n  const broker = createMockBroker({ latencyMs: 10, dropRate: 0.5 });\n  const rec = recordingSubscriber<number>();\n  broker.subscribe('voice.frame', rec.handler);\n  for (let i = 0; i < 10; i++) await broker.publish('voice.frame', i);\n  await broker.tickAll(); // drain scheduler\n  expect(rec.count()).toBeLessThan(10);\n  expect(broker.metrics().published).toBe(10);\n  expect(broker.metrics().delivered + broker.metrics().dropped).toBe(10);\n});\n```\n\n---\n\n## ‚ö†Ô∏è Pitfalls & Guardrails\n\n* Don‚Äôt let tests **implicitly depend on real time**; use the fake clock or explicit ticks.\n* Keep the mock **simple**: avoid re-implementing transport layers. We only mimic **observable behavior**, not internal sockets.\n* Mirror the **envelope shape** exactly to prevent brittle adapter code.\n* No new TS path aliases. Use existing **@shared/ts/dist/** import patterns if/when this lands in shared.\n* Resist feature creep: queue semantics can be stubbed behind an interface but implemented later.\n\n---\n\n## üóÇÔ∏è Proposed File Layout (suggest‚Äîadapt to your structure)\n\n* `shared/ts/src/broker/types.ts` ‚Äî `BrokerClient`, envelopes, opts\n* `shared/ts/src/broker/mock.ts` ‚Äî `InMemoryBroker`, scheduler, metrics\n* `shared/ts/src/broker/fakeClock.ts` ‚Äî tiny controllable clock\n* `shared/ts/src/test/broker-utils.ts` ‚Äî fixtures & assertions\n* `services/ts/*/test/*.spec.ts` ‚Äî conversions to use factory\n* `docs/testing/broker-mock.md` ‚Äî usage + patterns\n\n---\n\n## üì£ Definition of Done\n\n* Mock broker shipped in shared code, exported through **@shared/ts/dist**.\n* At least **two** broker-dependent services converted to use the factory in tests, with flaky tests removed.\n* CI green with network disabled for those suites.\n* Docs + examples merged.\n\n#promethean #broker #mock #testing #vitest #typescript #di #inmemory #deterministic\n\n#in-review\n\n"}
{"id":"35c0a7d2-a172-4cfd-8521-b3faee0417ca","title":"clearly seperate service dependency files md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.509Z","uuid":"35c0a7d2-a172-4cfd-8521-b3faee0417ca","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/clearly_seperate_service_dependency_files.md","content":"Q## üõ†Ô∏è Task: Clearly seperate service dependency files\n\nEach service should maintain its own dependency declarations so deployments remain isolated. Refer to the design notes in `file-structure.md` and the migration plan for guidance.\n\nCreate isolated dependency manifests for each service so they can be installed\nindependently. Currently some packages are shared in the root `package.json`\nand `Pipfile`, which complicates per-service deployment.\n\n---\n\n## üéØ Goals\n- Distinct `requirements.txt` or `package.json` for every service\n- Document dependency layout in `/docs/file-structure.md`\n- Each service should declare its own Python and Node dependencies\n(`requirements.txt` or `package.json`).\n- Reduce cross‚Äëservice coupling in the root manifests.\n- Document how to install dependencies for an individual service.\n\n---\n\n## üì¶ Requirements\n- [ ] Audit current shared dependency usage\n- [ ] Create per-service files and update CI accordingly\n\n- [ ] Audit existing packages in the root manifests.\n- [ ] Move service‚Äëspecific libraries into the corresponding service folder.\n- [ ] Keep shared dev tooling (lint, test runners) at the repository root.\n- [ ] Update documentation to describe per‚Äëservice setup.\n\n---\n\n## üìã Subtasks\n- [ ] Split Python requirements by service\n- [ ] Split Node dependencies by service\n- [ ] Update documentation with examples\n- [ ] List all current dependencies by service.\n- [ ] Create `requirements.txt` or `package.json` as needed.\n- [ ] Update the Makefile to install dependencies per service.\n- [ ] Clean up obsolete references in the root manifests.\n\n---\n\n## üîó Related Epics\n#devops #cicd\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\n- Update GitHub Actions to use makefile\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n- [file-structure](../file-structure.md)\n- [MIGRATION_PLAN](../MIGRATION_PLAN.md)\n#done\n\n"}
{"id":"36eddbb2-91d6-4930-8ad8-4fdc004e6672","title":"ecs projection jobs","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"36eddbb2-91d6-4930-8ad8-4fdc004e6672","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/ecs_projection_jobs.md","content":"ECS Projections and Compaction Jobs\n\nGoal: Add background jobs to project ECS component state into query-friendly collections and compact time-series.\n\nProjections:\n- `ConversationLog`: denormalized stream of user/agent messages by channel/user.\n- `AgentStateSnapshot`: current turn, speaking status, last VAD, last utterance.\n\nCompaction:\n- TTL or bucketed retention for high-churn components (VAD, RawVAD).\n- Periodic snapshots to reduce read amplification.\n\nExit Criteria:\n- Scripts/processes under `services/ts/cephalon/scripts/` or a shared service.\n- Verified indexes and query latencies.\n\n#incoming #ecs #projections #compaction\n\n"}
{"id":"371cb900-e46b-4bc1-ad48-2acf08d2887f","title":"ensure mongo indexes key 1 unique common query fie md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"371cb900-e46b-4bc1-ad48-2acf08d2887f","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/ensure_mongo_indexes_key_1_unique_common_query_fie.md","content":"## üõ†Ô∏è Task: Ensure Mongo indexes: `{ _key: 1 } unique` + common query fields\n\nGenerated from [../unique/2025.08.08.20.08.56.md](../unique/2025.08.08.20.08.56.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"391e27d8-9134-4362-95bc-73ca9470c9e3","title":"wire mongoeventstore mongocursorstore in place of md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.523Z","uuid":"391e27d8-9134-4362-95bc-73ca9470c9e3","created_at":"2025-09-15T02:02:58.523Z","path":"docs/agile/tasks/wire_mongoeventstore_mongocursorstore_in_place_of.md","content":"## üõ†Ô∏è Task: Wire MongoEventStore + MongoCursorStore in place of InMemory\n\nGenerated from [../unique/2025.08.08.15.08.47.md](../unique/2025.08.08.15.08.47.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#ice-box\n\n"}
{"id":"3d7e9f1a-2b3c-4d5e-8f9a-0b1c2d3e4f5a","title":"duck-web ‚Äî throttled RTCDataChannel sender with backpressure","status":"in_progress","priority":"P1","owner":"","labels":["duck-web","webrtc","perf"],"created":"2025-10-02T00:00:00.000Z","uuid":"3d7e9f1a-2b3c-4d5e-8f9a-0b1c2d3e4f5a","created_at":"2025-10-02T00:00:00.000Z","path":"docs/agile/tasks/throttled-datachannel-sender.md","content":"#Todo\n\n## üõ†Ô∏è Description\nImplement `makeThrottledSender(ch, threshold)` using `bufferedamountlow` event; fix typos; default threshold ~1 MiB.\n\n## Requirements\n- [ ] Unit tests simulate bufferedAmount changes\n- [ ] Type-correct (`RTCDataChannel`)\n- [ ] No magic globals\n\n## ‚õìÔ∏è Blocks\n- Merge PR #1445\n\n## üîç Related\n- PR #1445\n"}
{"id":"3ea0cf32-04c9-42eb-a256-22dc03360f1a","title":"finalize migration plan md md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"3ea0cf32-04c9-42eb-a256-22dc03360f1a","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/finalize_migration_plan.md","content":"## üõ†Ô∏è Task: [[Finalize `MIGRATION_PLAN.md`]]\n\nGenerated from [../unique/2025.07.28.18.07.20.md](../unique/2025.07.28.18.07.20.md)\n\n---\n\n## üéØ Goals\n\n- [ ] Complete `MIGRATION_PLAN.md` with scope, phased timeline, requirements, and risk mitigation\n- [ ] Link migration plan to relevant epic and task file for traceability\n- [ ] Confirm stakeholder review and readiness for execution\n\n---\n\n## üì¶ Requirements\n\n- [ ] Milestones clearly enumerated with dependencies and mitigation strategies\n- [ ] Owners assigned to each subtask\n- [ ] Stakeholder sign-off recorded\n\n---\n\n## üìã Subtasks\n\n- [ ] Draft scope and phased timeline (@codex)\n- [ ] Document requirements, dependencies, and risks (@riatzukiza)\n- [ ] Cross-link task, epic, and `MIGRATION_PLAN.md` (@codex)\n- [ ] Review with stakeholders and mark task `#Ready` (@riatzukiza)\n\n---\n\nEpic: [[epics#üõ† Developer Tooling & Build Pipeline]]\n\n#Ready #doc-this #framework-core\n\n"}
{"id":"4153471e-77dc-43ab-b2a4-a4a3bed605fe","title":"create vault config obsidian with kanban and minimal vault setup 1 md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"4153471e-77dc-43ab-b2a4-a4a3bed605fe","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/create_vault-config_obsidian_with_kanban_and_minimal_vault_setup_1.md","content":"## üõ†Ô∏è Task: Create vault-config .obsidian with Kanban and minimal vault setup\n\nProvide a shareable `.obsidian` folder that new contributors can copy to get the\nsame Kanban board and plugin configuration. Keep the config minimal so it works\nacross different operating systems.\n\n---\n\n## üéØ Goals\n\n- Include the Kanban plugin as the only required community plugin\n- Document how to copy the config into a fresh clone\n- Ensure markdown files render correctly on GitHub without extra plugins\n\n---\n\n## üì¶ Requirements\n\n- [ ] Store config under `vault-config/.obsidian/`\n- [ ] Include `config` and `community-plugins.json`\n- [ ] Mention the setup command in `vault-config/README.md`\n- [ ] Keep plugin list short (kanban only by default)\n\n---\n\n## üìã Subtasks\n\n- [ ] Verify `.obsidian/config` enables Kanban view\n- [ ] Add usage notes to `docs/vault-config-readme.md`\n- [ ] Link this task from the main README onboarding section\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#done\n\n"}
{"id":"4154f6ba-0c33-4bbd-96fc-d6b64140333f","title":"enable scripts lint topics ts in ci md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"4154f6ba-0c33-4bbd-96fc-d6b64140333f","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/enable_scripts_lint_topics_ts_in_ci.md","content":"## üõ†Ô∏è Task: Enable **scripts/lint-topics.ts** in CI\n\nGenerated from [../unique/2025.08.08.20.08.52.md](../unique/2025.08.08.20.08.52.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"41fec47c-e499-461b-b7e0-4ef1c86bdabb","title":"design ollama model file for use with codex","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.511Z","uuid":"41fec47c-e499-461b-b7e0-4ef1c86bdabb","created_at":"2025-09-15T02:02:58.511Z","path":"docs/agile/tasks/design_ollama_model_file_for_use_with_codex.md","content":"## üõ†Ô∏è Task: Design Ollama model file for use with Codex\n\nHaving a preconfigured, pre-prompted model could help agents perform better as Codex models.\n\n---\n\n## üéØ Goals\n\n- Provide a model file tuned for Codex workflows\n- Demonstrate improvement over the base model\n\n---\n\n## üì¶ Requirements\n\n- [ ] Model file committed to version control\n- [ ] Evaluation proving the model outperforms the base configuration for our use case\n\n---\n\n## üìã Subtasks\n\n- [ ] Add model file to version control\n- [ ] Design a test to evaluate model effectiveness\n- [ ] Evaluate the base model against this test\n- [ ] Generate several prompts to test\n- [ ] Select a range of hyperparameters for each prompt\n- [ ] Write a report on the outcome\n\n---\n\n## üîç Relevant Links\n\n- [ChatGPT - Ollama modelfile for Codex](https://chatgpt.com/share/68a741c9-9fc0-8004-8780-6d0a048900f3)\n- [ChatGPT - Improving codex performance](https://chatgpt.com/share/68a741ec-0674-8004-a8ff-af09cf427462)\n- [ChatGPT - Config.toml guide](https://chatgpt.com/share/68a74210-8c1c-8004-9a3f-e41a94ba6ffa)\n- [[kanban]]\n\n#ice-box\n\n"}
{"id":"424f67f8-c949-437a-8a59-8a4eb3d77f77","title":"implement transcendence cascade framework core md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.515Z","uuid":"424f67f8-c949-437a-8a59-8a4eb3d77f77","created_at":"2025-09-15T02:02:58.515Z","path":"docs/agile/tasks/implement_transcendence_cascade_framework_core.md","content":"## üõ†Ô∏è Task: Implement transcendence cascade #framework-core\n\nGenerated from [../unique/2025.07.28.18.07.20.md](../unique/2025.07.28.18.07.20.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"42feae0b-46f6-4580-b9bf-071720c0dc79","title":"Organize shared libraries into focused groups","status":"todo","priority":"p3","owner":"","labels":["framework-core","refactor"],"created":"2025-09-27T00:12:00.000Z","uuid":"42feae0b-46f6-4580-b9bf-071720c0dc79","created_at":"2025-09-27T00:12:00.000Z","path":"docs/agile/tasks/organize-library-packages.md","content":"#Todo\n\n## üõ†Ô∏è Description\n\n- Plan the third wave of the package reorg, splitting general-purpose libraries into `packages/libs/runtime/` and `packages/libs/domain/` (or similar) buckets.\n\n## Description\n- **What changed?** Frontends moved to `packages/frontends/`, and services are queued for a dedicated namespace.\n- **Where is the impact?** Applies to reusable modules that currently sit at `packages/*` but are neither frontends nor deployable services.\n- **Why now?** Keeps the `packages/` root from regrowing into an unmanageable mix after the initial clean-up.\n- **Supporting context** Continuation of the multi-stage workspace organization effort.\n\n## Goals\n- Define criteria distinguishing runtime utilities from domain-specific modules.\n- Draft the folder hierarchy for library subgroups and confirm with stakeholders.\n- Enumerate packages that will move in each subgroup.\n\n## Requirements\n- [ ] Publish a proposal for the library taxonomy.\n- [ ] Update workspace discovery paths once structure is agreed.\n- [ ] Prepare migration notes and acceptance tests for affected packages.\n- [ ] Ensure documentation reflects the new layout.\n\n## Subtasks\n1. Audit current library packages and classify them into runtime vs. domain.\n2. Validate the taxonomy with maintainers and capture acceptance notes.\n3. Stage the move plan and identify automation/documentation updates needed.\n\nEstimate: 3\n\n---\n\n## üîó Related Epics\n\n- [[kanban]]\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- None\n\n## ‚õìÔ∏è Blocks\n\n- None\n\n---\n\n## üîç Relevant Links\n\n- Pending proposal.\n"}
{"id":"44157ed4-2b52-4573-90aa-2b1a4cd1ecf6","title":"extract site modules from riatzukiza github io md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"44157ed4-2b52-4573-90aa-2b1a4cd1ecf6","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/extract_site_modules_from_riatzukiza_github_io.md","content":"## üõ†Ô∏è Task: Extract site modules from riatzukiza.github.io\n\nPort any useful HTML, CSS, or JS from the legacy `riatzukiza.github.io` site into our `/sites/` directory.\n\n---\n\n## üéØ Goals\n\n- Retain frontend components that describe the project\n- Ensure they build locally under the Promethean repo\n- Phase out obsolete assets\n\n---\n\n## üì¶ Requirements\n\n- [ ] Identify which assets are still relevant\n- [ ] Copy them into `/sites/` preserving structure\n- [ ] Add README notes on build or deployment steps\n\n---\n\n## üìã Subtasks\n\n- [ ] Review the old repository's site folder\n- [ ] Migrate HTML/CSS/JS files\n- [ ] Adjust paths or links for the new repo layout\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- Extract docs from riatzukiza.github.io\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n  #accepted\n\n"}
{"id":"4508b108-1326-43bf-b58b-06883f70c9fc","title":"implement pause resume ops on gateway md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.515Z","uuid":"4508b108-1326-43bf-b58b-06883f70c9fc","created_at":"2025-09-15T02:02:58.515Z","path":"docs/agile/tasks/implement_pause_resume_ops_on_gateway.md","content":"## üõ†Ô∏è Task: Implement `PAUSE/RESUME` ops on gateway\n\nGenerated from [../unique/2025.08.08.19.08.25.md](../unique/2025.08.08.19.08.25.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"452f1408-fe8c-41fb-87ab-761e8dcee2d6","title":"fix makefile test target md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"452f1408-fe8c-41fb-87ab-761e8dcee2d6","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/fix_makefile_test_target.md","content":"## üõ†Ô∏è Task: Fix Makefile test target\n\nThe `test-python` target originally pointed to `tests/python/` but tests live in `tests/`.\nUpdate the path so that `pytest` runs against `tests/`.\n\n---\n\n## üéØ Goals\n- Ensure `make test` runs all Python tests\n- Prevent false confidence in CI results\n\n---\n\n## üì¶ Requirements\n- [ ] Update the Makefile target\n- [ ] Document correct usage in `readme.md`\n\n---\n\n## üìã Subtasks\n- [ ] Modify `test-python` path\n- [ ] Run `make test` locally to confirm\n\n---\n\n## üîó Related Epics\n#codex-task #testing\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\nNothing\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n#done\n\n"}
{"id":"457fd7a3-bc99-4de6-b9f3-06ef6cf00d5e","title":"Author @promethean/omni-protocol package","status":"in_progress","priority":"P1","owner":"","labels":["omni","package","typescript"],"created":"2025-09-21T02:45:00Z","uuid":"457fd7a3-bc99-4de6-b9f3-06ef6cf00d5e","created_at":"2025-09-21T02:45:00Z","path":"docs/agile/tasks/author-omni-protocol-package.md","content":"## üéØ Outcome\nShip the initial `@promethean/omni-protocol` package containing TypeScript interfaces, JSON schema emitters, error envelopes, and streaming event definitions that mirror the SmartGPT bridge contracts.\n\n## üì• Inputs\n- [[docs/architecture/omni/omni-protocol-spec.md]]\n- Existing `bridge/src/routes/v1` handlers and shared actions.\n\n## ‚úÖ Definition of Done\n- [ ] Package skeleton created under `packages/omni-protocol/` with GPL-3.0-only license metadata.\n- [ ] Interfaces + types exported per spec.\n- [ ] Runtime validators implemented with Zod (source-of-truth) and JSON Schema emitted via zod-to-json-schema for adapters.\n- [ ] Unit tests covering envelope validation and error helpers.\n- [ ] Published API docs stub linked from `docs/packages/`.\n## üöß Constraints\n- Maintain parity with legacy response shapes (e.g., `{ ok, base, entries }`).\n- No transport logic‚Äîpure data contracts only.\n\n## ü™ú Steps\n1. Scaffold package via workspace generator; wire build/test scripts.\n2. Translate spec tables into TS interfaces and runtime validators.\n3. Port success/error envelope helpers from SmartGPT bridge.\n4. Write AVA tests for sample payloads and error handling.\n5. Update docs + changelog entry.\n\n## üîó Dependencies\n- [[docs/agile/tasks/omni-unified-service-spec.md]] (spec must exist).\n"}
{"id":"45a4101d-01ab-497b-8920-97cc563e9351","title":"evaluate and reward flow satisfaction","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"45a4101d-01ab-497b-8920-97cc563e9351","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/evaluate_and_reward_flow_satisfaction.md","content":"## üõ†Ô∏è Task: Evaluate and reward flow satisfaction\n\nDevelop a metric for how \"smooth\" an interaction feels and use it to\nreinforce the agent. This could combine response latency, emotional\nstability from Eidolon, and user feedback.\n\nGenerated from [../unique/2025.07.28.18.07.20.md](../unique/2025.07.28.18.07.20.md)\n\n---\n\n## üéØ Goals\n\n- Quantify conversation quality with a simple score\n- Feed that score back into the agent as a reward signal\n- Log scores for later analysis\n\n---\n\n## üì¶ Requirements\n\n- [ ] Capture turn duration and message sentiment\n- [ ] Expose current Eidolon emotional metrics\n- [ ] Store scores in `data/eidolon/flow_scores.csv`\n\n---\n\n## üìã Subtasks\n\n- [ ] Instrument Cephalon to timestamp message flow\n- [ ] Add a small Python module to compute satisfaction (0‚Äë1 scale)\n- [ ] Update Eidolon to accept a reward event\n- [ ] Write a minimal visualization notebook\n- [ ] Link baseline metrics from [[eidolon-field-math]]\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- Needs baseline emotion metrics from [[eidolon-field-math|Eidolon Fields]]\n\n## ‚õìÔ∏è Blocks\n\n- Future reinforcement-learning loops\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n## ‚ùì Questions\n\n- Should user feedback be captured explicitly or inferred from message content?\n- Do we need real-time updates to the reward signal or batched summaries?\n#IceBox\n#rejected\n\n"}
{"id":"46303bb8-ac2d-4083-96c5-3f2e5a7a317b","title":"migrate server side sibilant libs to promethean ar md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.516Z","uuid":"46303bb8-ac2d-4083-96c5-3f2e5a7a317b","created_at":"2025-09-15T02:02:58.516Z","path":"docs/agile/tasks/migrate_server_side_sibilant_libs_to_promethean_ar.md","content":"## üõ†Ô∏è Task: Migrate server side sibilant libs to Promethean architecture.\n\nGenerated from [../unique/2025.07.28.18.07.20.md](../unique/2025.07.28.18.07.20.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"4686ef75-1681-4fa5-a95d-ad1c7d1cb62e","title":"add unit tests for wav processing md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.506Z","uuid":"4686ef75-1681-4fa5-a95d-ad1c7d1cb62e","created_at":"2025-09-15T02:02:58.506Z","path":"docs/agile/tasks/add_unit_tests_for_wav_processing.md","content":"## üõ†Ô∏è Task: Add unit tests for wav_processing\n\nImprove coverage of `shared/py/utils/wav_processing.py`.\n\n---\n\n## üéØ Goals\n- Exercise batch conversion and resampling functions\n- Ensure correct handling of edge cases\n\n---\n\n## üì¶ Requirements\n- [ ] Provide small sample wav fixtures\n- [ ] Use pytest\n\n---\n\n## üìã Subtasks\n- [ ] Test `batch_convert`\n- [ ] Test `resample`\n\n---\n\n## üîó Related Epics\n#codex-task #testing\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\nNothing\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n- [Untested modules](../../untested-code.md)\n#done\n\n"}
{"id":"469ea721-b00c-4752-b3ad-9e85bd1f9ff9","title":"switch gateway auth to jwt generate temp hs256 tok md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.522Z","uuid":"469ea721-b00c-4752-b3ad-9e85bd1f9ff9","created_at":"2025-09-15T02:02:58.522Z","path":"docs/agile/tasks/switch_gateway_auth_to_jwt_generate_temp_hs256_tok.md","content":"## üõ†Ô∏è Task: Switch gateway auth to JWT; generate temp HS256 token for dev\n\nGenerated from [../unique/2025.08.08.19.08.49.md](../unique/2025.08.08.19.08.49.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"46d394da-ae41-44a8-8bea-47030a15802c","title":"auth migrate services to jwt","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"46d394da-ae41-44a8-8bea-47030a15802c","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/auth_migrate_services_to_jwt.md","content":"Auth: Migrate services to JWT-based auth\n\nGoal: Replace ad hoc/static tokens with OAuth2 client_credentials + JWT across internal HTTP endpoints.\n\nScope (initial wave):\n- SmartGPT Bridge: require Bearer JWT; verify scopes: `indexer:write`, `indexer:remove`, `search:read`.\n- File Watcher: request token and attach Authorization header.\n- Heartbeat service (JS): if any HTTP endpoints exposed later, protect; otherwise, skip.\n- Cephalon HTTP endpoints (if any) + internal bridges.\n- Broker (WS): evaluate minimal token on connect (optional phase 2).\n\nExit Criteria:\n- Services accept/verify JWT; file-watcher and bridge communicate successfully.\n- Docs updated to remove static token guidance.\n\n#incoming #auth #migration #services\n\n\n"}
{"id":"48d398a7-9f4e-4c2a-8b15-3d7e8f9c2a1b","title":"Migrate kanban system from JSONL to level-cache for memory efficiency","status":"breakdown","priority":"P1","owner":"","labels":["kanban","architecture","performance","level-cache","memory-optimization"],"created":"2025-10-05T00:00:00.000Z","uuid":"48d398a7-9f4e-4c2a-8b15-3d7e8f9c2a1b","created_at":"2025-10-05T00:00:00.000Z","path":"docs/agile/tasks/migrate-kanban-from-jsonl-to-level-cache.md","content":"\n## üõ†Ô∏è Task: Migrate kanban system from JSONL to level-cache for memory efficiency\n\n## üêõ Problem Statement\n\nThe current kanban system loads **all tasks into memory** using a JSONL index file, causing:\n- **Memory exhaustion**: Heap crashes with 300+ tasks (demonstrated in current session)\n- **Poor scalability**: O(n) memory usage regardless of operation\n- **No streaming**: All operations require full dataset in memory\n- **Performance degradation**: Linear search through all tasks for queries\n\n## üéØ Desired Outcome\n\n### Performance Goals:\n- **O(1) memory usage** regardless of task count (10, 100, 1000+ tasks)\n- **O(log n) query performance** vs current O(n) scanning\n- **Streaming operations** for large result sets\n- **Constant-time lookups** by UUID, status, priority, labels\n\n### Functional Goals:\n- **Backward compatibility**: Maintain existing CLI API surface\n- **Zero-downtime migration**: Seamless transition from JSONL to level-cache\n- **Cache management**: TTL, cleanup, rebuild capabilities\n- **Reliability**: No more heap exhaustion crashes\n\n## üìã Requirements\n\n### Phase 1: TaskCache Interface Design\n- [ ] Create `TaskCache` interface abstracting level-cache operations\n- [ ] Define cache key structure for task indexing\n- [ ] Design streaming APIs for large result sets\n- [ ] Plan backward-compatible API surface\n\n### Phase 2: Cache Schema Design\n```typescript\n// Cache structure design:\ntasks/by-id/{uuid}           ‚Üí IndexedTask (full task data)\ntasks/by-status/{status}     ‚Üí Set<uuid> (status indexes)\ntasks/by-priority/{priority} ‚Üí Set<uuid> (priority indexes)\ntasks/by-label/{label}       ‚Üí Set<uuid> (label indexes)\ntasks/search-index/{term}    ‚Üí Set<uuid> (full-text search)\nmeta/last-indexed            ‚Üí timestamp (cache metadata)\nmeta/task-count              ‚Üí number (cache statistics)\n```\n\n### Phase 3: Core Implementation\n- [ ] Implement `TaskCache` class using `@promethean/level-cache`\n- [ ] Create task indexing and storage methods\n- [ ] Implement query methods (by UUID, status, priority, labels)\n- [ ] Add streaming result iteration\n- [ ] Implement cache rebuild/refresh functionality\n\n### Phase 4: API Migration\n- [ ] Update `packages/kanban/src/board/indexer.ts` to use TaskCache\n- [ ] Modify `indexTasks` to return TaskCache instead of Array<IndexedTask>\n- [ ] Update `searchTasks`, `getColumn`, etc. to use streaming APIs\n- [ ] Maintain backward compatibility for CLI commands\n- [ ] Add migration utilities and fallback handling\n\n### Phase 5: Configuration & Migration\n- [ ] Add `cachePath` to kanban config (default: `.kanban/cache`)\n- [ ] Create migration script: JSONL ‚Üí level-cache\n- [ ] Add cache validation and repair tools\n- [ ] Update documentation for new architecture\n- [ ] Add cache management CLI commands\n\n### Phase 6: Testing & Performance\n- [ ] Unit tests for TaskCache operations\n- [ ] Integration tests with large datasets (1000+ tasks)\n- [ ] Memory usage profiling and validation\n- [ ] Performance benchmarks vs JSONL approach\n- [ ] CLI compatibility testing\n\n## üèóÔ∏è Implementation Plan\n\n### Step 1: TaskCache Interface\n```typescript\ninterface TaskCache {\n  // Direct access\n  getTask(uuid: string): Promise<IndexedTask | undefined>\n  hasTask(uuid: string): Promise<boolean>\n\n  // Query operations (streaming)\n  getTasksByStatus(status: string): AsyncIterable<IndexedTask>\n  getTasksByPriority(priority: string): AsyncIterable<IndexedTask>\n  getTasksByLabel(label: string): AsyncIterable<IndexedTask>\n  searchTasks(query: string): AsyncIterable<IndexedTask>\n\n  // Cache management\n  indexTasks(tasks: Iterable<IndexedTask>): Promise<number>\n  rebuildIndex(): Promise<void>\n  sweepExpired(): Promise<number>\n  close(): Promise<void>\n}\n```\n\n### Step 2: Replace Memory-Heavy Operations\n- **Before**: `indexTasks()` ‚Üí `Array<IndexedTask>` (all in memory)\n- **After**: `indexTasks()` ‚Üí `TaskCache` (key-based access)\n\n### Step 3: Streaming Query Results\n- **Before**: `getColumn(board, column)` ‚Üí `Array<Task>` (loaded all)\n- **After**: `getColumn(cache, column)` ‚Üí `AsyncIterable<Task>` (streaming)\n\n## üìä Cache Schema Design\n\n### Key Structure:\n```typescript\n// Namespaced keys using level-cache\n\"tasks/by-id/123e4567-e89b-12d3-a456-426614174000\"\n\"tasks/by-status/todo\"           ‚Üí Set<uuid>\n\"tasks/by-priority/P2\"           ‚Üí Set<uuid>\n\"tasks/by-label/kanban\"          ‚Üí Set<uuid>\n\"tasks/search-index/keyword\"     ‚Üí Set<uuid>\n\"meta/last-indexed\"              ‚Üí timestamp\n```\n\n### Value Types:\n```typescript\n// Full task data (only stored once per task)\ntype CachedTask = IndexedTask\n\n// Index sets (UUID arrays for fast lookups)\ntype UUIDSet = string[]\n\n// Metadata\ntype CacheMeta = {\n  lastIndexed: number\n  taskCount: number\n  version: string\n}\n```\n\n## üîÑ Migration Strategy\n\n### Zero-Downtime Approach:\n1. **Dual mode**: Support both JSONL and level-cache during transition\n2. **Automatic migration**: Detect JSONL, migrate to level-cache on first run\n3. **Fallback handling**: Graceful degradation if level-cache fails\n4. **Validation**: Verify migrated data integrity\n\n### Migration Steps:\n1. Create level-cache instance\n2. Read existing JSONL index (if exists)\n3. Populate level-cache with task data and indexes\n4. Validate migrated data completeness\n5. Switch to level-cache as primary store\n6. Archive/remove JSONL file (optional)\n\n## ‚ö° Performance Benefits\n\n### Memory Usage:\n- **Current**: O(n) where n = total tasks (316+ tasks = ~50MB+)\n- **After**: O(1) constant memory (~1MB cache overhead)\n\n### Query Performance:\n- **Current**: O(n) linear scan through all tasks\n- **After**: O(log n) key-based lookup + streaming\n\n### Scalability:\n- **Current**: Fails at ~300+ tasks (heap exhaustion)\n- **After**: Handles 10,000+ tasks with constant memory\n\n## üîó Related Resources\n\n- **Current code**: `packages/kanban/src/board/indexer.ts`\n- **Memory issue**: JSONL index loading all tasks into memory\n- **Level cache**: `packages/level-cache/src/index.ts` (Cache interface)\n- **Config**: `promethean.kanban.json` (needs cachePath addition)\n- **CLI commands**: All kanban operations need updating\n\n## ‚úÖ Acceptance Criteria\n\n1. **Memory Efficiency**: Kanban operations work with 1000+ tasks without memory issues\n2. **Performance**: Query operations complete in <100ms regardless of task count\n3. **Backward Compatibility**: All existing CLI commands work unchanged\n4. **Migration**: Automatic JSONL ‚Üí level-cache migration with data integrity\n5. **Reliability**: No more heap exhaustion crashes during normal operations\n6. **Documentation**: Updated architecture docs and cache management guide\n\n## üìù Technical Notes\n\n### level-cache Interface (reviewed):\n```typescript\ninterface Cache<T> {\n  get(key: string): Promise<T | undefined>\n  set(key: string, value: T, opts?: PutOptions): Promise<void>\n  del(key: string): Promise<void>\n  batch(ops: Array<PutOp|DelOp>): Promise<void>\n  entries(opts?: {limit?: number}): AsyncGenerator<[string, T]>\n  sweepExpired(): Promise<number>\n  withNamespace(ns: string): Cache<T>\n  close(): Promise<void>\n}\n```\n\n### Key Advantages:\n- **Namespaces**: Logical separation (tasks/, meta/, indexes/)\n- **TTL**: Automatic expiration of stale cache entries\n- **Batch operations**: Efficient bulk updates\n- **Streaming**: `entries()` provides AsyncGenerator for iteration\n- **No background processes**: Predictable resource usage\n\nThis migration will solve the fundamental scaling issue causing current memory exhaustion and provide a robust foundation for future kanban system growth."}
{"id":"49a002af-c299-402a-b922-8308556c437b","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.509Z","uuid":"49a002af-c299-402a-b922-8308556c437b","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/connect-bluesky.md","content":"# Description\n\nEnable agents to interact with Bluesky using the AT Protocol APIs and app-password authentication.\n\n## Target APIs and Authentication\n\n- **API**: [Bluesky AT Protocol](https://docs.bsky.app/docs/api) via `https://bsky.social/xrpc`\n- **Authentication**: App password exchange for session token (JWT)\n\n## Data Flow & Rate Limiting\n\n- Agent authenticates ‚ûú session token ‚ûú Bluesky endpoint ‚ûú internal broker/storage\n- Monitor `x-ratelimit-*` headers and throttle (~200 requests/minute). Back off when `429` is returned.\n\n## Requirements/Definition of done\n\n- Authenticated session can publish and read posts\n- Data flows into broker or storage services\n- Rate limit handling prevents API throttling\n\n## Tasks\n\n- [ ] Create app password and configure credentials\n- [ ] Implement session login and client wrappers\n- [ ] Publish and fetch posts via internal queue\n- [ ] Implement rate limit checks and exponential backoff\n- [ ] Integration tests for posting and retrieval\n- [ ] Unit tests for session refresh and error paths\n\n## Dependencies\n\n- AT Protocol client library\n- Bluesky developer account\n\n## Rough Scope\n\n- Authenticate with Bluesky using AT Protocol\n- Implement message retrieval and posting\n- Map data into existing messaging schema\n\n## Estimate\n\n- Story points: 3\n\n## Relevent resources\n\nYou might find [Bluesky API docs](https://docs.bsky.app/docs/api) useful while working on this task\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n\n#Ready\n#ready\n\n"}
{"id":"4a43e0ed-ae0d-4857-825b-c178d600ba09","title":"add prometheus events counters in ws server hook p md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.505Z","uuid":"4a43e0ed-ae0d-4857-825b-c178d600ba09","created_at":"2025-09-15T02:02:58.505Z","path":"docs/agile/tasks/add_prometheus_events_counters_in_ws_server_hook_p.md","content":"## üõ†Ô∏è Task: Add Prometheus `events_*` counters in WS server hook points\n\nGenerated from [../unique/2025.08.08.15.08.47.md](../unique/2025.08.08.15.08.47.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"4abed2f3-350f-4ec5-ac22-7e9d3882a7c9","title":"summarize clarified priorities for next sprint md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.522Z","uuid":"4abed2f3-350f-4ec5-ac22-7e9d3882a7c9","created_at":"2025-09-15T02:02:58.522Z","path":"docs/agile/tasks/summarize_clarified_priorities_for_next_sprint.md","content":"## üõ†Ô∏è Task: Summarize clarified priorities for next sprint\n\nAfter the alignment meeting, document the agreed-upon priorities and share them with the team.\n\n---\n\n## üéØ Goals\n- Produce a concise list of next-sprint priorities\n- Ensure all collaborators understand the focus areas\n\n---\n\n## üì¶ Requirements\n- [ ] Meeting notes from the alignment session\n\n- [ ] Summary saved as `docs/notes/next-sprint-priorities.md`\n\n---\n\n## üìã Subtasks\n- [ ] Review meeting notes for decisions\n- [ ] Draft the summary document\n- [ ] Circulate summary for confirmation\n\n---\n\n## üîó Related Epics\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n- Schedule alignment meeting with stakeholders\n\n## ‚õìÔ∏è Blocks\nNothing\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n\n#reject\n\n## Reason for rejection\n#rejected\n\n"}
{"id":"4ad8d468-babf-4ab8-9385-fec57642c074","title":"move discord scraper to ts","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.517Z","uuid":"4ad8d468-babf-4ab8-9385-fec57642c074","created_at":"2025-09-15T02:02:58.517Z","path":"docs/agile/tasks/move-discord-scraper-to-ts.md","content":"## üõ†Ô∏è Description\n**Status:** blocked\n\nPort the existing Discord scraper to TypeScript to align with the rest of the JS toolchain.\n\n---\n\n## üéØ Goals\n\n- Maintain current scraping capabilities in a TS implementation.\n- Simplify maintenance by using a single language stack.\n\n---\n\n## üì¶ Requirements\n\n- [ ] Feature parity with current scraper.\n- [ ] Type-safe interfaces and error handling.\n- [ ] Automated tests verifying Discord interactions.\n\n---\n\n## üìã Subtasks\n\n- [ ] Review the current scraper and identify core features.\n- [ ] Scaffold a TypeScript project with required dependencies.\n- [ ] Re-implement scraping logic in TS.\n- [ ] Add unit tests for message and attachment collection.\n- [ ] Document how to run the new scraper and remove the old one.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#accepted\n\n## Blockers\n- No active owner or unclear scope\n\n#breakdown\n\n"}
{"id":"4af34fd8-8f94-4013-ad4f-61bd0ce13c78","title":"make seperate execution pathways 1 md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.516Z","uuid":"4af34fd8-8f94-4013-ad4f-61bd0ce13c78","created_at":"2025-09-15T02:02:58.516Z","path":"docs/agile/tasks/make_seperate_execution_pathways_1.md","content":"## üõ†Ô∏è Description\n\nI have a GPU on my desktop, the target production environment, but testing environments don't\nThey may not even be able to use transformers from the looks of it in node.\n\nWe had to get rid of the default chroma embeddings function to make the embeder service work\n\nWe may have to make a seperate service for embeddings in python.\n\nI think that was mostly a javascript problem.\n\nIf we just don't do any ML in js for  now,  we shouldn't have any problems.\n\n\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Add Ollama formally to pipeline\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#done\n\n"}
{"id":"4b95c5fc-86cc-486e-a5df-c3dc7ebee209","title":"clearly standardize data models","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.509Z","uuid":"4b95c5fc-86cc-486e-a5df-c3dc7ebee209","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/clearly-standardize-data-models.md","content":"## üõ†Ô∏è Description\n\nDefine shared data models (e.g., User, Policy, File, Directory) that can be imported across services and languages.\n\n---\n\n## üéØ Goals\n\n- Provide a single source of truth for core entities\n- Enable consistent serialization and validation\n\n---\n\n## üì¶ Requirements\n\n- [ ] Canonical schemas published in a shared library\n- [ ] Language bindings for TypeScript and Python\n- [ ] Documentation for versioning and migration\n\n---\n\n## üìã Subtasks\n\n- [ ] Identify and document required entities\n- [ ] Design schema definitions (e.g., JSON Schema)\n- [ ] Implement library with generated types\n- [ ] Update services to consume shared models\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#framework-core #Ready\n#ready\n\n"}
{"id":"4beb36b0-7cc5-4116-a516-3927615031be","title":"Pin versions in configs","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.517Z","uuid":"4beb36b0-7cc5-4116-a516-3927615031be","created_at":"2025-09-15T02:02:58.517Z","path":"docs/agile/tasks/pin_versions_in_configs.md","content":"Got it. Here‚Äôs a practical expansion you can paste into the board and hand to Codex. It‚Äôs opinionated, deterministic, and aims to stop every ‚Äúoops it upgraded‚Äù class of failure.\n\n# Pin versions in configs\n\n**Owner:** Codex / Agent\n**Status:** Planned\n**Labels:** #ops #codex-task #release-engineering #ci #sre #supply-chain #promethean\n\n---\n\n## üõ†Ô∏è Task\n\nEnsure Promethean and Codex configs **explicitly pin** model, image, tool, and library versions. No floating tags, no implicit latest, no wildcard ranges.\n\n---\n\n## Why this matters (short and blunt)\n\n* Silent upgrades = ghost bugs.\n* Reproducibility > vibes.\n* If CI and prod don‚Äôt resolve to the **same** artifacts, your incident postmortems write themselves.\n\n---\n\n## üéØ Goals\n\n* All services resolve dependencies and models to **exact** versions/digests.\n* A **single source of truth** (matrix) lives in git for human review.\n* CI **fails** when an unpinned ref sneaks in.\n\n---\n\n## üì¶ Requirements\n\n* [ ] **Exact semver** for JS/TS (`\"1.2.3\"`), **no** `^`, `~`, `*`, ranges, or `latest`.\n* [ ] **Python locked** with `uv` (project uses `uv` + in-repo `.venv`): `pyproject.toml` + committed `uv.lock`. No bare `requirements.txt` without hashes.\n* [ ] **Docker images pinned** by **digest** (`image: node:22.5.1@sha256:...`). No floating majors/minors.\n* [ ] **GitHub Actions pinned** to **commit SHAs**, not `@v4` tags.\n* [ ] **Ollama/OpenVINO/Whisper/TTS/Embed** models pinned to **explicit tags or artifact hashes**. No ‚Äúlatest‚Äù or unqualified model names.\n* [ ] **System packages** pinned (`apt-get install pkg=1.2.3-*`) or vendor apt repo with version pin + `apt-mark hold`.\n* [ ] **Lockfiles committed** (`package-lock.json` or `pnpm-lock.yaml`, `uv.lock`, etc.).\n* [ ] **Engines/toolchains** declared and enforced (Node, Python, `uv`, `npm`/`pnpm`) across dev/CI.\n* [ ] **Version Matrix** doc (human-readable) committed and referenced by CI.\n\n---\n\n## üìã Subtasks\n\n* [ ] **Inventory & Audit**\n\n  * [ ] Locate all dependency surfaces:\n\n    * JS/TS: `package.json`, lockfiles, `pnpm-workspace.yaml`\n    * Python: `pyproject.toml`, `uv.lock`, any `requirements*.txt`\n    * Docker: `Dockerfile`, `docker-compose.yml`, k8s manifests\n    * CI: `.github/workflows/*.yml` actions refs\n    * System: any `apt`, `apk`, `dnf` invocations\n    * Models: Ollama model lists, OpenVINO IRs, Whisper/TTS configs\n    * Tools: PM2 ecosystem, Makefile/Hy wrappers (`tools/build.hy`), `ecosystem.config.js`\n  * [ ] Produce a quick diffable list of offenders (floating tags, ranges, latest).\n* [ ] **Define the Version Matrix**\n\n  * [ ] Create `docs/ops/VERSION_MATRIX.md` with sections:\n\n    * **Runtimes** (Node, Python, uv)\n    * **JS/TS deps** (critical libs pinned)\n    * **Python deps** (critical libs pinned)\n    * **Containers** (base image + digest)\n    * **System packages**\n    * **Models** (Ollama, OpenVINO IR/ops-set, Whisper/TTS, Embeddings)\n    * **CI actions** (action\\@commit SHA)\n  * [ ] Add rationale/compatibility notes (e.g., OpenVINO opset vs model).\n* [ ] **Pin Everything**\n\n  * [ ] JS/TS: replace `^`/`~` with exact; run `npm i --package-lock-only` (or `pnpm install --frozen-lockfile`) and commit.\n  * [ ] Python: lock with `uv lock`; commit `uv.lock`; forbid bare `pip install`.\n  * [ ] Docker: switch to digests (`FROM node:22.5.1@sha256:...`); update compose/manifests.\n  * [ ] CI: replace `uses: actions/checkout@v4` ‚Üí `@<full SHA>`; document how to bump.\n  * [ ] Models:\n\n    * **Ollama**: use explicit tags (`qwen2.5-coder:7b-instruct-q5_K_M`), capture with `ollama show <model> --modelfile` and record source SHA if available.\n    * **OpenVINO**: pin IR artifacts (store under versioned path), record framework/opset.\n    * **Whisper/TTS/Embeddings**: pin to model filename or release ID; store checksums.\n  * [ ] System packages: set explicit versions and `apt-mark hold` where feasible.\n* [ ] **Guardrails**\n\n  * [ ] Add `scripts/ci/check-pins.{js,py}` (pick one) that scans for:\n\n    * `latest`, unqualified `FROM`, `uses: ...@v\\d+`, semver ranges, unpinned apt, unqualified model names.\n  * [ ] Add a pre-commit hook + CI step that **fails** on violations.\n* [ ] **Docs & Runbooks**\n\n  * [ ] `docs/ops/VERSIONING_POLICY.md`: how to bump + verify + roll back.\n  * [ ] Update `README`/`CONTRIBUTING` with ‚ÄúNo floating versions‚Äù rule.\n* [ ] **Commit**\n\n  * [ ] Batch the change by service to keep diffs reviewable.\n  * [ ] Include matrix + lockfiles in each PR.\n\n---\n\n## üß© Design Notes / Conventions\n\n* **JS/TS**\n\n  * Use exact semver: `\"1.2.3\"`.\n  * Enforce with `npm ci`/`pnpm fetch` in CI; never `npm install` without a lockfile.\n* **Python**\n\n  * `pyproject.toml` ‚Üí `uv lock` ‚Üí commit `uv.lock`.\n  * For any `requirements.txt`, include `--require-hashes` + pinned hashes or delete in favor of uv.\n* **Docker**\n\n  * Always pin base images to **digest** and specific tag (both).\n  * Avoid multi-stage using different floating parents; pin all stages.\n* **CI**\n\n  * Pin each `uses:` to full SHA; add a comment with the tag for humans:\n\n    ```yaml\n    uses: actions/checkout@b4ffde... # v4.1.7\n    ```\n* **Models**\n\n  * Treat models like binaries: versioned path, checksum file (`.sha256`), and provenance (download URL or commit).\n  * Keep a `models/README.md` describing each model‚Äôs intent and compatibility.\n* **Tools & Engines**\n\n  * Add `\"engines\": { \"node\": \"22.5.1\" }` to root `package.json`; enforce with `engine-strict` in `.npmrc`.\n  * Record Python `3.x.y` in `VERSION_MATRIX.md` and enforce in CI image.\n\n---\n\n## ‚úÖ Acceptance Criteria\n\n* [ ] No floating versions in **any** config (JS/TS, Python, Docker, CI, models, system).\n* [ ] Lockfiles committed and verified by CI (`npm ci`/`pnpm install --frozen-lockfile`, `uv sync --frozen`).\n* [ ] All Docker/compose/k8s images use **tag+digest**.\n* [ ] All GitHub Actions pinned to **commit SHAs**.\n* [ ] ‚ÄúPins linter‚Äù runs in CI and **fails** on violations.\n* [ ] `docs/ops/VERSION_MATRIX.md` and `docs/ops/VERSIONING_POLICY.md` exist and are referenced from the repo root docs.\n* [ ] Rollback procedure documented (how to revert a pin bump safely).\n\n---\n\n## üß™ Suggested Automation (safe one-liners)\n\n> These are guardrails. Use them to find offenders; don‚Äôt blindly apply.\n\n* **Find semver ranges (JS/TS):**\n\n  ```bash\n  jq -r '.dependencies + .devDependencies | to_entries[] | select(.value|test(\"^[~^]\")) | \"\\(.key): \\(.value)\"' package.json\n  ```\n* **Detect `latest` & unpinned images:**\n\n  ```bash\n  rg -n 'FROM .*:latest|image: .*:latest|FROM [^@]+:[^@]+$' --glob '!node_modules'\n  ```\n* **Find unpinned GH Actions:**\n\n  ```bash\n  rg -n 'uses:\\s+\\S+@v\\d+(\\.\\d+)?$' .github/workflows\n  ```\n* **Find apt installs without version:**\n\n  ```bash\n  rg -n 'apt(-get)?\\s+install(?!.*=)' --glob '!node_modules'\n  ```\n* **Flag unqualified model names (Ollama et al.):**\n\n  ```bash\n  rg -n '\\bollama (run|pull)\\s+\\w+(:\\S+)?$' services | rg -v ':'\n  ```\n\n---\n\n## üóÇÔ∏è Deliverables / Files to touch\n\n* `docs/ops/VERSION_MATRIX.md` (new)\n* `docs/ops/VERSIONING_POLICY.md` (new)\n* `.github/workflows/*` (pin to SHAs + add check step)\n* `Dockerfile`, `docker-compose.yml`, k8s manifests (digests)\n* `package.json` (+ lockfile) across services\n* `pyproject.toml`, `uv.lock` across Python services\n* Model manifests: `models/**/manifest.json`, checksums\n* Pre-commit / CI linter: `scripts/ci/check-pins.js` (or `.py`)\n* Makefile / `tools/build.hy`: add frozen installs and version guards\n\n---\n\n## Commit template\n\n```\nchore(versions): pin runtimes, deps, images, models; add pins linter\n\n- Pin JS/TS deps to exact versions; commit lockfiles\n- Lock Python with uv; commit uv.lock\n- Pin Docker images to tag+digest\n- Pin GitHub Actions to commit SHAs\n- Add scripts/ci/check-pins and wire into CI\n- Add docs/ops/VERSION_MATRIX.md + VERSIONING_POLICY.md\n```\n\n---\n\n## üîó Related Epics\n\n* \\#ops\n* \\#codex-task\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n* Nothing\n\n## ‚õìÔ∏è Blocks\n\n* Reproducible CI/CD\n* Deterministic perf testing\n* Supply-chain controls\n\n---\n\n## ‚ö†Ô∏è Pitfalls & Guardrails\n\n* **Digest drift**: when bumping a tag, always update the digest; never rely on tag alone.\n* **Actions confusion**: tags can be force-moved; use SHAs and annotate with the tag in a comment.\n* **Model ambiguity**: different quantizations ‚â† same model; record quantization and build flags.\n* **Local vs CI**: enforce engines; otherwise ‚Äúworks on my machine‚Äù syndrome returns.\n* **Hidden installers**: scripts that call `pip install`/`npm i` at runtime undo reproducibility‚Äîpurge them or freeze.\n\n\\#tags #promethean #versioning #pinning #ci #docker #uv #ollama #openvino #sre #supplychain\n\n## Notes\n- Tests or documentation are missing; acceptance criteria not fully met.\n- Story Points: 5\n\n#in-progress\n\n"}
{"id":"4bf47f12-8489-46ea-bee4-ee5d826541e1","title":"Setup MCP exec server from approved commands","status":"done","priority":"P2","owner":"","labels":["MCP","Tooling"],"created":"2025-10-02T02:15:00.000Z","uuid":"4bf47f12-8489-46ea-bee4-ee5d826541e1","created_at":"2025-10-02T02:15:00.000Z","path":"docs/agile/tasks/setup-mcp-exec-server.md","content":"#InProgress\n\n## üõ†Ô∏è Description\n\n- Provision an MCP server endpoint that exposes a safe exec tool constrained to a vetted command allowlist.\n\n## Description\n- **What changed?** Need to add an allowlisted exec tool and wire it into the MCP server catalog/configuration.\n- **Where is the impact?** `@promethean/mcp` package plus root MCP configuration files.\n- **Why now?** Enables codex clients to run deterministic shell pipelines without broad command execution rights.\n- **Supporting context** `promethean.mcp.json` configuration, MCP tool registry docs.\n\n## Goals\n- Provide an `exec.run` MCP tool that only executes commands from a JSON allowlist.\n- Expose the tool via HTTP endpoint so clients can call it.\n- Cover parsing + execution behaviours with AVA tests.\n\n## Requirements\n- [x] test X passes: `pnpm --filter @promethean/mcp test`\n- [x] doc Y updated: Update MCP configuration docs if necessary.\n- [ ] PR merged: (link TBD)\n- [x] Additional constraints or non-functional requirements are addressed: ensure timeout + output capture guardrails.\n\n## Subtasks\n1. Design command allowlist schema and loader.\n2. Implement exec tool(s) leveraging allowlist.\n3. Register tool(s) and update configuration + tests.\n\nEstimate: 3\n\n---\n\n## üîó Related Epics\n\n- [[kanban]]\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- None\n\n## ‚õìÔ∏è Blocks\n\n- None\n\n---\n\n## üîç Relevant Links\n\n- `packages/mcp` module docs\n"}
{"id":"4c60df2e-78cf-400c-a950-c56c534118ac","title":"Task: Break down cephalon voice commands file","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"4c60df2e-78cf-400c-a950-c56c534118ac","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/breakdown-cephalon-voice-commands-file-using-ecs.md","content":"# Task: Break down cephalon voice commands file\n\nThis file is *really* complex, and we just added an ECS to deal with it. So we're dealing with it.\n\n## Notes\n- Tests or documentation are missing; acceptance criteria not fully met.\n- Story Points: 3\n\n#in-progress\n\n"}
{"id":"4c9726aa-9547-408b-a6db-2eebbc81d774","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.509Z","uuid":"4c9726aa-9547-408b-a6db-2eebbc81d774","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/ci-passses.md","content":"# Description\n\nDescribe your task\n\n## Requirements/Definition of done\n\n- If it doesn't have this, we can't accept it\n\n## Tasks \n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Relevent resources\n\nYou might find [this] useful while working on this task\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n"}
{"id":"4e7f70bb-bb32-4a55-b499-bdc347afe21a","title":"tamper monkey script for using templates defined in the vault","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.522Z","uuid":"4e7f70bb-bb32-4a55-b499-bdc347afe21a","created_at":"2025-09-15T02:02:58.522Z","path":"docs/agile/tasks/tamper-monkey-script-for-using-templates-defined-in-the-vault.md","content":"## üõ†Ô∏è Description\n\nI want to be able to use templates from the vault to ask for things like:\n\nWork on {task from board}\n\nBasically auto complete, turn the chat interfaces into little IDEs with auto complete that are aware of the structure of my vault and able to upload/download files super easy peasy.\n\nAs long as we don't use the scripts to send the message, we are gucci\n\n## üì¶ Requirements\n- Tampermonkey script loads templates from the vault.\n- Chat interfaces offer autocomplete for tasks using those templates.\n- Support simple upload and download interactions with the vault.\n\n## ‚úÖ Acceptance Criteria\n- Typing a task phrase suggests a corresponding template.\n- Selected template inserts content into the chat input.\n- Files can be uploaded from and saved to the vault through the script.\n\n## Tasks\n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Relevent resources\nYou might find [this] useful while working on this task\n\n## Comments\nUseful for agents to engage in append only conversations about this task.\n\n## Story Points\n\n- Estimate: 2\n- Assumptions: Vault templates are organized and accessible.\n- Dependencies: Tampermonkey environment and vault API endpoints.\n\n"}
{"id":"4e8f0a2b-3c4d-5e6f-8a9b-1c2d3e4f5a6b","title":"duck-web ‚Äî PCM16k worklet + mic wiring fixes","status":"in_progress","priority":"P1","owner":"","labels":["duck-web","audio","worklet"],"created":"2025-10-02T00:00:00.000Z","uuid":"4e8f0a2b-3c4d-5e6f-8a9b-1c2d3e4f5a6b","created_at":"2025-10-02T00:00:00.000Z","path":"docs/agile/tasks/pcm16k-worklet-and-mic-fixes.md","content":"#Todo\n\n## üõ†Ô∏è Description\nFix syntax errors in `pcm16k-worklet.js`, track fractional position to avoid drift; correct imports in `mic.ts`; reuse `duck-audio` helpers.\n\n## Requirements\n- [ ] `registerProcessor('pcm16k', ...)` correct\n- [ ] Drift-free decimator\n- [ ] `float32ToInt16` from `duck-audio`\n- [ ] Integration test: mic ‚Üí worklet ‚Üí PCM16\n\n## ‚õìÔ∏è Blocks\n- Merge PR #1443\n\n## üîç Related\n- PR #1443, PR #1442\n"}
{"id":"4f0d3b3f-4012-431f-96f3-9cde2cc7f06a","title":"launch replayapi on 8083 test replay and export nd md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.515Z","uuid":"4f0d3b3f-4012-431f-96f3-9cde2cc7f06a","created_at":"2025-09-15T02:02:58.515Z","path":"docs/agile/tasks/launch_replayapi_on_8083_test_replay_and_export_nd.md","content":"## üõ†Ô∏è Task: Launch `ReplayAPI` on `:8083`; test `/replay` and `/export?ndjson=1`\n\nGenerated from [../unique/2025.08.08.19.08.25.md](../unique/2025.08.08.19.08.25.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"4fb7f392-4473-4d58-a8c0-ec5fba5d6474","title":"gather open questions about system direction md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.514Z","uuid":"4fb7f392-4473-4d58-a8c0-ec5fba5d6474","created_at":"2025-09-15T02:02:58.514Z","path":"docs/agile/tasks/gather_open_questions_about_system_direction.md","content":"## üõ†Ô∏è Task: Gather open questions about system direction\n\nThis task collects ambiguous or unresolved points about Promethean's goals and architecture that require human clarification.\n\n---\n\n## üéØ Goals\n- Compile a list of unclear areas in the project's roadmap or design\n- Prepare targeted questions for an alignment meeting\n\n---\n\n## üì¶ Requirements\n- [ ] Create `docs/notes/open-questions.md` with the collected items\n\n---\n\n## üìã Subtasks\n- [ ] Review existing docs and code for points that need clarification\n- [ ] Draft the list of questions\n\n---\n\n## üîó Related Epics\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\n- Schedule alignment meeting with stakeholders\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n#IceBox\n#ice-box\n\n"}
{"id":"51e0f7e8-87ab-4b8d-b20c-b0dfc34d891d","title":"add lag checks to ci smoke ensure small lag after md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.504Z","uuid":"51e0f7e8-87ab-4b8d-b20c-b0dfc34d891d","created_at":"2025-09-15T02:02:58.504Z","path":"docs/agile/tasks/add_lag_checks_to_ci_smoke_ensure_small_lag_after.md","content":"## üõ†Ô∏è Task: Add `/lag` checks to CI smoke (ensure small lag after publishing bursts)\n\nGenerated from [../unique/2025.08.08.19.08.49.md](../unique/2025.08.08.19.08.49.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"51fc390b-40d7-4dd4-a435-cdd86298d1b8","title":"allow configuration of hyperparameters through discord context size spectrogram resolution interuption threshold md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.507Z","uuid":"51fc390b-40d7-4dd4-a435-cdd86298d1b8","created_at":"2025-09-15T02:02:58.507Z","path":"docs/agile/tasks/allow_configuration_of_hyperparameters_through_discord_context_size_spectrogram_resolution_interuption_threshold.md","content":"## üõ†Ô∏è Description\n\nExpose Discord commands that let users tune audio processing hyperparameters‚Äîcontext\nsize, spectrogram resolution, and interruption thresholds‚Äîwithout redeploying the\nservice.\n\n---\n\n## üéØ Goals\n\n- Allow real‚Äëtime configuration of audio processing parameters via Discord.\n- Improve interruption handling by using audio‚Äëbased thresholds that pause output\n  when user speech exceeds configurable energy levels.\n- Detect interruptions within **200‚ÄØms** and correctly classify user speech with\n  **‚â•95‚ÄØ% accuracy** against background noise.\n- Allow authorized Discord users to configure model and audio pipeline hyperparameters on the fly.\n- Persist chosen values so they survive service restarts.\n- Provide clear feedback and validation when values are queried or updated.\n\n---\n\n## üì¶ Requirements\n\n- [ ] Provide `/config audio` commands for `context_size`, `spectrogram_res`, and\n      `interruption_threshold` values.\n- [ ] Implement adjustable voice‚Äëactivity detection that pauses TTS once the\n      energy threshold is crossed.\n- [ ] Log every interruption event with timestamps and configured parameters.\n- [ ] Meet **‚â§200‚ÄØms** interruption latency and **‚â•95‚ÄØ%** detection accuracy in\n      controlled tests.\n| Hyperparameter           | Allowed Range                        | Validation Rules |\n|-------------------------|--------------------------------------|------------------|\n| `context_size`          | `256`‚Äì`32768` tokens                 | Integer within range; reject unsupported values for active model |\n| `spectrogram_resolution`| `64`‚Äì`4096` bins per dimension, even | Input as `WIDTHxHEIGHT`; both integers, multiples of `2`, and in range |\n| `interruption_threshold`| `0.1`‚Äì`5.0` seconds                  | Float; clamp to range; round to two decimals |\n\n### Discord Interface\n\n- Slash command: `/config get <param>` to view current value.\n- Slash command: `/config set <param> <value>` to update a value.\n- Autocomplete or choices should limit input to valid ranges where possible.\n\n---\n\n## üîê Security & Privacy\n\n- Restrict `/config` commands to Discord roles flagged as administrators.\n- Log every change with user ID and timestamp for auditing.\n- Reject values containing code, markup, or unexpectedly long strings to prevent injection or abuse.\n- Avoid exposing model secrets or internal topology when responding to unauthenticated queries.\n\n---\n\n## ‚úÖ Success Criteria\n\n- [ ] Valid commands update in-memory settings and persist to the service's configuration store.\n- [ ] Invalid ranges produce informative error messages without modifying existing values.\n- [ ] A `get` command reflects the latest stored value.\n- [ ] Security logging shows who changed what and when.\n\n---\n\n## üìã Subtasks\n\n- [ ] Add Discord command handlers exposing `context_size`, `spectrogram_res`, and\n      `interruption_threshold` options.\n- [ ] Integrate a VAD module using the configurable energy threshold.\n- [ ] Write tests measuring latency and accuracy of interruption detection.\n- [ ] Deploy changes to staging and collect metric samples.\n- [ ] Document tuning guidelines for operators.\n- [ ] Implement `/config get` and `/config set` Discord commands.\n- [ ] Add range validation utilities for each hyperparameter.\n- [ ] Persist updates to database or environment file.\n- [ ] Gate command usage behind role/permission checks and add audit logging.\n- [ ] Add unit tests for validation logic and command handlers.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#IceBox\n\n"}
{"id":"52028599-a34a-40ff-97a0-d8f47970e4b0","title":"flatten services","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"52028599-a34a-40ff-97a0-d8f47970e4b0","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/flatten-services.md","content":"## üõ†Ô∏è Description\n\nReorganize the repository so service directories are flattened, reducing unnecessary nesting and clarifying ownership.\n\n---\n\n## üéØ Goals\n\n- Simplify service discovery and navigation\n- Align service layout with package and deployment structure\n\n---\n\n## üì¶ Requirements\n\n- [ ] Proposed flat structure documented\n- [ ] Imports and build scripts updated for new paths\n- [ ] Migration plan for moving existing code\n\n---\n\n## üìã Subtasks\n\n- [ ] Audit current service directory tree\n- [ ] Draft new flat layout with maintainers\n- [ ] Move or alias services accordingly\n- [ ] Update tooling references and docs\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#framework-core #Ready\n\n\n"}
{"id":"529b7471-6d7c-4135-9de3-8e22970692f0","title":"ecs migration path docs","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"529b7471-6d7c-4135-9de3-8e22970692f0","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/ecs_migration_path_docs.md","content":"ECS Migration Path Documentation\n\nGoal: Document how to migrate each service to ECS-backed persistence incrementally.\n\nContent:\n- Service inventory (cephalon, heartbeat, stt/tts, file-watcher, bridge).\n- For each: target components, mapping to Mongo, migration phases.\n- Feature flags and rollback plan.\n\nExit Criteria:\n- Markdown doc published under `docs/ecs/migration.md` with cross-links from service READMEs.\n\n#incoming #ecs #migration #docs\n\n"}
{"id":"5334798c-3a31-4edc-822b-e761c0354509","title":"auth key rotation and bootstrap","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"5334798c-3a31-4edc-822b-e761c0354509","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/auth_key_rotation_and_bootstrap.md","content":"Auth: Key rotation and bootstrap\n\nGoal: Establish a secure bootstrap and rotation procedure for the auth service.\n\nScope:\n- Bootstrap admin secret (env or file) to create initial admin client.\n- Generate initial RSA keypair; store private securely; publish JWKS; set `kid`.\n- Rotation CLI/endpoint to create new key, mark old as retiring; overlap window for dual-verify.\n- Document ops runbook and rotation cadence.\n\nExit Criteria:\n- Rotation works without downtime; old tokens valid until expiry; new tokens issued with new kid.\n\n#incoming #auth #security #rotation\n\n\n"}
{"id":"54927d5e-d90f-4575-be6a-307a30c2578c","title":"thinking model integration md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.523Z","uuid":"54927d5e-d90f-4575-be6a-307a30c2578c","created_at":"2025-09-15T02:02:58.523Z","path":"docs/agile/tasks/thinking_model_integration.md","content":"## üõ†Ô∏è Description\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- What are we trying to accomplish?\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## ‚úÖ Definition of Done\n\n- [ ] Integration points with existing agent pipeline are documented\n- [ ] API surface for the thinking model is defined and reviewed\n- [ ] Prototype demonstrates model hook executing within a test agent\n\n## üßÆ Story Points\n\n8\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- Awaiting selection of underlying thinking model and alignment with architecture\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## Dependencies\n\n- Core reasoning modules\n- Training data for thinking model\n\n---\n\n## Rough Scope\n\n- Define interfaces between existing agents and thinking model\n- Prototype integration path for inference and feedback\n- Evaluate performance impacts\n\n---\n\n## Estimate\n\n- Story points: 5\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#Breakdown\n\n"}
{"id":"56da5f84-d732-4455-b0a6-97b36fbae026","title":"Cephalon Commands ‚Üí Actions Refactor ‚Äî Task Refinement","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.520Z","uuid":"56da5f84-d732-4455-b0a6-97b36fbae026","created_at":"2025-09-15T02:02:58.520Z","path":"docs/agile/tasks/seperate-discord-commands-from-the-actions-they-perform.md","content":"# Cephalon Commands ‚Üí Actions Refactor ‚Äî Task Refinement\n\n> Split Discord-facing **commands** from reusable **actions** (DI via `scope`).\n\n---\n\n## üéØ Objectives\n\n* The `Bot` class becomes a thin **router** for Discord commands only.\n* Each **action** lives in its own file under `./src/actions`. Most actions are **Discord-agnostic**, but **Discord‚Äëcoupled actions** are allowed where it makes sense (e.g., voice control).\n* Each **command** lives in its own file under `./src/commands` and is **Discord-specific**.\n* Commands **extract/build** the `scope` for actions and map interaction params ‚Üí action inputs.\n* Actions are callable from **other entry points** (broker handlers, schedulers, CLI).\n\n---\n\n## üì¶ Requirements (Detailed)\n\n* **Project Structure**\n\n  * `./services/ts/cephalon/src/actions/*` ‚Äî one action per file, default export `run(scope, input)`.\n  * Each action defines **its own local scope** in a colocated file, e.g. `actions/foo.scope.ts` or `actions/foo/scope.ts`.\n  * `./services/ts/cephalon/src/commands/*` ‚Äî one command per file, default export `execute(interaction)`.\n  * `./services/ts/cephalon/src/factories/*` ‚Äî optional reusable dependency factories (e.g., `logger`, `mongo`, `broker`, `discordCtx`, `voice`, `testing`).\n  * Prefer **adapters** for platform APIs. Discord‚Äëcoupled actions may depend on `discord.js` **via a small adapter** or directly if necessary for clarity.\n  * Imports follow repo rules: use **@shared/ts/dist/...** for shared libs (no new aliases).\n\n* **Action Design**\n\n  * Pure async functions with signature (per-action scope type):\n\n    ```ts\n    // actions/ping.ts\n    import type { PingScope } from \"./ping.scope\";\n\n    export type PingInput = { userId: string };\n    export type PingOutput = { message: string };\n\n    export default async function run(scope: PingScope, input: PingInput): Promise<PingOutput> {\n      // ... business logic, using only PingScope deps\n      return { message: \"pong\" };\n    }\n    ```\n  * No global singletons; everything flows through `scope`.\n  * Errors are **typed** (domain errors) and surfaced to caller; no direct user messaging.I = unknown, O = unknown> = (scope: ActionScope, input: I) => Promise<O>;\n    export default async function run(scope: ActionScope, input: I): Promise<O> { /\\* ... \\*/ }\n\n  * No global singletons; everything flows through `scope`.\n  * Errors are **typed** (domain errors) and surfaced to caller; no direct user messaging.\n\n* **Scope Object**\n\n  * Defined per-action next to the action file (e.g., `./src/actions/ping.scope.ts` or `./src/actions/ping/scope.ts`); no global `ActionScope`:\n\n    ```ts\n    // Example: actions/ping.ts scope\n    export type PingScope = {\n      logger: Logger;\n      policy: PolicyChecker;\n      time: () => Date;                  // inject for testability\n      // Optional deps for THIS action only\n      tts?: VoiceSynth; stt?: SttClient;\n      // Add only what the action truly needs (e.g., db, broker) ‚Äî keep minimal\n    };\n    ```\n  * Commands compose the **action-local scope** using a small builder exported next to the action (e.g., `buildPingScope(ctx)`), which may internally use helpers in `src/factories/*`.\n  * No monolithic `makeBaseScope` or global `ActionScope` type; keep scopes **minimal and action-specific**.\n\n* **Command Responsibilities**\n\n  * Parse/validate interaction options ‚Üí `input` for action.\n  * Build action-specific scope via its `build*Scope(interactionOrCtx)` (colocated next to the action), using `src/factories/*` as needed.\n  * Handle Discord reply lifecycle: defer, follow-up, ephemeral.\n  * Map action outcomes/errors ‚Üí user-visible messages.\n  * No business logic; no DB calls directly.\n\n* **Permissions & Policy (Circuit 2)**\n\n  * Actions call `scope.policy.assertAllowed(subject, action, resource)` early.\n  * Commands **never** bypass policy.\n\n* **Error Handling**\n\n  * Standard error types: `UserError`, `NotAllowedError`, `RetryableError`, `SystemError`.\n  * Commands render friendly text for `UserError` & `NotAllowedError`; log others.\n\n* **Testing**\n\n  * Unit tests for **actions** with action-local scope mocks (e.g., `actions/foo.scope.mock.ts`) ‚Äî no Discord.\n  * Contract tests for **commands** using `discord.js` mocks verifying defers, replies.\n  * One integration test proving an **action is invokable via non-Discord path** (e.g., broker handler or CLI) using `build*Scope()` without Discord context.\n\n* **Docs & DX**\n\n  * Short README: how to add a command, add an action, define its local scope, and compose with factories.\n  * Keep factories small and composable under `src/factories/*`; avoid global containers.\n\n---\n\n## ‚úÖ Acceptance Criteria\n\n* [ ] `Bot` class contains only command registration/routing; no business logic.\n* [ ] All existing commands from `bot.ts` moved to `./src/commands/*` with matching names.\n* [ ] Each command calls an action in `./src/actions/*`.\n* [ ] Each action has a colocated `*.scope.ts` defining its minimal scope + builder.\n* [ ] Discord‚Äëcoupled actions (if any) isolate behavior behind a small adapter and expose a **broker‚Äëfriendly** scope builder; tests include adapter mocks.\n* [ ] At least one action exercised from a **non-Discord** context (broker/CLI) in tests.\n* [ ] Lint, typecheck, and tests green in CI; PM2 processes unaffected.\n* [ ] Docs updated: how to add a command, how to add an action, how to call from elsewhere.\n\n---\n\n## üß± Runtime State & Store (Redux‚Äëlite, no deps)\n\n**Why**: The `Bot` currently acts like a process‚Äëwide variable bucket. Introduce a tiny, framework‚Äëfree **store + effects** so the Bot only routes Discord events ‚Üí `dispatch()`, and **effects** call our domain **actions/**.\n\n**Principles**\n\n* Library‚Äëfree (no Redux pkg): \\~40 lines of `createStore` + `subscribe`, `dispatch`, `getState`.\n* **Events** (not to be confused with `actions/`): immutable payloads describing *what happened / intent*.\n* **Reducer**: pure `(state, event) => state'` for ephemeral runtime state (voice status, shard health, throttles, in‚Äëflight tasks).\n* **Effects**: async listeners triggered by specific events; they call `actions/*` with an action‚Äëlocal scope, may `dispatch` follow‚Äëup events.\n* **Broker bridge**: middleware to forward selected events to the message bus and to dispatch incoming broker messages.\n* **Selectors**: small pure helpers to read derived data for scope builders.\n\n**State is ephemeral** (per process): never the source of truth for DB; mirrors live runtime only.\n\n---\n\n## üß≠ Non‚ÄëGoals\n\n* Building a full IoC container framework; keep DI **manual & simple**.\n* Changing business logic or command behavior; only **moving/separating** concerns.\n* Introducing new path aliases beyond **@shared/ts/dist**.\n\n---\n\n## üß± Constraints\n\n* Monorepo conventions (#shared folder). Do **not** introduce new shared packages or aliases.\n* Keep action signatures stable and minimal; prefer adding to `scope` over passing many params.\n\n---\n\n## ü™ú Subtasks (Implementation Plan)\n\n1. **Inventory & Plan**\n\n   * [ ] List commands currently in `bot.ts` (names, options, permissions, long‚Äërunning?).\n   * [ ] Group by domain; propose action filenames.\n2. **Scaffold**\n\n   * [ ] Add `src/actions/`, `src/commands/`, and `src/factories/` directories.\n   * [ ] Add minimal helpers in `src/factories/` (e.g., `logger.ts`, `mongo.ts`, `broker.ts`, `discord.ts`, `voice.ts`, `testing.ts`).\n   * [ ] Add `src/store/` with `createStore.ts`, `events.ts`, `reducer.ts`, `effects/`.\n   * [ ] Add adapter for `discord.js` voice under `src/factories/voice.ts`.\n3. **Move Bot to Router**\n\n   * [ ] `Bot` wires Discord ‚Üí `dispatch()` only; no business logic.\n   * [ ] Register effects on store startup.\n4. **Tracer Bullet**\n\n   * [ ] Implement `ping` (agnostic) + `leave-voice` (Discord‚Äëcoupled via adapter).\n   * [ ] For each, add: event(s) ‚Üí effect(s) ‚Üí action call ‚Üí follow‚Äëup event.\n5. **Port Remaining Commands**\n\n   * [ ] Create events for each command; route input parsing in commands; business in actions.\n   * [ ] Ensure long-running actions use `deferReply()` & follow-ups.\n6. **Broker Bridge**\n\n   * [ ] Middleware to forward certain events to broker; subscribe to broker topics and `dispatch` incoming messages.\n7. **Testing**\n\n   * [ ] Unit tests: reducer (pure), selectors (pure), effects (mock factories + actions).\n   * [ ] Command tests with Discord mocks verifying defers, replies, and `dispatch` calls.\n   * [ ] Integration test: broker ‚Üí store ‚Üí effect ‚Üí action path.\n8. **Docs & Scripts**\n\n   * [ ] Update README for store/effects patterns and naming (Events vs Actions).\n9. **Cleanup**\n\n   * [ ] Delete business logic from `bot.ts`; keep only routing & `dispatch`.\n\n---\n\n## üóÇÔ∏è Proposed File Layout\n\n```\nservices/ts/cephalon/\n  src/\n    actions/\n      ping.ts\n      ping.scope.ts\n      leave-voice.ts\n      leave-voice.scope.ts\n      ...\n    commands/\n      ping.ts\n      leave-voice.ts\n      ...\n    factories/\n      logger.ts\n      mongo.ts\n      broker.ts\n      chroma.ts\n      discord.ts\n      voice.ts\n      telemetry.ts\n      tts.ts\n      stt.ts\n      testing.ts\n    store/\n      createStore.ts        // tiny framework-free store\n      events.ts             // union of event types\n      reducer.ts            // pure state updates\n      effects/\n        voice.ts            // reacts to leave/join events ‚Üí calls actions\n        health.ts           // shard/heartbeat events ‚Üí actions\n        ...\n    bot/\n      index.ts              // registers commands, routes ‚Üí store.dispatch\n    broker/\n      handlers.ts           // non-Discord invocations ‚Üí actions or dispatch\n      bridge.ts             // subscribes broker ‚Üí dispatch; forwards events ‚Üí broker\n    cli/\n      cephalon-cli.ts\n  test/\n    actions/\n    commands/\n    store/\n    integration/\n```\n\n---\n\n## üìê Reference Sketches\n\n### Mermaid ‚Äî High-Level Flow\n\n```mermaid\nflowchart LR\n  A[Discord Interaction] --> B(Bot Router)\n  B -->|dispatch| S[Store]\n  S --> R[Reducer]\n  S --> E(Effects)\n  E -->|call| ACT[Action (domain fn)]\n  ACT -->|result| E\n  E -->|dispatch follow-up| S\n  E -->|reply/DM via command wrapper| B\n\n  subgraph Non-Discord\n    X[Broker/Event/CLI] -->|dispatch or call| S\n    E -->|broker bridge| X\n  end\n```\n\n### Mermaid ‚Äî Module Boundaries\n\n```mermaid\ngraph TD\n  subgraph Commands (discord.js only)\n    CMD1[commands/*]\n  end\n  subgraph Actions (pure)\n    ACT1[actions/* + *.scope.ts]\n  end\n  subgraph Factories\n    F1[logger]\n    F2[mongo]\n    F3[broker]\n    F4[discord]\n  end\n  CMD1 --> ACT1\n  ACT1 --> F1\n  ACT1 --> F2\n  ACT1 --> F3\n  CMD1 --> F4\n```\n\n---\n\n## ‚ú® Code Skeletons\n\n**Tiny store (framework‚Äëfree)**\n\n```ts\n// src/store/createStore.ts\nexport type Unsubscribe = () => void;\nexport type Listener<E> = (event: E) => void | Promise<void>;\n\nexport function createStore<S, E>(initial: S, reducer: (s: S, e: E) => S) {\n  let state = initial;\n  const listeners = new Set<Listener<E>>();\n\n  function getState() { return state; }\n  function subscribe(l: Listener<E>): Unsubscribe { listeners.add(l); return () => listeners.delete(l); }\n  async function dispatch(e: E) {\n    state = reducer(state, e); // pure update\n    for (const l of listeners) await l(e); // fire effects (can be async)\n  }\n\n  return { getState, subscribe, dispatch };\n}\n```\n\n**Events & reducer**\n\n```ts\n// src/store/events.ts\nexport type Event =\n  | { type: 'VOICE/LEAVE_REQUESTED'; guildId: string; channelId?: string; by: string }\n  | { type: 'VOICE/LEFT'; guildId: string }\n  | { type: 'HEARTBEAT/UP'; pid: number; name: string }\n  | { type: 'HEARTBEAT/DOWN'; pid: number; name: string }\n  // ... add more\n;\n\n// src/store/reducer.ts\nexport type CephalonState = {\n  voice: Record<string, { connected: boolean; channelId?: string }>;\n  heartbeats: Record<string, { up: boolean; pid?: number }>;\n};\n\nexport const initialState: CephalonState = { voice: {}, heartbeats: {} };\n\nexport function reducer(s: CephalonState, e: Event): CephalonState {\n  switch (e.type) {\n    case 'VOICE/LEFT': {\n      const next = { ...s.voice }; next[e.guildId] = { connected: false };\n      return { ...s, voice: next };\n    }\n    case 'HEARTBEAT/UP': {\n      const key = e.name; return { ...s, heartbeats: { ...s.heartbeats, [key]: { up: true, pid: e.pid } } };\n    }\n    case 'HEARTBEAT/DOWN': {\n      const key = e.name; return { ...s, heartbeats: { ...s.heartbeats, [key]: { up: false } } };\n    }\n    default: return s;\n  }\n}\n```\n\n**Effect that calls a Discord‚Äëcoupled action**\n\n```ts\n// src/store/effects/voice.ts\nimport type { Event } from '../events';\nimport { buildLeaveVoiceScope } from '../../actions/leave-voice.scope';\nimport runLeave from '../../actions/leave-voice';\n\nexport function registerVoiceEffects(store: { subscribe: (l: (e: Event) => void) => () => void; dispatch: (e: Event) => Promise<void> }) {\n  store.subscribe(async (e) => {\n    if (e.type === 'VOICE/LEAVE_REQUESTED') {\n      const scope = await buildLeaveVoiceScope();\n      await runLeave(scope, { guildId: e.guildId, channelId: e.channelId });\n      await store.dispatch({ type: 'VOICE/LEFT', guildId: e.guildId });\n    }\n  });\n}\n```\n\n**Bot router uses dispatch, not business logic**\n\n```ts\n// src/bot/index.ts\nimport { SlashCommandBuilder, type ChatInputCommandInteraction } from 'discord.js';\nimport { createStore } from '../store/createStore';\nimport { reducer, initialState } from '../store/reducer';\nimport type { Event } from '../store/events';\nimport { registerVoiceEffects } from '../store/effects/voice';\n\nexport const store = createStore(initialState, reducer);\nregisterVoiceEffects(store);\n\nexport const leaveVoiceData = new SlashCommandBuilder().setName('leave-voice').setDescription('Disconnect');\n\nexport async function executeLeaveVoice(interaction: ChatInputCommandInteraction) {\n  await interaction.deferReply({ ephemeral: true });\n  await store.dispatch({ type: 'VOICE/LEAVE_REQUESTED', guildId: interaction.guildId!, by: interaction.user.id });\n  await interaction.editReply('Leaving‚Ä¶');\n}\n```\n\n**Broker bridge sketch**\n\n```ts\n// src/broker/bridge.ts\nimport type { Event } from '../store/events';\nimport { BrokerClient } from '@shared/ts/dist/broker';\n\nexport function attachBrokerBridge(store: { dispatch: (e: Event) => Promise<void> }) {\n  const broker = new BrokerClient();\n  broker.subscribe('cephalon.events', async (busEvent) => {\n    // validate & map busEvent ‚Üí Event\n    await store.dispatch(busEvent as Event);\n  });\n\n  // Optional: forward certain events to bus\n  // store.subscribe(e => broker.publish('cephalon.events', e))\n}\n```\n\n---\n\n## üß™ Test Matrix\n\n* **Actions**\n\n  * Happy-path returns\n  * Policy denied ‚Üí typed error\n  * Retryable system error bubbles\n* **Commands**\n\n  * Defers reply for long-running actions\n  * Renders user-friendly errors for `UserError`/`NotAllowedError`\n  * Ephemeral vs public replies respected\n* **Non-Discord**\n\n  * Action callable from CLI/broker with mock scope\n\n---\n\n## üöö Migration Plan\n\n1. Land scaffolding & base scope.\n2. Port one simple command (ping) end-to-end.\n3. Port medium-complexity command (needs DB/vector access).\n4. Port remaining commands in small PRs; remove logic from `bot.ts`.\n5. Add Discord voice adapter and broker invocation example for a Discord‚Äëcoupled action.\n\n---\n\n## üß® Risks & Mitigations\n\n* **Hidden Discord coupling in logic** ‚Üí First tracer PR reveals API needs; iterate scope.\n* **Overgrown `scope`** ‚Üí Establish boundaries; prefer adapters per domain; document fields.\n* **Test flakiness** ‚Üí Provide stable mocks per-action (e.g., `actions/foo.scope.mock.ts`) or shared in `src/factories/testing.ts`.\n\n---\n\n\n## üßæ PR Checklist (Definition of Done)\n\n* [ ] New files placed under `src/actions` and `src/commands` with matching names.\n* [ ] Discord‚Äëcoupled actions document their adapter dependency; tests cover adapter usage.\n* [ ] `Bot` only routes.\n* [ ] Unit & integration tests added/updated.\n* [ ] Docs updated; examples compile.\n* [ ] Imports follow **@shared/ts/dist/** convention.\n\n---\n\n## üîó Related Epics / Tags\n\n\\#framework-core #cephalon #dependency-injection #separation-of-concerns #actions #commands #discord\n\n---\n\n## üîç Relevant Links\n\n* \\[\\[kanban.md]]\n\n## Notes\n- Tests or documentation are missing; acceptance criteria not fully met.\n- Story Points: 3\n\n#in-progress\n\n"}
{"id":"57a44b9c-b4b8-4fef-8ba6-cfcd8649ada0","title":"use subscribepartitioned for cpu heavy consumers t md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.523Z","uuid":"57a44b9c-b4b8-4fef-8ba6-cfcd8649ada0","created_at":"2025-09-15T02:02:58.523Z","path":"docs/agile/tasks/use_subscribepartitioned_for_cpu_heavy_consumers_t.md","content":"## üõ†Ô∏è Task: Use **subscribePartitioned** for CPU-heavy consumers; tune `partitions` (power of 2 is fine)\n\nGenerated from [../unique/2025.08.08.20.08.56.md](../unique/2025.08.08.20.08.56.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"58399a92-d702-4529-9fc8-7eb32bd2a567","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"58399a92-d702-4529-9fc8-7eb32bd2a567","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/database-migration-system.md","content":"# Description\n**Status:** blocked\n\nDesign and implement a versioned migration system for persistent data stores so services can evolve schemas safely.\n\n## Requirements/Definition of done\n\n- Migrations can be listed and executed in order against target databases.\n- Support applying and rolling back migrations.\n- Persist current schema version to prevent reapplication.\n- Include an example migration for an existing service.\n\n## Tasks\n\n- [ ] Survey existing data stores and identify migration needs.\n- [ ] Choose a migration framework or create a lightweight module.\n- [ ] Implement CLI commands for `migrate up` and `migrate down`.\n- [ ] Document workflow and provide example migration file.\n\n## Relevant resources\n\nYou might find [Alembic](https://alembic.sqlalchemy.org/) or [migrate](https://github.com/golang-migrate/migrate) useful while working on this task.\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n\n#breakdown\n\n## Blockers\n- No active owner or unclear scope\n\n"}
{"id":"58e246ed-a8e3-4942-bf8d-73532959156e","title":"smart task templater md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.520Z","uuid":"58e246ed-a8e3-4942-bf8d-73532959156e","created_at":"2025-09-15T02:02:58.520Z","path":"docs/agile/tasks/smart_task_templater.md","content":"## üõ†Ô∏è Description\n**Status:** blocked\n\nAutomate the creation of new task files using a command-line script or Obsidian Templater so every Kanban card has a properly formatted markdown stub.\n\n---\n\n## üéØ Goals\n\n- Reduce friction when adding tasks to the Kanban board.\n- Ensure consistent headings and metadata across all task docs.\n- Optionally support command-line generation outside of Obsidian.\n\n---\n\n## üì¶ Requirements\n\n- [ ] Use `docs/agile/templates/task.stub.template.md` as the base.\n- [ ] Accept task title as a required argument.\n- [ ] Optional `--tags` flag appends tag lines to the new file.\n- [ ] Output files to `docs/agile/tasks/` with spaces encoded for board links.\n- [ ] Document usage in `docs/agile/AGENTS.md`.\n\n---\n\n## üìã Subtasks\n\n- [ ] Implement script `scripts/new_task.py` performing template substitution.\n- [ ] Add `make new-task` target for convenience.\n- [ ] Update `docs/board_sync.md` with workflow instructions.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#breakdown\n\n## Blockers\n- No active owner or unclear scope\n\n"}
{"id":"5c27542a-5611-457c-9062-24d8e137c5d8","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"5c27542a-5611-457c-9062-24d8e137c5d8","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/convert-smartgpt-bridge-to-ts.md","content":"# Description\n\nDescribe your task\n\n## Requirements/Definition of done\n\n- If it doesn't have this, we can't accept it\n\n## Tasks \n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Relevent resources\n\nYou might find [this] useful while working on this task\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n#ready\n\n"}
{"id":"5c686b01-e6d0-4008-81bb-6f429d26a64a","title":"convert current services to packages then redefine the services using config files","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"5c686b01-e6d0-4008-81bb-6f429d26a64a","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/convert-current-services-to-packages--then-redefine-the-services-using-config-files.md","content":"## üõ†Ô∏è Description\n\nTransition existing services into reusable packages and instantiate concrete services via configuration files.\n\n---\n\n## üéØ Goals\n\n- Modularize service logic for reuse\n- Enable service composition through declarative configs\n\n---\n\n## üì¶ Requirements\n\n- [x] Extract core logic of each service into language-appropriate package\n- [ ] Provide config-driven bootstrap that loads desired modules\n- [ ] Document migration steps for existing services\n\n---\n\n## üìã Subtasks\n\n- [ ] Audit services and identify shared components\n- [ ] Publish packages for shared logic\n- [ ] Create template config for redefining services\n- [ ] Update build pipeline to consume packages\n\n---\n## üßÆ Story Points\n\n5\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#framework-core #Ready\n#ready\n\n"}
{"id":"5c8d8016-f4b8-421d-b335-73f7646173af","title":"write simple ecosystem declaration library for new md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.524Z","uuid":"5c8d8016-f4b8-421d-b335-73f7646173af","created_at":"2025-09-15T02:02:58.524Z","path":"docs/agile/tasks/write_simple_ecosystem_declaration_library_for_new.md","content":"## üõ†Ô∏è Task: write simple ecosystem declaration library for new agents\n\nCreate a lightweight module that lets each agent declare which\nservices or processes it needs. The goal is a shared \"ecosystem\"\nformat‚Äîlikely a JS or JSON file‚Äîthat PM2 or similar tools can read to\nspawn the correct services for an agent.\n\n---\n\n## üéØ Goals\n\n- Provide a single declaration file per agent (e.g. `duck.ecosystem.js`)\n- Make it easy to add new agents without copying boilerplate\n- Integrate with pm2 so `pm2 start duck.ecosystem.js` just works\n\n---\n\n## üì¶ Requirements\n\n- [ ] Define the declaration schema (services, env vars, command)\n- [ ] Implement a loader that reads the file and spawns processes\n- [ ] Document usage in `docs/` and reference `MIGRATION_PLAN.md`\n- [ ] Clarify how this interacts with the Makefile PM2 targets\n\n---\n\n## üìã Subtasks\n\n- [ ] Draft an example for Duck using existing service commands\n- [ ] Write a small Node script (`ecosystem-loader.js`)\n- [ ] Test launching Cephalon, STT and TTS for one agent\n- [ ] Add instructions in `agents/README.md`\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- Pending PM2 config decisions in [MIGRATION_PLAN.md](../MIGRATION_PLAN.md)\n\n## ‚õìÔ∏è Blocks\n\n- Future multi-agent orchestration features\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n## ‚ùì Questions\n\n- Should PM2 remain the default process manager or is a custom tool planned?\n- How will per-agent environment variables be stored?\n#done\n\n"}
{"id":"5e02a76a-4b66-4dbb-8a93-16fdbee83283","title":"gpt bridge fuzzy lookup should return multiple matches when it is used","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.514Z","uuid":"5e02a76a-4b66-4dbb-8a93-16fdbee83283","created_at":"2025-09-15T02:02:58.514Z","path":"docs/agile/tasks/gpt-bridge-fuzzy-lookup-should-return-multiple-matches-when-it-is-used.md","content":"## üõ†Ô∏è Description\n**Status:** blocked\n\nUpdate the GPT bridge fuzzy lookup so it returns multiple potential matches instead of only the first result.\n\n---\n\n## üéØ Goals\n\n- Provide a ranked list of candidates for fuzzy searches.\n- Allow users to select from the list when ambiguity exists.\n\n---\n\n## üì¶ Requirements\n\n- [ ] Search endpoint returns top N matches with relevance scores.\n- [ ] Client can display and choose among suggestions.\n- [ ] Unit tests cover multi-match scenarios.\n\n---\n\n## üìã Subtasks\n\n- [ ] Audit current fuzzy lookup implementation.\n- [ ] Modify search logic to emit a list of matches.\n- [ ] Expose new response format through bridge API.\n- [ ] Add selection handling to consuming clients.\n- [ ] Write tests for ranking and selection.\n- [ ] Document the updated behaviour.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#accepted\n\n## Blockers\n- No active owner or unclear scope\n\n#breakdown\n\n"}
{"id":"5e290190-24fd-4245-86ee-f7bb66af63be","title":"run bench subscribe ts with mongo bus and record p md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.519Z","uuid":"5e290190-24fd-4245-86ee-f7bb66af63be","created_at":"2025-09-15T02:02:58.519Z","path":"docs/agile/tasks/run_bench_subscribe_ts_with_mongo_bus_and_record_p.md","content":"## üõ†Ô∏è Task: Run `bench/subscribe.ts` with Mongo bus and record p50/p99\n\nGenerated from [../unique/2025.08.08.19.08.25.md](../unique/2025.08.08.19.08.25.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#ice-box\n\n"}
{"id":"5e7d15b7-50dc-4bd8-9ab3-8c7737a8dbe7","title":"Fix failing tests suite","status":"done","priority":"P2","owner":"","labels":["testing","stabilization"],"created":"2025-02-07T00:00:00.000Z","uuid":"5e7d15b7-50dc-4bd8-9ab3-8c7737a8dbe7","created_at":"2025-02-07T00:00:00.000Z","path":"docs/agile/tasks/fix_failing_tests_suite.md","content":"## üß≠ Context\n- **What changed?**: Incoming bug report indicates some unit tests in the Promethean monorepo are currently failing on main.\n- **Where?**: Tests appear in the shared `tests/` workspace.\n- **Why now?**: Keeping regression suite green is required before other teams can merge changes.\n\n## üì• Inputs / Artifacts\n- Local reproduction of failing test output\n- Recent CI report documenting failures (see `docs/reports/codex_cloud/describe/latest/summary.tsv`)\n\n## ‚úÖ Definition of Done\n- [ ] Identify and document root cause for current failing tests\n- [ ] Implement fix with accompanying tests if necessary\n- [ ] Confirm the full affected test command passes locally\n- [ ] Update changelog entry describing the fix\n\n## üó∫ Plan\n1. Run the failing test command to confirm current status and capture error details.\n2. Investigate source modules referenced in the failure to understand regression.\n3. Implement minimal fix restoring expected behaviour, adding tests if coverage lacking.\n4. Re-run tests to ensure suite passes and no new failures introduced.\n5. Document changes and update changelog.\n\n## ‚ö†Ô∏è Risks / Open Questions\n- Failing tests may depend on external services or data fixtures requiring additional setup.\n- Potential mismatch between local environment and CI configuration.\n\n## üîó Related Resources\n- `docs/reports/codex_cloud/describe/latest/summary.tsv`\n- Repository AGENTS guidelines\n"}
{"id":"5f9a1b2c-4d5e-6f7a-8b9c-0d1e2f3a4b5c","title":"duck-audio ‚Äî share clamp + constants across browser/node","status":"todo","priority":"P3","owner":"","labels":["duck-audio","shared","tests"],"created":"2025-10-02T00:00:00.000Z","uuid":"5f9a1b2c-4d5e-6f7a-8b9c-0d1e2f3a4b5c","created_at":"2025-10-02T00:00:00.000Z","path":"docs/agile/tasks/duck-audio-shared-clamp-and-constants.md","content":"#Todo\n\n## üõ†Ô∏è Description\nExtract clamp and decimation ratio constants for reuse; fix test import typos.\n\n## Requirements\n- [ ] Constants exported from `duck-audio`\n- [ ] Tests green\n\n## ‚õìÔ∏è Blocks\n- None\n\n## üîç Related\n- PR #1444, PR #1442\n"}
{"id":"5fb47630-6822-4ecf-a035-625ed0e733bf","title":"Assemble unified @promethean/omni-service host","status":"in_progress","priority":"P2","owner":"","labels":["omni","service"],"created":"2025-09-21T02:47:00Z","uuid":"5fb47630-6822-4ecf-a035-625ed0e733bf","created_at":"2025-09-21T02:47:00Z","path":"docs/agile/tasks/assemble-omni-service-host.md","content":"## üéØ Outcome\nStand up `@promethean/omni-service`, a Fastify application that mounts REST, GraphQL, WebSocket, and MCP adapters on a single server/port with shared auth, RBAC, and lifecycle hooks.\n\n## üì• Inputs\n- [[docs/architecture/omni/omni-service-roadmap.md]]\n- [[docs/agile/tasks/author-omni-protocol-package.md]]\n- [[docs/agile/tasks/extract-omni-core-services.md]]\n\n## ‚úÖ Definition of Done\n- [ ] Service package created with bootstrapping entrypoint and configuration docs.\n- [ ] Auth/RBAC registered once; adapters compose via plugins.\n- [ ] `/rest`, `/graphql`, `/ws`, `/mcp` routes respond to smoke tests.\n- [ ] Nginx example config updated to proxy all interfaces through one domain/port.\n- [ ] Automated smoke tests covering each interface executed in CI.\n\n## ü™ú Steps\n1. Wire Fastify app with shared context builder + lifecycle hooks.\n2. Mount Omni REST/GraphQL/WebSocket adapters; embed MCP HTTP transport.\n3. Provide start scripts + environment configuration examples.\n4. Write integration smoke tests hitting all endpoints.\n5. Document deployment guidance and update changelog.\n\n## üîó Dependencies\n- Omni protocol + core packages must be published.\n"}
{"id":"624a05e5-e5db-4a28-896e-50be1d5ea778","title":"build data structures for eidolon field md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"624a05e5-e5db-4a28-896e-50be1d5ea778","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/build_data_structures_for_eidolon_field.md","content":"## üõ†Ô∏è Task: Build data structures for Eidolon field\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Build data structures for Eidolon field\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#done\n\n"}
{"id":"62feb0e6-5ba9-4808-9465-7da1f2e74cac","title":"add snapshot consumer to warm cache on boot md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.506Z","uuid":"62feb0e6-5ba9-4808-9465-7da1f2e74cac","created_at":"2025-09-15T02:02:58.506Z","path":"docs/agile/tasks/add_snapshot_consumer_to_warm_cache_on_boot.md","content":"## üõ†Ô∏è Task: Add snapshot consumer to warm cache on boot\n\nGenerated from [../unique/2025.08.08.15.08.47.md](../unique/2025.08.08.15.08.47.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"63e44f3d-fc18-4caf-85b7-08936527317e","title":"pr 688 nitpack extract","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.518Z","uuid":"63e44f3d-fc18-4caf-85b7-08936527317e","created_at":"2025-09-15T02:02:58.518Z","path":"docs/agile/tasks/pr-688-nitpack-extract.md","content":"### `docs/agile/tasks/pr-688-nitpack-extract.md`\n\n**Title:** PR #688 ‚Äî extract & dedupe review nits into actionable tasks\n**Status:** #ready\n**Actor:** #agent-mode #codex-task\n**Epic:** #merge-hygiene #docops\n**Priority:** P0\n**Links:** PR #688\n\n#### Intent\n\nPull every review comment on PR #688 via API, **normalize + deduplicate** the tiny nits, cluster them into 1‚Äì2 **repo-wide tasks** (codemods + policy), and write the tasks out as markdown checklists with exact globs and instance counts. No UI scraping, no page loads.\n\n---\n\n### Deliverables\n\n1. **Task A ‚Äî Codemods batch (repo-wide)**\n\n   * A checklist of deduped rules, each with:\n\n     * **Pattern name** (e.g., ‚ÄúAppend `.js` to relative TS imports‚Äù)\n     * **Count** of instances found\n     * **Glob(s)** where they appear\n     * **One-liner fix strategy** (ts-morph/jscodeshift)\n   * Saved to `docs/agile/tasks/pr-688-nitpack-codemods.md`.\n\n2. **Task B ‚Äî Policy/Config follow-ups**\n\n   * Lint/format/pre-commit rule tweaks that prevent reoccurrence (e.g., ESLint import rules, markdownlint exceptions, `.prettierignore` for dumps).\n   * Saved to `docs/agile/tasks/pr-688-nitpack-policy.md`.\n\n3. A short comment on **PR #688** linking both tasks and stating ‚Äúreview nits have been consolidated; follow these two tasks.‚Äù\n\n---\n\n### Agent playbook (how to do it without touching the UI)\n\n1. **Fetch comments (both kinds)**\n\n   * Review comments: `GET /repos/{owner}/{repo}/pulls/688/comments`\n   * Conversation comments: `GET /repos/{owner}/{repo}/issues/688/comments`\n   * If `gh` CLI available:\n\n     * `gh api repos/{owner}/{repo}/pulls/688/comments > .cache/pr688_review.json`\n     * `gh api repos/{owner}/{repo}/issues/688/comments > .cache/pr688_issue.json`\n\n2. **Normalize**\n\n   * Lowercase\n   * Strip code fences/blocks and quoted snippets\n   * Remove line/file prefixes like `packages/x/src/foo.ts:123`\n   * Collapse whitespace\n   * Keep **rule tokens** (e.g., ‚Äúuse .js suffix‚Äù, ‚Äúno TS paths‚Äù, ‚Äúnative esm‚Äù, ‚Äúno default export‚Äù, ‚Äúorder imports‚Äù, ‚Äúprefer immutable‚Äù, ‚Äúavoid mutation‚Äù, ‚Äúlicense gpl-3.0-only‚Äù, ‚Äú@fastify/static‚Äù, ‚Äúdon‚Äôt embed html‚Äù, ‚Äúava test‚Äù, etc.)\n\n3. **Cluster / dedupe**\n\n   * First pass: regex map common nits ‚Üí canonical keys, e.g.:\n\n     * `REL_JS_SUFFIX`: add `.js` to relative TS imports\n     * `NO_TS_PATHS`: remove TS path aliases\n     * `NATIVE_ESM`: `\"type\":\"module\"`, NodeNext, no CJS\n     * `NO_DEFAULT_EXPORT`: switch to named exports\n     * `IMPORT_ORDER`: consistent import grouping\n     * `IMMUTABLE_FP`: no `let`, no mutation in utils\n     * `GPL_ONLY`: package.json `\"license\":\"GPL-3.0-only\"`\n     * `NO_EMBED_HTML`: backend must not embed HTML; serve via `@fastify/static`\n     * `AVA_TESTS`: add/rename AVA tests to pattern\n     * (Add more keys found in comments)\n   * Second pass: fuzzy dedupe (Levenshtein/Jaccard) to merge same-meaning comments.\n\n4. **Locate occurrences in repo**\n\n   * For each key, compute candidate globs and counts using ripgrep:\n\n     * e.g., `REL_JS_SUFFIX` ‚Üí `rg -n \"from ['\\\"][./]\" packages/**/src/**/*.{ts,tsx}`\n     * `NO_TS_PATHS` ‚Üí check `tsconfig.*.json` for `paths`\n     * `NO_EMBED_HTML` ‚Üí `rg -n \"<html|<!doctype|res.send\\\\(\" packages/**/src/**/*.ts`\n   * Record **count** and **top offending files** (cap to first 10 per key).\n\n5. **Emit the two tasks**\n\n   * **Task A**: for each key with `count > 0`, add a checklist item:\n\n     * `- [ ] REL_JS_SUFFIX ‚Äî 87 matches ‚Äî packages/**/src/**/*.{ts,tsx} ‚Äî Fix: codemod to append \".js\" on relative imports`\n   * **Task B**: propose minimal, enforceable config/rule changes:\n\n     * ESLint rules (e.g., `import/extensions`, `no-restricted-imports` for TS paths)\n     * `markdownlint` exceptions for dumps\n     * `.prettierignore` lines to keep huge exports untouched\n     * `ava` test naming/concurrency defaults\n     * Pre-commit hooks ordering\n\n6. **Save files**\n\n   * `docs/agile/tasks/pr-688-nitpack-codemods.md`\n   * `docs/agile/tasks/pr-688-nitpack-policy.md`\n\n7. **Post summary to PR**\n\n   * Comment with a short note and links to the two files.\n\n---\n\n### Acceptance criteria\n\n* Both task files exist with **‚â§ 15** total checklist items, each deduped and concrete.\n* Each checklist item has **counts + globs + one-line fix**.\n* No lockfile or code changes made by this task‚Äîjust the task files.\n* A single comment posted on PR #688 linking the tasks.\n\n---\n\n### Guardrails (Promethean law)\n\n* Flat `packages/` only; **no nested** package folders.\n* **Native ESM** everywhere (NodeNext), **no TS paths**.\n* All **relative imports end with `.js`** in TS source.\n* **GPL-3.0-only** in every package.json.\n* **Never** embed HTML in backend; use `@fastify/static`.\n\n---\n\n#### Optional: Implementation hint the agent can use (no UI)\n\nIf the agent needs to auto-fill counts/globs:\n\n* Use `gh` + `ripgrep` only; no browser access.\n* Build a tiny in-memory classifier: map regex ‚Üí key, else fuzzy-hash to nearest key.\n* Output markdown with sections: `## Codemods`, `## Policy`, each checklist item = one nit family.\n\n"}
{"id":"63fd4d57-aa99-4b77-9de6-fa7acf6e6d83","title":"Broker ‚Üî GPT Bridge Parity Test Plan","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"63fd4d57-aa99-4b77-9de6-fa7acf6e6d83","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/broker-gpt-bridge-parity-plan.md","content":"# Broker ‚Üî GPT Bridge Parity Test Plan\n\n**Owner:** Codex / Agent\n**Status:** needs review\n**Labels:** #broker #gptbridge #testing #parity #ws #http #ci #observability #promethean\n\n---\n\n## üéØ Scope\n\nCompare, for the same inputs, that **outputs, streaming behavior, errors, timings, and metadata** are equivalent between:\n\n* **Broker (WS)** canonical actions/events, and\n* **GPT Bridge (HTTP/WS)** wrapper that proxies + sheds load.\n\nWe normalize outputs so harmless differences (timestamps, ids) don‚Äôt fail the suite.\n\n---\n\n## üîë Preconditions\n\n* Deterministic models & prompts: **temperature=0**, pinned models (#versioning).\n* Fixed chunking rules for streaming (server sends stable deltas).\n* Same tool/plugin catalog visible to both sides.\n* Same env, routing, and auth claims.\n* Dual sinks off for tests unless specifically under ‚Äúoverload/backpressure‚Äù.\n\n---\n\n## üß™ Test Matrix\n\n| Area          | Case                                    | What we assert                                                                    |\n| ------------- | --------------------------------------- | --------------------------------------------------------------------------------- |\n| Chat (sync)   | small, medium, long prompts             | identical `text`, `usage`, `finish_reason`                                        |\n| Chat (stream) | delta chunking                          | same **chunk boundaries**, bytes, order, terminal `complete`                      |\n| Embeddings    | single/batch inputs                     | vector length, numeric closeness (ULP / 1e-6), same metadata                      |\n| Tools         | 1-tool, multi-tool, parallel tool calls | same tool selection, JSON args, tool result merge                                 |\n| Errors        | invalid schema, bad model, 401/403, 5xx | same error class/code/message class                                               |\n| Timeouts      | model, tool, end-to-end                 | fail shape + elapsed within tolerance                                             |\n| Rate limits   | per-user & global                       | bridge sheds, broker returns backpressure; identical HTTP status & WS close codes |\n| Backpressure  | queue caps reached                      | same `overloaded` semantics and retry-after hints                                 |\n| Cancellation  | client abort mid-stream                 | both stop within Œî‚â§200ms; consistent terminal event                               |\n| Metadata      | request\\_id, model, usage               | identical after normalization                                                     |\n| Multi-tenant  | org/user scopes                         | identical policy enforcement & audit trail                                        |\n| Ordering      | concurrent requests                     | per-request ordering preserved                                                    |\n| Large I/O     | 1‚Äì2MB input (allowed)                   | no truncation; same tokenization & usage                                          |\n| Binary edge   | non-text MIME guarded                   | both reject with same code                                                        |\n| Health        | health/metrics parity                   | counters increase consistently after a run                                        |\n\n---\n\n## üß± Normalization Rules\n\nBefore diffing:\n\n* Strip or canonicalize: `request_id`, timestamps, `queue_position`, transient URLs, floating latency fields (we check them separately with tolerances).\n* Round floats in embeddings to **6 decimals** and compare with **absolute tolerance 1e-6**.\n* Normalize whitespace in text: collapse multiple spaces, unify `\\r\\n` ‚Üí `\\n`.\n\n---\n\n## üì¶ Fixtures\n\n* `tests/parity/fixtures/`:\n\n  * `chat/*.json` (prompts, system msgs)\n  * `tools/catalog.json` (stable tool set + deterministic mock backends)\n  * `embeddings/texts.jsonl` (50 cases, multilingual + emojis)\n  * `errors/*.json` (invalid payloads)\n  * `overload/profile.json` (synthetic load profile)\n* Test doubles for tools: deterministic side-effects, seeded RNG, or pure functions.\n\n---\n\n## üß∞ Harness & Helpers (TypeScript / Vitest)\n\n**Paths**\n\n* `shared/ts/src/parity/normalizers.ts` ‚Äî text/usage/embedding normalizers\n* `shared/ts/src/parity/runner.ts` ‚Äî runs both paths and returns comparable outputs\n* `tests/parity/*.spec.ts` ‚Äî suites\n* `tests/parity/utils/matchers.ts` ‚Äî custom expect matchers (embeddings‚âà, streamEqual)\n\n**Runner sketch**\n\n```ts\n// shared/ts/src/parity/runner.ts\nimport { brokerClient } from \"@shared/ts/dist/broker/client\";\nimport { bridgeClient } from \"@shared/ts/dist/bridge/client\";\nimport { normalizeChat, normalizeEmbed, normalizeError, normalizeStream } from \"./normalizers\";\n\nexport async function runChatBoth(payload) {\n  const [b, h] = await Promise.all([\n    brokerClient.chat(payload),\n    bridgeClient.chat(payload),\n  ]);\n  return { broker: normalizeChat(b), http: normalizeChat(h) };\n}\n\nexport async function runChatStreamBoth(payload, onChunk?) {\n  const acc = { broker: [], http: [] };\n  await Promise.all([\n    brokerClient.chatStream(payload, (c)=> { acc.broker.push(c); onChunk?.(\"broker\", c); }),\n    bridgeClient.chatStream(payload, (c)=> { acc.http.push(c);   onChunk?.(\"http\", c); }),\n  ]);\n  return normalizeStream(acc);\n}\n\nexport async function runEmbBoth(inputs) {\n  const [b, h] = await Promise.all([\n    brokerClient.embed(inputs),\n    bridgeClient.embed(inputs),\n  ]);\n  return { broker: normalizeEmbed(b), http: normalizeEmbed(h) };\n}\n\nexport async function runToolBoth(payload) {\n  const [b, h] = await Promise.all([\n    brokerClient.toolCall(payload),\n    bridgeClient.toolCall(payload),\n  ]);\n  return { broker: b, http: h }; // tool result is deterministic, normalize later\n}\n```\n\n**Custom matchers**\n\n```ts\n// tests/parity/utils/matchers.ts\nexport function expectVectorsClose(a: number[], b: number[], tol=1e-6) {\n  expect(a.length).toBe(b.length);\n  for (let i = 0; i < a.length; i++) {\n    expect(Math.abs(a[i]-b[i])).toBeLessThanOrEqual(tol);\n  }\n}\n```\n\n---\n\n## üß™ Representative Test Cases\n\n### 1) Chat ‚Äî Sync\n\n```ts\nimport { runChatBoth } from \"@shared/ts/dist/parity/runner\";\n\nit(\"chat sync: medium prompt parity\", async () => {\n  const payload = { model: \"qwen3:14b-instruct\", messages: [...], temperature: 0 };\n  const { broker, http } = await runChatBoth(payload);\n  expect(broker.text).toEqual(http.text);\n  expect(broker.finish_reason).toEqual(http.finish_reason);\n  expect(broker.usage).toEqual(http.usage);\n});\n```\n\n### 2) Chat ‚Äî Streaming\n\n```ts\nimport { runChatStreamBoth } from \"@shared/ts/dist/parity/runner\";\nimport { expectStreamEqual } from \"./utils/stream\";\n\nit(\"chat stream: chunk-by-chunk parity\", async () => {\n  const payload = { model: \"qwen3:14b-instruct\", stream: true, messages: [...], temperature: 0 };\n  const acc = await runChatStreamBoth(payload);\n  expectStreamEqual(acc.broker, acc.http); // same chunk boundaries & content\n});\n```\n\n### 3) Embeddings\n\n```ts\nimport { runEmbBoth } from \"@shared/ts/dist/parity/runner\";\nimport { expectVectorsClose } from \"./utils/matchers\";\n\nit(\"embeddings: batch parity\", async () => {\n  const inputs = [\"a\", \"b\", \"c\"];\n  const { broker, http } = await runEmbBoth(inputs);\n  expect(broker.vectors.length).toBe(http.vectors.length);\n  broker.vectors.forEach((v,i)=> expectVectorsClose(v, http.vectors[i]));\n  expect(broker.dim).toBe(http.dim);\n});\n```\n\n### 4) Tool Calls\n\n```ts\nimport { runToolBoth } from \"@shared/ts/dist/parity/runner\";\nimport { normalizeToolResult } from \"@shared/ts/dist/parity/normalizers\";\n\nit(\"tools: multi-tool selection parity\", async () => {\n  const payload = { tools: [...], messages: [...], tool_choice: \"auto\", temperature: 0 };\n  const { broker, http } = await runToolBoth(payload);\n  expect(normalizeToolResult(broker)).toEqual(normalizeToolResult(http));\n});\n```\n\n### 5) Error Equivalence\n\n```ts\nimport { brokerClient, bridgeClient } from \"@shared/ts/dist/...\";\n\nit(\"errors: invalid schema yields same code\", async () => {\n  const bad = { action: \"chat.completion\", payload: { model: \"\", messages: [] } };\n  const b = await brokerClient.request(bad).catch(e=>e);\n  const h = await bridgeClient.request(bad).catch(e=>e);\n  expect(normalizeError(b)).toEqual(normalizeError(h));\n});\n```\n\n### 6) Overload / Rate Limit\n\n* Drive synthetic load from `tests/parity/overload.spec.ts` using worker threads.\n* Expect **bridge** to shed (`429`, `retry_after_ms`) while **broker** signals backpressure (`overloaded`, server close code `1013` or app-level event).\n* Verify documented mapping parity (wrapper must translate broker overload ‚Üí HTTP 429 consistently).\n\n---\n\n## ‚è±Ô∏è Timing Tolerances\n\n* Stream cancellation delta ‚â§ **200ms**.\n* Timeout parity within **¬±10%** of configured timeout or **¬±250ms** (whichever larger).\n* Queueing delay parity within **¬±1s** under load (bridge may add shaping).\n\n---\n\n## üìä Metrics & Artifacts\n\nAfter each parity run, capture:\n\n* **Parity score**: passed / total, broken down by feature.\n* **Drift report**: first mismatch diff (redacted), histograms for chunk counts, embedding drift stats.\n* **Performance**: latency distributions (p50/p95) for both paths.\n  Artifacts under: `docs/data/reports/parity/<run-id>/*`.\n\n---\n\n## üßØ CI Wiring\n\n**Workflow steps**\n\n1. Spin local broker & bridge (compose or PM2).\n2. Seed tool registry (deterministic mocks).\n3. Run parity suites with `NODE_ENV=test PARITY=1`.\n4. Upload artifacts (reports + logs).\n5. Gate: **block merge** if any parity test fails.\n\n**Minimal job**\n\n```yaml\n- name: Start stack\n  run: docker compose -f docker-compose.test.yml up -d broker bridge tools\n\n- name: Parity tests\n  run: |\n    pnpm i --frozen-lockfile\n    pnpm test --filter parity\n\n- name: Upload parity artifacts\n  uses: actions/upload-artifact@v4\n  with:\n    name: parity-reports\n    path: docs/data/reports/parity/**\n```\n\n---\n\n## ‚ö†Ô∏è Known Edge Cases & Guards\n\n* **Chunk boundaries**: ensure both sides use the same tokenizer/chunker. If not feasible, assert **content equality** with flexible chunk counts but require terminal framing parity.\n* **Usage accounting**: minor differences from transport wrappers ‚Üí normalize by rounding & asserting within ¬±1‚Äì2 tokens if exact matching is impossible. Prefer exact.\n* **Request IDs**: compare after stripping UUIDs.\n* **Clock**: use a **fake clock** for timeout tests to avoid flake.\n\n---\n\n## ‚úÖ Acceptance Criteria (for this plan)\n\n* [ ] Suites exist for **chat (sync/stream)**, **embeddings**, **tools**, **errors**, **timeouts**, **overload**, **cancellation**, **metadata**.\n* [ ] Normalizers & helpers implemented and reused by all cases.\n* [ ] CI enforces parity gate on every PR.\n* [ ] Reports are generated and stored per run with pass/fail summary.\n* [ ] Documented **allowed deltas** (timing/rounding) are the only tolerated differences.\n\n---\n\n## üìÇ Proposed Files/Paths\n\n* `shared/ts/src/parity/normalizers.ts`\n* `shared/ts/src/parity/runner.ts`\n* `tests/parity/chat.sync.spec.ts`\n* `tests/parity/chat.stream.spec.ts`\n* `tests/parity/embeddings.spec.ts`\n* `tests/parity/tools.spec.ts`\n* `tests/parity/errors.spec.ts`\n* `tests/parity/overload.spec.ts`\n* `tests/parity/cancellation.spec.ts`\n* `tests/parity/metadata.spec.ts`\n* `docs/data/reports/parity/*`\n* `.github/workflows/parity.yml` (or folded into existing)\n\n---\n\n## Commit Template\n\n```\ntest(parity): add broker‚Üîbridge parity suites + normalizers + CI gate\n\n- Chat (sync/stream), embeddings, tools, errors, timeouts, overload, cancellation\n- Shared parity runner & normalizers\n- CI workflow and artifact reports\n```\n\n\\#tags #broker #gptbridge #parity #ws #http #testing #ci #observability #promethean\n\n#in-review\n\n"}
{"id":"654e88a0-ce43-46b0-a67b-3c18270be8eb","title":"setup a second agent md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.524Z","uuid":"654e88a0-ce43-46b0-a67b-3c18270be8eb","created_at":"2025-09-15T02:02:58.524Z","path":"docs/agile/tasks/setup_a_second_agent.md","content":"## üõ†Ô∏è Description\n\nGet a second agent running so we can have the two  talk to each other, and  get an opertunity to abstract the creation of agents like mentioned  in [Update makefile to have commands specific for agents](Update%20makefile%20to%20have%20commands%20specific%20for%20agents.md)\n[write simple ecosystem declaration library for new agents](write%20simple%20ecosystem%20declaration%20library%20for%20new%20agents.md)\n\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Add Ollama formally to pipeline\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#IceBox\n\n"}
{"id":"66f98367-0bb2-43a8-a11d-187a0225b0d3","title":"suggest metaprogramming updates codex task md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.521Z","uuid":"66f98367-0bb2-43a8-a11d-187a0225b0d3","created_at":"2025-09-15T02:02:58.521Z","path":"docs/agile/tasks/suggest_metaprogramming_updates_codex_task.md","content":"## üõ†Ô∏è Task: Suggest metaprogramming updates #codex-task\n\nGenerated from [../unique/2025.07.28.18.07.20.md](../unique/2025.07.28.18.07.20.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"67368c6a-0102-4eb4-bcfa-7287dd747f9c","title":"implement timetravel processat processid t in a sm md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.515Z","uuid":"67368c6a-0102-4eb4-bcfa-7287dd747f9c","created_at":"2025-09-15T02:02:58.515Z","path":"docs/agile/tasks/implement_timetravel_processat_processid_t_in_a_sm.md","content":"## üõ†Ô∏è Task: Implement `timetravel.processAt(processId, T)` in a small CLI for debugging\n\nGenerated from [../unique/2025.08.08.20.08.83.md](../unique/2025.08.08.20.08.83.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"6840269b-1415-4ae0-90b5-5d520b6a88a8","title":"cephalon event schema updates","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"6840269b-1415-4ae0-90b5-5d520b6a88a8","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/cephalon_event_schema_updates.md","content":"Cephalon: Event schema updates for speech pipeline\n\nGoal: Define/align event topics and payloads for transcript and utterance lifecycle to make downstream processing consistent.\n\nWhy: We publish `agent.transcript.final` and consume `agent.llm.result` but lack standardized schemas. Add versions in `bridge/events/`.\n\nScope:\n- Add/extend specs in `bridge/events/events.md` and `events.json` for:\n  - `agent.transcript.final.v1`\n  - `agent.utterance.started.v1` / `agent.utterance.finished.v1`\n  - `agent.llm.result.v1`\n- Include fields: `turnId`, `ts`, `text`, `channelId`, `userId`, `utteranceId`.\n\nExit Criteria:\n- Schemas documented and referenced by publishers/subscribers.\n\n#incoming #cephalon #events #schemas\n\n"}
{"id":"68f9fe6d-3602-44b2-b3d4-471173e2b4fc","title":"<verb> <thing> <qualifier>","status":"todo","priority":"p3","owner":"","labels":["board:auto","lang:ts"],"created":"2025-09-15T02:02:58.514Z","uuid":"68f9fe6d-3602-44b2-b3d4-471173e2b4fc","created_at":"2025-09-15T02:02:58.514Z","path":"docs/agile/tasks/github-crawler.md","content":"## Context\n- What changed?\n- Where?\n- Why now?\n\n## Inputs / Artifacts\n- (link or path)\n\n## Definition of Done\n- [ ] test X passes\n- [ ] doc Y updated\n- [ ] PR merged: <link>\n\n## Plan\n1. ‚Ä¶\n2. ‚Ä¶\n\n## Relevent resources\n\nYou might find [this] useful while working on this task\n\n## Notes\n- ‚Ä¶\n\n"}
{"id":"695632f6-8bb2-4051-bbbc-42dd6729fe8f","title":"phase out proxy in favor of bridge service","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.517Z","uuid":"695632f6-8bb2-4051-bbbc-42dd6729fe8f","created_at":"2025-09-15T02:02:58.517Z","path":"docs/agile/tasks/phase-out-proxy-in-favor-of-bridge-service.md","content":"## üõ†Ô∏è Description\n\nRetire the existing proxy layer and expose all external APIs through the broker-driven bridge service.\n\n---\n\n## üéØ Goals\n\n- Consolidate public access points into the bridge\n- Ensure bridge speaks OpenAI-compatible API for LLM interactions\n\n---\n\n## üì¶ Requirements\n\n- [ ] Frontends route through bridge for API access\n- [ ] LLM requests forwarded through broker to backend models\n- [ ] STT and TTS endpoints available via bridge\n- [ ] Access controlled via policy rules\n\n---\n\n## üìã Subtasks\n\n- [ ] Audit current proxy usage\n- [ ] Extend bridge with OpenAI-compatible endpoints\n- [ ] Migrate frontend/LLM/STT/TTS clients to bridge\n- [ ] Remove legacy proxy code\n\n---\n## üßÆ Story Points\n\n8\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [API spec](https://err-stealth-16-ai-studio-a1vgg.tailbe888a.ts.net/v1/openapi.json)\n- [[kanban]]\n\n#framework-core #Todo\n\n\n"}
{"id":"69a70883-0a6d-4d2a-a502-9d5cfab5ac6c","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"69a70883-0a6d-4d2a-a502-9d5cfab5ac6c","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/finish-work-on-gptbridge-agent-integration.md","content":"# Description\n\nDescribe your task\n\n## Requirements/Definition of done\n\n- If it doesn't have this, we can't accept it\n\n## Tasks \n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Relevent resources\n\n[ChatGPT - Codex script tool](https://chatgpt.com/share/68a742cc-3270-8004-9caf-544d42d78166)\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n"}
{"id":"6a932fef-3759-44c1-951c-9b0669ae200c","title":"add process txn projector to upsert processes host md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.505Z","uuid":"6a932fef-3759-44c1-951c-9b0669ae200c","created_at":"2025-09-15T02:02:58.505Z","path":"docs/agile/tasks/add_process_txn_projector_to_upsert_processes_host.md","content":"## üõ†Ô∏è Task: Add `process.txn` projector to upsert `processes` + `host_stats` atomically\n\nGenerated from [../unique/2025.08.08.20.08.83.md](../unique/2025.08.08.20.08.83.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"6d4f5bd0-36e6-446c-bf62-db300e276f97","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"6d4f5bd0-36e6-446c-bf62-db300e276f97","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/context-service.md","content":"# Description\n\nDescribe your task\n\n## Requirements/Definition of done\n\n- If it doesn't have this, we can't accept it\n\n## Tasks \n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Relevent resources\n\nYou might find [this] useful while working on this task\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n#ready\n\n"}
{"id":"6e58071f-eeec-4a12-8d36-e5e3a8d29a24","title":"Dev mode","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.511Z","uuid":"6e58071f-eeec-4a12-8d36-e5e3a8d29a24","created_at":"2025-09-15T02:02:58.511Z","path":"docs/agile/tasks/dev mode for each package.md","content":"# Dev mode\n\nEvery package should have a dev mode that watches the files and builds the package when something changes. The tests should be reran after the the project builds. Lint should be ran, etc. Just the whole everything. Everything gets done as soon as it restarts.\n\nIf there is a webserver, frontend, etc, the frontend should restart, the app should restart, after the project is built.\nIf it builds successfully. Do not restart the package instance if it does not build successfuly\n\nNot all packages are applications, many are libraries, modules, utilities, some are just CLI commands.\n\n\n"}
{"id":"6fb5b136-e4fe-4c01-8df4-3ddfd7a160e2","title":"scripts update ci and refs","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.519Z","uuid":"6fb5b136-e4fe-4c01-8df4-3ddfd7a160e2","created_at":"2025-09-15T02:02:58.519Z","path":"docs/agile/tasks/scripts_update_ci_and_refs.md","content":"Scripts: Update CI and references post-reorg\n\nGoal: Update CI workflows and docs to reflect new script locations and Make targets.\n\nScope:\n- Search and replace old paths in docs and scripts invoking other scripts.\n- Update any GitHub Actions to call Make targets instead of raw paths where possible.\n- Verify path assumptions in tests (e.g., fixtures referencing script locations).\n\nExit Criteria:\n- CI green with new structure; no broken references.\n\n#incoming #scripts #ci #organization\n\n\n"}
{"id":"701696e2-8054-4ccd-951f-7a40248b6628","title":"discord image awareness md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.511Z","uuid":"701696e2-8054-4ccd-951f-7a40248b6628","created_at":"2025-09-15T02:02:58.511Z","path":"docs/agile/tasks/discord_image_awareness.md","content":"## üõ†Ô∏è Description\n**Status:** blocked\n\nEnable the system to **ingest and selectively include Discord images** into the context pipeline.\nCurrently, image handling is limited to a single ‚Äúmoving frame‚Äù model, where only the latest image passes through the system. This change will allow **multiple relevant images** to be retained, filtered, and passed selectively based on context needs.\n\nImages should be:\n\n* Captured from Discord messages in real time.\n* Tagged and stored alongside transcripts or other message context.\n* Queryable so that only **relevant** images are sent through for processing (e.g., to Ollama, vision models, or downstream tasks).\n\nThis allows the ‚ÄúDuck‚Äù to have **visual memory** tied to conversational context instead of a rolling overwrite.\n\n---\n\n## üéØ Goals\n\n* Integrate **Discord image ingestion** into the context pipeline.\n* Enable **selective** image passing rather than always overwriting with the latest.\n* Store images with **contextual metadata** (author, timestamp, related transcript segment).\n* Make images **queryable** by relevance for use in generation pipelines.\n* Support expansion to **other image sources** in the future.\n\n---\n\n## üì¶ Requirements\n\n* [ ] Modify Discord listener to capture and store image attachments with metadata.\n* [ ] Store images in retrievable format (local FS, object storage, or DB reference).\n* [ ] Link images to transcript segments for contextual retrieval.\n* [ ] Implement filtering logic to decide which images enter active context.\n* [ ] Support multi-modal prompt building with images + text for Ollama or other models.\n* [ ] Ensure storage limits and cleanup mechanisms are in place.\n\n---\n\n## üìã Subtasks\n\n* [ ] Extend Discord message ingestion to parse attachments.\n* [ ] Save image to storage and store metadata in DB.\n* [ ] Link images to transcript or event records.\n* [ ] Build query API for retrieving relevant images.\n* [ ] Integrate image retrieval into prompt-building step.\n* [ ] Add configurable relevance filters (recentness, author, keyword match).\n\n---\n\n## üîó Related Epics\n\n#framework-core\n#ollama-integration\n\n## Blockers\n- No active owner or unclear scope\n\n#breakdown\n\n## Notes\n- Tests or documentation are missing; acceptance criteria not fully met.\n- Story Points: 5\n\n#ready\n\n"}
{"id":"7057f956-e24b-4402-89a4-ada6eea88c41","title":"add dev harness int test ts to ci integration stag md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.505Z","uuid":"7057f956-e24b-4402-89a4-ada6eea88c41","created_at":"2025-09-15T02:02:58.505Z","path":"docs/agile/tasks/add_dev_harness_int_test_ts_to_ci_integration_stag.md","content":"## üõ†Ô∏è Task: Add `dev.harness.int.test.ts` to CI integration stage\n\nGenerated from [../unique/2025.08.08.20.08.83.md](../unique/2025.08.08.20.08.83.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"709c00c8-6e75-4719-850b-d58ea26ab255","title":"make the system hashtag aware","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.516Z","uuid":"709c00c8-6e75-4719-850b-d58ea26ab255","created_at":"2025-09-15T02:02:58.516Z","path":"docs/agile/tasks/make-the-system-hashtag-aware.md","content":"## üõ†Ô∏è Description\n\nWe want agents to be aware of the available hashtags using the vault graph service\n\n## üì¶ Requirements\n- Query the vault graph service to retrieve available hashtags.\n- Expose a command or API for agents to list and search hashtags.\n- Sync task files and board entries with recognized hashtags.\n\n## ‚úÖ Acceptance Criteria\n- Agents can list existing hashtags via the new command or API.\n- Updating a task with a hashtag is reflected on the Kanban board after sync.\n- Documentation describes how hashtags are discovered and used.\n\n## Tasks\n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Relevent resources\nYou might find [this] useful while working on this task\n\n## Comments\nUseful for agents to engage in append only conversations about this task.\n\n## Story Points\n\n- Estimate: 3\n- Assumptions: A consistent hashtag taxonomy is available.\n- Dependencies: Vault graph service and parsing hooks.\n#ready\n\n"}
{"id":"71a16cd2-8c1a-4285-90a1-d7a85b0d1af0","title":"add unit tests for gui helpers md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.506Z","uuid":"71a16cd2-8c1a-4285-90a1-d7a85b0d1af0","created_at":"2025-09-15T02:02:58.506Z","path":"docs/agile/tasks/add_unit_tests_for_gui_helpers.md","content":"## üõ†Ô∏è Task: Add unit tests for GUI helpers\n\nCover `shared/py/utils/gui.py` with pytest.\n\n---\n\n## üéØ Goals\n- Verify widget creation and layout helpers\n- Improve maintainability of GUI utilities\n\n---\n\n## üì¶ Requirements\n- [ ] Mock Tkinter root objects\n- [ ] Avoid real window rendering\n\n---\n\n## üìã Subtasks\n- [ ] Test `build_root_window`\n- [ ] Test default widget options\n\n---\n\n## üîó Related Epics\n#codex-task #testing\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\nNothing\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n- [Untested modules](../../untested-code.md)\n\n#archive\n\n"}
{"id":"72661a56-1dd1-495d-ba6e-68cffe8425d7","title":"add file system to context management system md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.504Z","uuid":"72661a56-1dd1-495d-ba6e-68cffe8425d7","created_at":"2025-09-15T02:02:58.504Z","path":"docs/agile/tasks/add_file_system_to_context_management_system.md","content":"## üõ†Ô∏è Description\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- What are we trying to accomplish?\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#IceBox\n\n"}
{"id":"727fd6bd-7c63-4488-b29e-c09640d4cc8a","title":"consolidate all kanban helpers to kanban package","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.503Z","uuid":"727fd6bd-7c63-4488-b29e-c09640d4cc8a","created_at":"2025-09-15T02:02:58.503Z","path":"docs/agile/tasks/Consolidate all kanban helpers to kanban package.md","content":"Scripts: Group remaining Kanban utilities and verify\n\nGoal: Ensure all Kanban-related scripts are under `packages/kanban/` and update their docs.\n\nScope:\n- Rewrite `kanban_to_issues.py` as a TypeScript module in `packages/kanban/`.\n- Move all scripts in `scripts/kanban` to `packages/kanban/`.\n\t- Rewrite any Python scripts to TypeScript.\n\t- Avoid/remove direct string manipulation; prefer using a Markdown parser (e.g., `marked`).\n\nExit Criteria:\n- All Kanban scripts consolidated; README accurate; sample commands tested.\n\n#incoming #scripts #kanban #organization\n\n\n"}
{"id":"7495aea8-a866-4fae-b5b3-be0c40e72644","title":"ecs persistence integration cephalon","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"7495aea8-a866-4fae-b5b3-be0c40e72644","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/ecs_persistence_integration_cephalon.md","content":"ECS Persistence Integration: Cephalon\n\nGoal: Wire Mongo-backed ECS adapter into Cephalon‚Äôs world so transcripts, utterances, and agent messages are persisted consistently.\n\nScope:\n- Instantiate `MongoStorageAdapter` in `start-dialog.scope.ts` and pass into `createAgentWorld`.\n- Persist on:\n  - `TranscriptFinal` set\n  - `Utterance` lifecycle (queued ‚Üí playing ‚Üí done)\n  - Agent LLM reply arrival (`agent.llm.result`)\n- Hydrate world on startup from Mongo for continuity.\n\nExit Criteria:\n- Conversation flows produce Mongo upserts for components.\n- On restart, context is reconstructed from persisted components.\n\n#incoming #ecs #cephalon #mongo\n\n"}
{"id":"76f70720-6082-4a02-8c10-a90468d1a85b","title":"register v 1 schema for any evolving topic and wri md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.518Z","uuid":"76f70720-6082-4a02-8c10-a90468d1a85b","created_at":"2025-09-15T02:02:58.518Z","path":"docs/agile/tasks/register_v_1_schema_for_any_evolving_topic_and_wri.md","content":"## üõ†Ô∏è Task: Register **v+1** schema for any evolving topic and write minimal **upcaster**\n\nGenerated from [../unique/2025.08.08.20.08.52.md](../unique/2025.08.08.20.08.52.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"76ff159a-166e-43e1-8d11-14df53f50e6f","title":"research github projects board api md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.519Z","uuid":"76ff159a-166e-43e1-8d11-14df53f50e6f","created_at":"2025-09-15T02:02:58.519Z","path":"docs/agile/tasks/research_github_projects_board_api.md","content":"## üõ†Ô∏è Task: Research GitHub Projects board API\n\nWe need to understand how to interact programmatically with GitHub Projects so our kanban board can sync with it.\n\n---\n\n## üéØ Goals\n- Identify API endpoints for creating and updating project items\n- Note authentication requirements and rate limits\n- Decide if a GitHub Action or local script will perform the sync\n\n---\n\n## üì¶ Requirements\n- [x] Document relevant API endpoints\n- [x] Summarize required auth tokens\n\n---\n\n## üìã Subtasks\n- [x] Read GitHub API documentation\n- [x] Experiment with basic calls via `curl`\n- [x] Outline how board data maps to GitHub structures\n\n---\n\n## üîó Related Epics\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\n- Write board sync script\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n#done\n\n"}
{"id":"77011385-81b7-42aa-9137-cd522a63910e","title":"Use `node:` protocol for builtin imports","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.523Z","uuid":"77011385-81b7-42aa-9137-cd522a63910e","created_at":"2025-09-15T02:02:58.523Z","path":"docs/agile/tasks/use_node_protocol_for_builtin_imports.md","content":"# Use `node:` protocol for builtin imports\n\n## Description\nBiome flagged `useNodejsImportProtocol` violations where Node.js builtins like `path` and `url` are imported without the `node:` prefix in `packages/codex-context`.\n\n## Goals\n- Ensure all Node.js builtin modules use the `node:` import protocol.\n\n## Requirements\n- Locate imports of core Node modules without `node:`.\n- Update imports to include `node:` prefix and adjust tests.\n\n## Subtasks\n- [ ] Fix imports in `packages/codex-context` `ecosystem.config.js`.\n- [ ] Audit other packages for missing `node:` prefixes.\n\n#Todo #codex-task\n\n"}
{"id":"78747905-9183-4fd8-a747-c064f11a861a","title":"write a smoke test client subscribes publish 10 ms md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.524Z","uuid":"78747905-9183-4fd8-a747-c064f11a861a","created_at":"2025-09-15T02:02:58.524Z","path":"docs/agile/tasks/write_a_smoke_test_client_subscribes_publish_10_ms.md","content":"## üõ†Ô∏è Task: Write a smoke test: client subscribes, publish 10 msgs, assert all ACKed\n\nGenerated from [../unique/2025.08.08.15.08.47.md](../unique/2025.08.08.15.08.47.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"79bbbd38-ace9-4833-89be-b6dd50bbc940","title":"migrate portfolio client code to promethean md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.516Z","uuid":"79bbbd38-ace9-4833-89be-b6dd50bbc940","created_at":"2025-09-15T02:02:58.516Z","path":"docs/agile/tasks/migrate_portfolio_client_code_to_promethean.md","content":"## üõ†Ô∏è Task: Migrate portfolio client code to Promethean\n\nMove the legacy portfolio web client from `riatzukiza.github.io` into\nthe Promethean monorepo under `sites/`. This keeps the project‚Äôs demo\npages available and ensures the build pipeline works from inside the\nnew repository.\n\n---\n\n## üéØ Goals\n\n- Preserve the existing portfolio site as part of Promethean\n- Ensure the client code builds and serves from `/sites/`\n- Reference the old repo history for attribution\n\n---\n\n## üì¶ Requirements\n\n- [ ] Access the `riatzukiza.github.io` repository\n- [ ] Link original repo for reference: <https://github.com/riatzukiza/riatzukiza.github.io>\n- [ ] Copy HTML/CSS/JS assets into `sites/portfolio/`\n- [ ] Update asset paths and links for the new layout\n- [ ] Document build steps in `sites/README.md`\n\n---\n\n## üìã Subtasks\n\n- [ ] Clone or download the old repository\n- [ ] Identify which pages and assets to keep\n- [ ] Move selected files under `sites/portfolio/`\n- [ ] Add build or serve command to `package.json` or `Makefile`\n- [ ] Reference [../MIGRATION_PLAN.md](../MIGRATION_PLAN.md.md) for context\n- [ ] Decide whether to import history via submodule or direct copy\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- [Migrating relevant modules from riatzukiza.github.io to -site- and -docs-](Migrating%20relevant%20modules%20from%20riatzukiza.github.io%20to%20-site-%20and%20-docs-.md)\n\n## ‚õìÔ∏è Blocks\n\n- Future site styling or CI tasks\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n## ‚ùì Questions\n\n- Should the portfolio be maintained as a submodule or migrated directly?\n- Do we need a static build step or can GitHub Pages handle raw files?\n  #accepted\n\n"}
{"id":"7ac178ae-b95a-465c-a3e7-603c5a4b64b4","title":"tree diffing tools","status":"todo","priority":"p3","owner":"","labels":[],"created":"2025-09-15T02:02:58.523Z","uuid":"7ac178ae-b95a-465c-a3e7-603c5a4b64b4","created_at":"2025-09-15T02:02:58.523Z","path":"docs/agile/tasks/tree-diffing-tools.md","content":"<hr class=\"__chatgpt_plugin\">\n\n<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n### role::assistant\n\nHere's a suggested revision of your context section:\n\n## Context\n### Changes and Updates\n- **What changed?**: [Describe the key changes that have occurred, e.g., updated requirements, new feature added]\n- **Where?**: [Specify the location or scope where these changes are relevant, e.g., specific project, department]\n- **Why now?**: [Explain why these changes are necessary at this time, e.g., due to deadline, feedback from stakeholders]\n\n## Inputs / Artifacts\n- ([link or path] to relevant documentation, data, or assets)\n\n## Definition of Done\n- [ ] test X passes: [ Briefly describe the testing scenario]\n- [ ] doc Y updated: [Mention the specific documentation or resource that has been updated]\n- [ ] PR merged: ([link to the PR] with a brief summary of changes)\n\n## Plan\n1. ‚Ä¶ [ Outline the high-level steps for completing this task, including any dependencies or resources required]\n2. ‚Ä¶\n\n## Relevant Resources\n\nYou might find [this](link) useful while working on this task.\n\n### Smart Connections Configuration\n```smart-connections\n{\n  \"render_markdown\": true,\n  \"show_full_path\": false,\n  \"exclude_blocks_from_source_connections\": false,\n  \"exclude_frontmatter_blocks\": true,\n  \"expanded_view\": false,\n  \"results_limit\": \"20\",\n  \"exclude_inlinks\": false,\n  \"exclude_outlinks\": false\n}\n```\n\n### Smart ChatGPT Configuration\n```smart-chatgpt\n```\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\n"}
{"id":"7c508df7-4463-461a-8549-0ac8dd256192","title":"create base readme md templates for each service md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"7c508df7-4463-461a-8549-0ac8dd256192","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/create_base_readme_md_templates_for_each_service.md","content":"## üß† Description\n\nEach service in `services/` should include a minimal `README.md` explaining its purpose, how to start it, and any dependencies. This task enforces that standard.\n\nMost directories currently lack a README, so we need to create them using a common template.\n\n## üéØ Goals / Outcomes\n- Ensure every service directory has a `README.md`\n- Document launch commands and environment variables\n- Provide links to related `AGENT.md` files\n\n## üß© Related Concepts\n- [process_board_flow](../process_board_flow.md)\n- [[docs/agile/agents|agents.md]]\n- [service directory conventions](../service%20directory%20conventions.md)\n- #doc-this #framework-core #ritual\n\n## üõ† Requirements\n- Add `README.md` if missing\n- Confirm each has:\n  - Overview\n  - Launch instructions\n  - Related AGENT.md or spec file link\n\n## ‚úÖ Tasks\n- [ ] Create `services/cephalon/README.md`\n- [ ] Create `services/discord-embedder/README.md`\n- [ ] Create `services/discord-indexer/README.md`\n- [ ] Create `services/stt/README.md`\n- [ ] Create `services/tts/README.md`\n\n## üîó Links\n- [[docs/agile/agents|agents.md]]\n\n\n\n#archive\n\n"}
{"id":"7ce1467e-ca43-407e-bcc2-e7026ee9daf7","title":"add vault instructions to main readme md md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.506Z","uuid":"7ce1467e-ca43-407e-bcc2-e7026ee9daf7","created_at":"2025-09-15T02:02:58.506Z","path":"docs/agile/tasks/add_vault_instructions_to_main_readme.md","content":"## üõ†Ô∏è Task: Add vault instructions to main README.md\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Add vault instructions to main README.md\".\n\n---\n\n## üì¶ Requirements\n\n- [x] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [x] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n## Comments\n\nVault usage instructions were added to `readme.md` alongside a reference to\n`vault-config/README.md`.\n#rejected\n\n"}
{"id":"7da8bb11-51ea-4103-adfb-5c3dfb149472","title":"design circular buffers for inputs with layered states of persistance in memory on disk cold storage so md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.511Z","uuid":"7da8bb11-51ea-4103-adfb-5c3dfb149472","created_at":"2025-09-15T02:02:58.511Z","path":"docs/agile/tasks/design_circular_buffers_for_inputs_with_layered_states_of_persistance_in_memory_on_disk_cold_storage_so.md","content":"## üõ†Ô∏è Description\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- What are we trying to accomplish?\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## ‚úÖ Definition of Done\n\n- [ ] Architecture document describes in‚Äëmemory, on‚Äëdisk, and cold‚Äëstorage tiers\n- [ ] Prototype circular buffer persists and rolls data across all tiers\n- [ ] Tests demonstrate data rollover between tiers without loss\n\n## üßÆ Story Points\n\n8\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- Pending design on memory hierarchy and storage strategy\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## Dependencies\n\n- DualStore persistence layer\n- Disk and cold-storage APIs\n\n---\n\n## Rough Scope\n\n- Design buffer tiers for memory, disk, and cold storage\n- Specify read/write and eviction strategies\n- Document data flow across layers\n\n---\n\n## Estimate\n\n- Story points: 8\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#Breakdown\n\n"}
{"id":"7ecab1fa-247e-4968-9352-5fd6c97f8eb2","title":"wrap writers with withdualwrite md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.523Z","uuid":"7ecab1fa-247e-4968-9352-5fd6c97f8eb2","created_at":"2025-09-15T02:02:58.523Z","path":"docs/agile/tasks/wrap_writers_with_withdualwrite.md","content":"## üõ†Ô∏è Task: Wrap writers with **withDualWrite**\n\nGenerated from [../unique/2025.08.08.20.08.52.md](../unique/2025.08.08.20.08.52.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"7f2a8d19-4f72-4c39-9b23-08fd4e33c4f0","title":"Add rate limiting to SmartGPT Bridge file routes","status":"done","priority":"P1","owner":"","labels":["security","bug"],"created":"2025-01-06T12:00:00.000Z","uuid":"7f2a8d19-4f72-4c39-9b23-08fd4e33c4f0","created_at":"2025-01-06T12:00:00.000Z","path":"docs/agile/tasks/missing-rate-limiting-smartgpt-files.md","content":"# Description\n\nCodeQL flagged the SmartGPT Bridge file routes for missing rate limiting. We need to scope the affected endpoints under sensible rate limits to prevent abuse while keeping the UX responsive.\n\n## Requirements/Definition of done\n\n- Add appropriate per-route rate limiting to SmartGPT Bridge v1 file routes.\n- Keep the existing global route registration working without breaking Fastify encapsulation.\n- Ensure the SmartGPT Bridge package continues to build successfully.\n- Document the change in the changelog directory.\n\n## Tasks\n\n- [x] Audit `packages/smartgpt-bridge/src/routes/v1/files.ts` for missing rate limiting.\n- [x] Implement per-route rate limit configuration for read and write endpoints.\n- [x] Run the SmartGPT Bridge build to confirm no regressions.\n- [x] Capture summary in `changelog.d`.\n\n## Relevant resources\n\n- CodeQL alert #426 (Missing rate limiting)\n- Fastify rate limit plugin documentation\n\n## Comments\n\n- 2025-01-06: Initial triage confirms v1 file routes lack explicit per-route rate limits even though the plugin is registered for v1.\n- 2025-01-06: Added scoped rate limit configuration, ran package build, and documented the change in the changelog.\n"}
{"id":"7f5d0e5d-2a38-4bb7-bb3a-2f8a4b1b2e31","title":"enso-browser-gateway ‚Äî handshake guard env timeout + cache ready","status":"todo","priority":"P2","owner":"","labels":["enso","gateway","handshake","ops"],"created":"2025-10-02T00:00:00.000Z","uuid":"7f5d0e5d-2a38-4bb7-bb3a-2f8a4b1b2e31","created_at":"2025-10-02T00:00:00.000Z","path":"docs/agile/tasks/enso-handshake-guard-env-timeout-and-cache.md","content":"#Todo\n\n## üõ†Ô∏è Description\nAdd `ENSO_HANDSHAKE_TIMEOUT_MS` env override. Short-circuit `ensureHandshake()` once ready; avoid hot-path awaits.\n\n## Requirements\n- [ ] Read timeout from env with sane default (10_000)\n- [ ] Cache readiness in ensureHandshake()\n- [ ] Tests cover timeout override + cache\n\n## ‚õìÔ∏è Blocks\n- ~~Merge PR #1451~~ ‚úÖ MERGED 2025-10-02T20:51:28Z\n\n## üîç Related\n- PR #1451\n"}
{"id":"8259052d-bf4c-48e8-9443-812569e19763","title":"auto generate agents md stubs from services structure md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"8259052d-bf4c-48e8-9443-812569e19763","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/auto-generate_agents_md_stubs_from_services_structure.md","content":"## üõ†Ô∏è Task: Auto-generate AGENTS.md stubs from services structure\n\nBuild a small script that scans `services/` and produces initial `AGENTS.md` files for each service. These stubs should include minimal metadata and links back to implementation files.\n\n---\n\n## üéØ Goals\n\n- Ensure every service has a corresponding documentation stub\n- Keep docs up to date as services are added or removed\n- Provide a foundation for more detailed service docs\n\n---\n\n## üì¶ Requirements\n\n- [ ] Script can be run via `npm run build:docs` or similar\n- [ ] Output files go to `docs/services/<service>/AGENTS.md`\n- [ ] Include service description, path, and tags\n\n---\n\n## üìã Subtasks\n\n- [ ] Enumerate existing services\n- [ ] Generate stub for each service\n- [ ] Add instructions to `docs/vault-config-readme.md`\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [vault config readme](../../vault-config-readme.md)\n- [[kanban]]\n#done\n\n"}
{"id":"8304b498-6811-40e3-8b44-60224b3ff846","title":"allow old unnessisary messages to decay from database while retaining index entries ids md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.507Z","uuid":"8304b498-6811-40e3-8b44-60224b3ff846","created_at":"2025-09-15T02:02:58.507Z","path":"docs/agile/tasks/allow_old_unnessisary_messages_to_decay_from_database_while_retaining_index_entries_ids.md","content":"Here‚Äôs a refined version that keeps your analogy to hardware memory hierarchies but makes it concrete enough to implement as a framework component:\n\n---\n\n## üõ†Ô∏è Description\n\nImplement a **multi-tier memory management system** for the Promethean framework that supports movement of context between different ‚Äúdistance‚Äù levels, enabling Eidolon fields and other cognitive components to access relevant data at the right speed and cost.\n\nInstead of a single monolithic context buffer, the system will maintain **phased memory tiers**:\n\n* **Working** ‚Äì Actively in use, highest priority, lowest latency. Directly influences ongoing processing.\n* **Recent** ‚Äì Recently used or possibly relevant. May drop out of active context but still cached in memory for fast retrieval.\n* **Important** ‚Äì Older but high-value information. May be stored in slower, larger-capacity layers (disk, DB, distributed storage) but still queryable.\n\nData flows **between tiers automatically** based on recency, relevance scoring, and available space‚Äîmirroring the behavior of CPU caches (L1/L2/L3), RAM, swap, and persistent storage. The further the tier from the working set, the higher the retrieval cost.\n\nThis task is not about strictly defining the scoring model for ‚Äúrelevance,‚Äù but about **building the mechanisms** (caches, queues, retrieval APIs) that make tiered memory movement possible.\n\n---\n\n## üéØ Goals\n\n* Design and implement a **tiered memory architecture** for context handling.\n* Ensure **automatic promotion/demotion** of items between tiers based on activity and scoring signals.\n* Allow all tiers to be **queryable** even if not in active context.\n* Support **pluggable storage backends** for lower tiers (local DB, remote FS, etc.).\n* Optimize for **speed in working tier** while enabling **deep historical retrieval**.\n\n---\n\n## üì¶ Requirements\n\n* [ ] Define tier structure and interfaces (`Working`, `Recent`, `Important`).\n* [ ] Implement tier-specific capacity limits and eviction strategies.\n* [ ] Create a **movement mechanism** for items between tiers.\n* [ ] Provide a **query API** that transparently retrieves from all tiers.\n* [ ] Support in-memory caches for higher tiers and pluggable persistence for lower tiers.\n* [ ] Expose metrics/logging for tier usage and retrieval latency.\n\n---\n\n## ‚úÖ Acceptance Criteria\n- Messages beyond the retention threshold are removed from the database.\n- Index entries continue to return valid IDs after cleanup.\n- Retention settings are configurable and documented.\n\n---\n\n## üìã Subtasks\n\n* [ ] Create in-memory LRU cache for **Working** tier.\n* [ ] Implement **Recent** tier with in-memory + optional on-disk swap.\n* [ ] Implement **Important** tier backed by DB or object storage.\n* [ ] Write tier migration logic (promotion/demotion).\n* [ ] Write tier-aware retrieval API.\n* [ ] Add hooks for relevance scoring (to be defined later).\n* [ ] Add metrics for retrieval frequency, promotion/demotion events, and hit/miss ratios.\n\n---\n\n## üîó Related Epics\n\n\\#framework-core\n\\#eidolon-support\n\n---\n\nIf you want, I can also give you a **mermaid diagram showing the memory tier flow**‚ÄîWorking ‚Üî Recent ‚Üî Important‚Äîwith promotion/demotion paths, which will help when we wire it into the Eidolon field loop.\nThat would make it much easier to see where scoring and eviction logic plug in.\n#IceBox\n#ready\n\n"}
{"id":"849ef7a0-6109-49be-9bab-c77739bbeec2","title":"build tiny web page that uses promclient in the br md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"849ef7a0-6109-49be-9bab-c77739bbeec2","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/build_tiny_web_page_that_uses_promclient_in_the_br.md","content":"## üõ†Ô∏è Task: Build tiny web page that uses `PromClient` in the browser to show live `process.state` (optional)\n\nGenerated from [../unique/2025.08.08.19.08.49.md](../unique/2025.08.08.19.08.49.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n\n\n\n#archive\n\n"}
{"id":"84b2e724-dd49-4519-ab5e-402973a70551","title":"find music that triggered copyright mute on twitch for analysis incoming","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"84b2e724-dd49-4519-ab5e-402973a70551","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/find-music-that-triggered-copyright-mute-on-twitch-for-analysis-incoming.md","content":"## üõ†Ô∏è Description\n\nThe videos have the title of the track in there. Just get rid of those ones.\n\n---\n\n## üéØ Goals\n\n- What are we trying to accomplish?\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#rejected\n\n"}
{"id":"8542a56c-b037-4a71-90e8-8bd0a0c8b49f","title":"Maintain Testing vs Working Databases w/ Migration Contract","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.520Z","uuid":"8542a56c-b037-4a71-90e8-8bd0a0c8b49f","created_at":"2025-09-15T02:02:58.520Z","path":"docs/agile/tasks/set-up-data-migration-pipeline-and-clearly-describe-conventions.md","content":"Here‚Äôs a repo-ready task you can paste into the board. It carves a bright line between **testing** and **working** (dev/staging/prod) databases and enforces a **migration contract** that every migration must satisfy before touching working data.\n\n# Maintain Testing vs Working Databases w/ Migration Contract\n\n**Owner:** Codex / Agent\n**Status:** Planned\n**Labels:** #data #migrations #contracts #mongodb #chroma #testing #ci #promethean\n\n---\n\n## üõ†Ô∏è Description\n\nSet up clean, isolated **testing databases** for MongoDB and Chroma that can be brought up, seeded, migrated, verified against a **formal contract**, and torn down‚Äîwithout ever touching **working** databases. Enforce this separation in code, config, and CI. Make migrations refuse to run if the contract doesn‚Äôt hold.\n\n---\n\n## üéØ Goals\n\n* Deterministic, ephemeral **test DBs** for migration authoring and verification.\n* A **migration contract** (schema + indexes + invariants + collection/embedding metadata) that is validated **pre** and **post** migration.\n* Guardrails so working DBs cannot be mutated from test paths (and vice versa).\n* CI path that spins Mongo+Chroma locally, runs migrations, validates, and publishes a report.\n\n---\n\n## üì¶ Requirements (DoD ‚Äî if missing, reject PR)\n\n* [ ] Distinct **URIs and names** for testing vs working DBs. No shared prefixes.\n* [ ] **Contract file(s)** committed (JSON/YAML) for Mongo (collections, indexes, required fields) and Chroma (collections, embedding dim/model, metadata).\n* [ ] **Preflight contract check** blocks migrations when actual ‚â† declared.\n* [ ] **Safety interlocks:** migrations require an explicit `MIGRATION_TARGET=test|working`, refuse on mismatch, and require a second confirm var for working.\n* [ ] **Spin-up/tear-down** scripts for ephemeral test DBs, including seed + snapshots.\n* [ ] CI job that runs a full **contract ‚Üí migrate ‚Üí verify** loop using the test DBs.\n* [ ] No new TS path aliases; exports via **@shared/ts/dist/** only.\n\n---\n\n## üìã Subtasks\n\n### 1) Naming, URIs, and Safety Interlocks\n\n* [ ] Add envs (documented; sample `.env.example`):\n\n  ```\n  # Working (dev/staging/prod)\n  MONGO_URI_WORKING=mongodb://...\n  MONGO_DB_WORKING=promethean_working\n  CHROMA_URL_WORKING=http://...\n  CHROMA_PREFIX_WORKING=promethean_working_\n\n  # Testing (ephemeral)\n  MONGO_URI_TEST= mongodb://localhost:37017\n  MONGO_DB_TEST= prom_test_${RUN_ID}\n  CHROMA_URL_TEST= http://localhost:38000\n  CHROMA_PREFIX_TEST= prom_test_${RUN_ID}_\n  MIGRATION_TARGET= test|working\n  MIGRATION_CONFIRM= I_ACKNOWLEDGE_WORKING_MUTATION\n  ```\n* [ ] In shared config `@shared/ts/dist/config/db.ts`, expose a **resolver**:\n\n  ```ts\n  export function resolveDbTarget(target: 'test'|'working') { /* returns URIs, names, prefixes */ }\n  ```\n* [ ] Hard fail if `MIGRATION_TARGET=working` without `MIGRATION_CONFIRM` exact match.\n* [ ] Hard fail if any test runner tries to connect to a `*_WORKING` URI.\n\n### 2) Contract Definition (source of truth)\n\n* [ ] Add `docs/data/contracts/mongo.schema.json`:\n\n  * Collections (name), **primary key field**, required fields, types (JSON Schema), **indexes** (fields, unique, partial filter).\n* [ ] Add `docs/data/contracts/chroma.schema.json`:\n\n  * Collections (name), **embedding\\_dim**, **embedding\\_model id**, required metadata keys.\n* [ ] Add contract loader/validator: `shared/ts/src/migrations/contract.ts`\n\n  * `validateMongoContract(db): Promise<Report>`\n  * `validateChromaContract(client): Promise<Report>`\n  * Emits actionable diffs (missing index, wrong dim, missing collection).\n\n### 3) Test DB Lifecycle (spin/seed/teardown)\n\n* [ ] Docker/Testcontainers helper under `scripts/db/`:\n\n  * `db-test-up`: start Mongo (@ 37017) + Chroma (@ 38000) locally (compose or Testcontainers).\n  * `db-test-down`: stop and prune.\n  * `db-test-seed`: load minimal seed fixtures (`fixtures/*.json`) to Mongo; create empty Chroma collections with correct dim/model.\n  * All commands must respect `RUN_ID=$(date +%s)` to isolate parallel runs.\n* [ ] Seed data factories in TS: `shared/ts/src/migrations/fixtures.ts` for testable content.\n\n### 4) Migration Runner + Contract Gates\n\n* [ ] Migration CLI `services/ts/migrations/run.ts`:\n\n  * `--target test|working`, `--up <version>`, `--down <version>`, `--dry-run`.\n  * **Preflight:** load target, run `validateMongoContract` & `validateChromaContract` **before** migration; **fail on diff** unless `--allow-contract-drift` explicitly provided (only for working with approval).\n  * Execute migration scripts from `services/ts/migrations/steps/*.ts` (versioned, idempotent).\n  * **Post-flight:** rerun validators; write JSON+MD report under `docs/data/reports/`.\n* [ ] Keep applied versions in per-DB collection `migrations_applied` (contains `env`, `version`, `git_sha`, `run_id`, timestamps).\n\n### 5) CI Integration\n\n* [ ] Job `migrations-contract-check` steps:\n\n  1. `db-test-up` ‚Üí `db-test-seed`\n  2. `node services/ts/migrations/run.js --target test --up latest`\n  3. `node services/ts/migrations/run.js --target test --verify-only`\n  4. Upload report artifacts (`*.json`, `*.md`).\n* [ ] CI must **fail** on any contract deviation or validator failure.\n\n### 6) Docs & Runbooks\n\n* [ ] `docs/data/contracts/README.md` ‚Äî how contracts are authored, reviewed, and bumped.\n* [ ] `docs/data/runbooks/test-migrations.md` ‚Äî spin up test DBs, run a migration, view reports, tear down.\n* [ ] `CONTRIBUTING.md` ‚Äî ‚ÄúNever point tests at working DBs; use `MIGRATION_TARGET=test`.‚Äù\n\n### 7) Guardrails & Linting\n\n* [ ] Pre-commit check: forbid `MONGO_URI=` literals or hardcoded URIs in code.\n* [ ] Script `scripts/ci/check-db-targets.ts`: greps for working URIs in test code and fails if found.\n* [ ] Runtime banner logs: print **Target=TEST/WORKING**, URIs redacted; capture in reports.\n\n---\n\n## üß© Design Notes\n\n* **Contracts are law.** Migrations that change schema must **update the contract** in the same PR.\n* **Dim/model pinning**: Chroma must refuse collection creation when `embedding_dim` or `embedding_model` mismatches contract.\n* **Idempotency**: every step checks existing state and only mutates when truly needed.\n* **Isolation by RUN\\_ID** prevents cross-test bleed and enables parallel CI.\n* **No new aliases**: exports available via **@shared/ts/dist/migrations/**.\n\n---\n\n## ‚úÖ Acceptance Criteria\n\n* [ ] `db-test-up`/`db-test-down` works locally; seeds load; isolated by `RUN_ID`.\n* [ ] `run.ts --target test` performs **preflight contract check**, migrates, and emits **success report**.\n* [ ] CI job passes on green contract; **fails** on any deviation.\n* [ ] Attempting `--target working` without `MIGRATION_CONFIRM` hard-fails.\n* [ ] Working and testing DBs **never share** database names or collection prefixes.\n* [ ] Docs exist (contracts README, runbook) and are linked from root README.\n\n---\n\n## üóÇÔ∏è Proposed Files/Paths\n\n* `shared/ts/src/migrations/contract.ts`\n* `shared/ts/src/migrations/report.ts`\n* `shared/ts/src/migrations/index.ts` (barrel export)\n* `services/ts/migrations/run.ts`\n* `services/ts/migrations/steps/NNNN_name.ts` (versioned steps)\n* `scripts/db/db-test-up.sh`, `db-test-down.sh`, `db-test-seed.sh`\n* `scripts/ci/check-db-targets.ts`\n* `docs/data/contracts/mongo.schema.json`\n* `docs/data/contracts/chroma.schema.json`\n* `docs/data/contracts/README.md`\n* `docs/data/runbooks/test-migrations.md`\n* `docs/data/reports/*.json`, `*.md`\n\n---\n\n## CLI Sketches\n\n```bash\n# Local: spin ephemeral test DBs\nRUN_ID=$(date +%s) ./scripts/db/db-test-up.sh && ./scripts/db/db-test-seed.sh\n\n# Run all pending migrations against test target, with contract verification\nMIGRATION_TARGET=test node services/ts/migrations/run.js --up latest\n\n# Verify only (no changes), output report\nMIGRATION_TARGET=test node services/ts/migrations/run.js --verify-only\n\n# (Danger) Working target requires explicit confirm\nMIGRATION_TARGET=working MIGRATION_CONFIRM=I_ACKNOWLEDGE_WORKING_MUTATION \\\n  node services/ts/migrations/run.js --up 2025_08_25_001\n```\n\n---\n\n## Step 1‚Äì4 Milestones\n\n* [ ] **Step 1 ‚Äî Separation & Safety:** envs/URIs, resolver, interlocks, naming.\n* [ ] **Step 2 ‚Äî Contracts:** author `mongo.schema.json` + `chroma.schema.json`, validator code.\n* [ ] **Step 3 ‚Äî Lifecycle & Runner:** test DB scripts, seed, migration CLI w/ pre/post contract checks.\n* [ ] **Step 4 ‚Äî CI & Docs:** CI pipeline, reports, runbooks; enforce in pre-commit.\n\n---\n\n## Commit Template\n\n```\nfeat(migrations): isolate test DBs and enforce migration contracts\n\n- Separate testing vs working URIs/names with safety interlocks\n- Add Mongo/Chroma contract schemas + validators\n- Test DB lifecycle scripts (spin/seed/teardown)\n- Migration runner with pre/post contract verification and reports\n- CI job for contract->migrate->verify loop\n- Docs: contracts README + runbook\n```\n\n\\#tags #promethean #mongodb #chroma #migration #contracts #testing #ci #safety #idempotent\n\n## Notes\n- Tests or documentation are missing; acceptance criteria not fully met.\n- Story Points: 8\n\n#in-progress\n\n"}
{"id":"8584c1fd-c8b6-4bf0-8cca-db9f91fe52c2","title":"just so much batman","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.515Z","uuid":"8584c1fd-c8b6-4bf0-8cca-db9f91fe52c2","created_at":"2025-09-15T02:02:58.515Z","path":"docs/agile/tasks/just-so-much-batman.md","content":"<hr class=\"__chatgpt_plugin\">\n\n### role::assistant<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n\n---\npromptId: smart_task_templater_md\nname: \"{{title}}\"\ndescription: A task on the kanban of the promethean system.\nauthor: Aaron Beavers\ntags:\n  - prompt-refinement\n  - promptcompiler\n  - \"#metaprogramming\"\n  - \"#metacompiler\"\nversion: 0.0.1\ndisableProvider: false\nprovider: ollama\ncommands: generate\nmode: replace\nstreaming: \"true\"\nmodel: ollama@llama3.2:latest\nprompt: \"{{title}}\"\ntask-id: TASK-{{date}}\npriority: p3\nsystem_commands: ['You are a helpful assistant.']\nfrequency_penalty: 0\nmax_tokens: 400\npresence_penalty: 0\nstream: true\ntemperature: 0.7\n---\n<hr class=\"__chatgpt_plugin\">\n\n<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n### role::assistant\n\nHere's a suggested revision of your context section:\n\n## Context\n### Changes and Updates\n- **What changed?**: [Describe the key changes that have occurred, e.g., updated requirements, new feature added]\n- **Where?**: [Specify the location or scope where these changes are relevant, e.g., specific project, department]\n- **Why now?**: [Explain why these changes are necessary at this time, e.g., due to deadline, feedback from stakeholders]\n\n## Inputs / Artifacts\n- ([link or path] to relevant documentation, data, or assets)\n\n## Definition of Done\n- [ ] test X passes: [ Briefly describe the testing scenario]\n- [ ] doc Y updated: [Mention the specific documentation or resource that has been updated]\n- [ ] PR merged: ([link to the PR] with a brief summary of changes)\n\n## Plan\n1. ‚Ä¶ [ Outline the high-level steps for completing this task, including any dependencies or resources required]\n2. ‚Ä¶\n\n## Relevant Resources\n\nYou might find [this](link) useful while working on this task.\n\n### Smart Connections Configuration\n```smart-connections\n{\n  \"render_markdown\": true,\n  \"show_full_path\": false,\n  \"exclude_blocks_from_source_connections\": false,\n  \"exclude_frontmatter_blocks\": true,\n  \"expanded_view\": false,\n  \"results_limit\": \"20\",\n  \"exclude_inlinks\": false,\n  \"exclude_outlinks\": false\n}\n```\n\n### Smart ChatGPT Configuration\n```smart-chatgpt\n```\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\nUsing this title write a reasonable task kanban task document:\n\n \n\n\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\n w\n"}
{"id":"8663f3b7-58fc-407b-b3fe-8460f47c7083","title":"identify ancestral resonance patterns md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.514Z","uuid":"8663f3b7-58fc-407b-b3fe-8460f47c7083","created_at":"2025-09-15T02:02:58.514Z","path":"docs/agile/tasks/identify_ancestral_resonance_patterns.md","content":"## üõ†Ô∏è Task: Identify ancestral resonance patterns\n\nResearch how ideas echo through the system over time and across\ngenerations of agents. \"Resonance patterns\" refer to recurring\nfragments or emotional states that reappear in different contexts.\n\n---\n\n## üéØ Goals\n\n- Detect repeated phrases or motifs in long‚Äëterm memory\n- Correlate them with shifts in Eidolon field metrics\n- Map possible \"ancestral\" links between agents or sessions\n\n---\n\n## üì¶ Requirements\n\n- [ ] Access historical chat logs and field data\n- [ ] Implement pattern search across logs (ngrams or embeddings)\n- [ ] Visualize clusters of related fragments\n\n---\n\n## üìã Subtasks\n\n- [ ] Export memory snapshots from `data/` directories\n- [ ] Use Python/NumPy to compute cosine similarity of embeddings\n- [ ] Plot resonance timelines with Matplotlib\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- Requires migrated history from [Extract docs from riatzukiza.github.io](Extract%20docs%20from%20riatzukiza.github.io.md)\n\n## ‚õìÔ∏è Blocks\n\n- Later emotional field calibration work\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n## ‚ùì Questions\n\n- Should resonance search include emotional embeddings or just lexical ones?\n- What time span of logs is considered \"ancestral\" for this project?\n#IceBox\n\n"}
{"id":"87df3419-fc1f-4adc-b258-dcf5b47da9d1","title":"start eidolon","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.521Z","uuid":"87df3419-fc1f-4adc-b258-dcf5b47da9d1","created_at":"2025-09-15T02:02:58.521Z","path":"docs/agile/tasks/start_eidolon.md","content":"## üõ†Ô∏è Task: Start Eidolon\n\nBootstrap the **Eidolon** service that manages emotion-state tracking and\nreward calculations. This task sets up a minimal Python package under\n`services/eidolon/` with a command-line entry point and README so future tasks\ncan iterate on the underlying field mechanics.\n\n---\n\n## üéØ Goals\n\n- Provide a runnable service skeleton in `services/eidolon/`\n- Mirror patterns used by `services/cephalon` for configuration and logging\n- Enable future integration with emotional metrics collection\n- Include placeholder classes that mirror structures in `pseudo/eidolon-field-scratchpad.lisp`\n- Ensure the service can log basic state information to the console\n\n---\n\n## üì¶ Requirements\n\n- [ ] Create package structure `services/eidolon/` with `__init__.py` and `main.py`\n- [ ] Implement a minimal `EidolonState` class with a simple update loop that prints \"Eidolon running\"\n- [ ] Add a placeholder config file `services/eidolon/config.json`\n- [ ] Include a `README.md` describing how to run the service with Python\n- [ ] Implement a minimal `EidolonState` class with a simple update loop\n- [ ] Expose a CLI entry point: `python -m services.eidolon`\n\n---\n\n## üìã Subtasks\n\n- [ ] Copy patterns from `services/stt/` for CLI entry point\n- [ ] Wire up a simple logging setup using `shared/py/utils`\n- [ ] Add an npm script or Makefile target to start Eidolon\n- [ ] Translate key pseudocode structures from `pseudo/eidolon-field-scratchpad.lisp`\n- [ ] Write a basic unit test ensuring the service starts and updates\n- [ ] Document next steps for expanding the field model\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#done\n\n"}
{"id":"88f4ecec-11b4-4883-ab71-7d3822171a2f","title":"ecs mongo adapter library","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"88f4ecec-11b4-4883-ab71-7d3822171a2f","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/ecs_mongo_adapter_library.md","content":"ECS‚ÄìMongo Adapter Library\n\nGoal: Implement a reusable MongoDB persistence adapter for the ECS so ECS acts as our ORM.\n\nDeliverables:\n- `shared/ts/src/agent-ecs/persist/mongo.ts`: `MongoStorageAdapter` with:\n  - `save(entityId, componentName, data, meta)` upsert to `{ _id: <eid::component>, eid, component, data, ts }`\n  - `remove(entityId, componentName)` delete\n  - `load(world, filters?)` hydrate world from collections\n  - Index helpers per component\n- Hook into ECS world via lifecycle callbacks (component set/remove) to persist writes.\n- Configurable write modes: immediate, buffered debounce, snapshot.\n\nExit Criteria:\n- Unit tests: component set/remove triggers Mongo upsert/delete.\n- Works with existing components (Turn, Utterance, TranscriptFinal, VAD).\n\n#incoming #ecs #mongo #orm\n\n"}
{"id":"8925f5a8-40ab-44e4-ad9e-54db7d617996","title":"validate voice-demo microphone integration","status":"icebox","priority":"P2","owner":"","labels":["audio","manual-test"],"created":"2025-09-24T00:00:00.000Z","uuid":"8925f5a8-40ab-44e4-ad9e-54db7d617996","created_at":"2025-09-24T00:00:00.000Z","path":"docs/agile/tasks/validate-voice-demo-microphone-integration.md","content":"# Description\n\nValidate the `voice-demo` CLI command against real microphone devices and document any platform-specific setup guidance needed for contributors.\n\n## Requirements/Definition of done\n\n- Run the CLI on macOS and Linux with a physical microphone attached.\n- Capture at least two short transcripts to confirm audio capture, streaming, and transcript playback.\n- Document working `arecord`/`sox`/`ffmpeg` pipelines (or native equivalents) for piping microphone PCM into the CLI.\n- File follow-up issues for any defects uncovered during testing.\n\n## Tasks\n\n- [ ] Verify microphone recording pipeline on macOS.\n- [ ] Verify microphone recording pipeline on Linux.\n- [ ] Record findings and setup steps in the package README.\n- [ ] Open issues for any transport or audio capture bugs encountered.\n\n## Relevant resources\n\n- `packages/enso-protocol/src/cli.ts`\n- `packages/enso-protocol/src/audio.ts`\n- Voice transport notes in the latest PR description.\n\n## Comments\n\nAdd observations about latency, buffering, or compatibility as testing progresses.\n"}
{"id":"89643344-84c1-499f-82cc-9b5430fffb72","title":"define permission schema in agents 1 md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.511Z","uuid":"89643344-84c1-499f-82cc-9b5430fffb72","created_at":"2025-09-15T02:02:58.511Z","path":"docs/agile/tasks/define_permission_schema_in_agents_1.md","content":"## üõ†Ô∏è Task: Define permission schema in AGENTS.md\n\nCreate a concise section in the root `AGENTS.md` explaining how agents declare\nallowed actions and resource access. The schema will be consumed by the future\n\"permission gating\" middleware to enforce boundaries.\nallowed actions and resource access. The schema should be simple enough for\nmanual editing but structured so a parser can enforce permissions during agent\nexecution.\n\n---\n\n## üéØ Goals\n\n- Specify a human-readable schema for permission rules\n- Ensure the schema can be parsed by the permission gating layer\n- Provide examples for different agent roles\n\n---\n\n## üì¶ Requirements\n\n- [ ] Outline required fields (action, scope, default behavior)\n- [ ] Document YAML and JSON examples\n- [ ] Provide one sample per agent in `agents/*/config/permissions.yaml`\n- [ ] Provide at least one example for a read-only agent and one for a full-access agent\n- [ ] Link to any mathematical reasoning notes\n\n---\n\n## ‚úÖ Acceptance Criteria\n- Root `AGENTS.md` contains a permission schema section.\n- YAML and JSON examples illustrate read-only and full-access agents.\n- Sample config files for both roles exist under `agents/*/config/`.\n\n---\n\n## üìã Subtasks\n\n- [ ] Draft schema description inside `AGENTS.md`\n- [ ] Add example snippet under `agents/duck/config/`\n- [ ] Review with team for completeness\n- [ ] Update any affected tasks\n- [ ] Mirror the schema in `bridge/protocols/permission-schema.md` for future API use\n\n---\n\n## üîó Related Epics\n\n#framework-core #eidolon #Dorian #layer2 \n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\n- [Create permission gating layer](Create%20permission%20gating%20layer.md)\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n## ‚ùì Questions\n\n- Should permissions support wildcards for actions or be explicit only?\n#ice-box\n\n"}
{"id":"8a791d5f-757a-4154-bba2-e14886da4c30","title":"move board tooling into kanban package","status":"in_progress","priority":"P3","owner":"","labels":["framework-core","devtools"],"created":"2025-09-21T00:00:00.000Z","uuid":"8a791d5f-757a-4154-bba2-e14886da4c30","created_at":"2025-09-21T00:00:00.000Z","path":"docs/agile/tasks/move-board-tools-into-kanban-package.md","content":"Move the TypeScript utilities under `tools/board/` into the maintained `@promethean/kanban` package so they ship with the rest of the board automation CLI.\n\n## Plan\n- [x] Inspect the legacy `tools/board/*` entrypoints and data contracts.\n- [x] Relocate the scripts into a `packages/kanban/src/board` directory, exporting them as part of the package build.\n- [x] Update documentation and any hard-coded paths or references to the old location.\n- [ ] Ensure `@promethean/kanban` builds cleanly and lint passes on touched files. *(Blocked: `tsc -p tsconfig.json` currently fails in `src/lib/types.ts` and `src/lib/jsonl.ts`.)*\n\n## Definition of Done\n- Board utilities live inside `packages/kanban/src/` with updated imports.\n- Docs reference the new invocation path (`pnpm tsx packages/kanban/...`).\n- Tests and builds for the `@promethean/kanban` package succeed.\n"}
{"id":"8b3c665c-d6ff-4a0a-9fb9-b2dbefa35fb4","title":"Get all existing pipelines functional","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.514Z","uuid":"8b3c665c-d6ff-4a0a-9fb9-b2dbefa35fb4","created_at":"2025-09-15T02:02:58.514Z","path":"docs/agile/tasks/get all existing piper pipelines functional.md","content":"# Get all existing pipelines functional\nWe've started working on several pipelines but only one of them works to any meaningful extant.\nMost of them were set up before I thought about a \"piper\".\nThey may kinda work when ran in isolation.\nbut they do not work with piper\n\n# Existing Pipeines\n\"name\": \"symdocs\",\n\n\"name\": \"simtasks\",\n\"name\": \"codemods\",\n\"name\": \"semver-guard\",\n\"name\": \"board-review\",\n\"name\": \"sonar\",\n\"name\": \"readmes\",\n\n\"name\": \"buildfix\",\n\n\"name\": \"test-gap\",\n\"name\": \"docops\",\n\n\n## Working Pipeline\n\n\"name\": \"docops\",\n\n## Non working pipelines\n\n\"name\": \"symdocs\",\n\n\"name\": \"simtasks\",\n\"name\": \"codemods\",\n\"name\": \"semver-guard\",\n\"name\": \"board-review\",\n\"name\": \"sonar\",\n\"name\": \"readmes\",\n\n\"name\": \"buildfix\",\n\n\"name\": \"test-gap\",\n\n"}
{"id":"8b52c0aa-1f68-4db1-bd4e-612c7c9f853c","title":"Create MCP tool for Nx package scaffolding","status":"done","priority":"P3","owner":"","labels":[],"created":"2025-10-02T03:30:00.000Z","uuid":"8b52c0aa-1f68-4db1-bd4e-612c7c9f853c","created_at":"2025-10-02T03:30:00.000Z","path":"docs/agile/tasks/create-nx-package-mcp-tool.md","content":"## üõ†Ô∏è Description\n\nAdd an MCP tool that runs the workspace Nx package generator so agents can scaffold new packages consistently with repository presets.\n\n---\n\n## üéØ Goals\n\n- Allow MCP clients to create new packages by calling the existing Nx generator.\n- Support preset selection that maps to repository package templates.\n- Provide dry-run support so agents can preview the scaffold when needed.\n\n---\n\n## üì¶ Requirements\n\n- [x] Implement an MCP tool that executes the Nx package generator with name and preset inputs.\n- [x] Accept human-friendly preset aliases (library, service, frontend) that map to generator presets.\n- [x] Return stdout/stderr and exit information so agents can audit generator runs.\n- [x] Cover the new tool with unit tests.\n- [x] Document the tool in package change notes.\n\n---\n\n## üìã Subtasks\n\n- [x] Define schema validation and preset normalization for the tool input.\n- [x] Implement command execution via pnpm exec nx.\n- [x] Add AVA tests for argument construction and alias mapping.\n- [x] Register the tool with the MCP server registry.\n- [x] Add changelog entry summarizing the tool.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n- tools/generators/package/README.md\n- packages/mcp/src/tools/pnpm.ts\n\n## Notes\n- Story Points: 3\n\n#done\n\n"}
{"id":"8b57e951-99d2-4242-a56d-578a3f11cda6","title":"Fix Piper pipeline caching regressions","status":"in_progress","priority":"P2","owner":"","labels":["piper","pipelines"],"created":"2025-09-28T23:20:35.509916+00:00","uuid":"8b57e951-99d2-4242-a56d-578a3f11cda6","created_at":"2025-09-28T23:20:35.509916+00:00","path":"docs/agile/tasks/fix-piper-pipeline-caching.md","content":"## üõ†Ô∏è Task: Fix Piper pipeline caching regressions\n\n### Context\n- AVA runner tests `runPipeline executes steps and caches on second run` and `runPipeline re-executes only affected steps when an intermediate input changes` are failing.\n- Both failures report that the second pipeline run does not read from cache (expected `true`, observed `false`).\n- Resolving this is necessary to keep Piper's caching contract intact and unblock the test suite.\n\n### Definition of Done\n- [x] Identify the regression preventing cache hits on the second pipeline run.\n- [ ] Add or update automated coverage to guard against the regression.\n- [x] Ensure `pnpm test --filter @promethean/piper` passes locally.\n- [x] Document findings and remediation in this task note.\n\n### Plan\n1. Reproduce the failing AVA tests locally and trace pipeline cache metadata for the affected steps.\n2. Inspect Piper runner cache key generation and cache read logic for regressions introduced since the last passing state.\n3. Implement fixes ensuring pipeline steps skip execution when cached data is valid.\n4. Update or extend tests if needed and re-run the Piper package test suite.\n\n### References\n- `packages/piper/src/runner.ts`\n- `packages/piper/src/tests/runner.test.ts`\n- `packages/piper/src/cache/*`\n\n### Notes\n- Persist both content and mtime output hashes for each step so cache mode changes do not invalidate stored fingerprints.\n- `shouldSkip` now compares against hash values keyed by mode, preventing mismatches when switching between content and mtime hashing.\n- Reworked the file-tree dev-ui test to spin up the server lazily and record API calls via `sessionStorage`, eliminating orphaned watchers when filtering test runs.\n- Re-ran the previously failing runner tests and the dev-ui file-tree scenario to confirm caching and watcher behaviour are both green.\n"}
{"id":"8c0e512b-27b8-4132-83b1-5da880210b84","title":"document board usage guidelines 1 md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"8c0e512b-27b8-4132-83b1-5da880210b84","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/document_board_usage_guidelines_1.md","content":"## üõ†Ô∏è Task: Document board usage guidelines\n\nProvide a concise how-to for contributors on interacting with the Obsidian Kanban board. Include common conventions, WIP limits, and tips for linking tasks.\n\n---\n\n## üéØ Goals\n- Explain how to add new tasks using the template\n- Clarify column purposes and typical transitions\n- Encourage linking to relevant docs and code\n\n---\n\n## üì¶ Requirements\n- [ ] Create a guideline page under `docs/`\n- [ ] Reference `Process.md` and `AGENTS.md`\n- [ ] Mention WIP limits noted in column headings\n\n---\n\n## üìã Subtasks\n- [ ] Draft initial guide\n- [ ] Review with maintainers\n- [ ] Add cross-links from board and README\n\n---\n\n## üîó Related Epics\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\n- Better contributor onboarding\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n#done\n\n"}
{"id":"8d666a9c-d5aa-4acd-a3b1-d71f4a21d223","title":"move all testing to individual services md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.517Z","uuid":"8d666a9c-d5aa-4acd-a3b1-d71f4a21d223","created_at":"2025-09-15T02:02:58.517Z","path":"docs/agile/tasks/move_all_testing_to_individual_services.md","content":"#Duplicate\n#rejected\n\n"}
{"id":"8e11e83f-6f3b-4f64-88b6-40905faba1f6","title":"add mongodedupe and replace critical consumers wit md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.505Z","uuid":"8e11e83f-6f3b-4f64-88b6-40905faba1f6","created_at":"2025-09-15T02:02:58.505Z","path":"docs/agile/tasks/add_mongodedupe_and_replace_critical_consumers_wit.md","content":"## üõ†Ô∏è Task: Add `MongoDedupe` and replace critical consumers with `subscribeExactlyOnce`\n\nGenerated from [../unique/2025.08.08.19.08.25.md](../unique/2025.08.08.19.08.25.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"9061fb44-92a0-4799-a40d-420dd395577c","title":"ensure github compatible markdown settings are doc md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"9061fb44-92a0-4799-a40d-420dd395577c","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/ensure_github_compatible_markdown_settings_are_doc.md","content":"## üõ†Ô∏è Task: Ensure GitHub-compatible markdown settings are documented\n\nSome Obsidian features don't render well on GitHub. We need clear guidelines so contributors produce markdown that looks correct both locally and on GitHub.\n\n---\n\n## üéØ Goals\n- Document the vault settings that affect GitHub rendering\n- Explain how to convert wikilinks to standard links\n- Provide formatting tips (line breaks, code fences, etc.)\n\n---\n\n## üì¶ Requirements\n- [x] Add a section to `vault-config/README.md` describing recommended settings\n- [x] Mention any plugins used to export or format markdown\n- [ ] Include Prettier or markdownlint configuration if applicable\n\n---\n\n## üìã Subtasks\n- [x] Review current Obsidian settings\n- [x] Summarize options that influence markdown syntax\n- [x] Update documentation and link from `AGENTS.md`\n\n---\n\n## üîó Related Epics\n#documentation\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\nNothing\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n#rejected\n\n"}
{"id":"907f268b-ced3-4206-aa3e-82b56dc811a1","title":"<verb> <thing> <qualifier>","status":"todo","priority":"p3","owner":"","labels":["board:auto","lang:ts"],"created":"2025-09-15T02:02:58.517Z","uuid":"907f268b-ced3-4206-aa3e-82b56dc811a1","created_at":"2025-09-15T02:02:58.517Z","path":"docs/agile/tasks/migrate legacy js services to ts.md","content":"# Legacy JS services are migrated to TS\n\nI had failed to move several of these to the new packages before, I recovered them from the git history. Now they need to be integrated\n\n## Goals\n- The legacy projects have their package name field updated to reflect the new namespace\n- The legacy projects are refactored under the new tsconfig\n- The legacy projects work as they used to.\n\n## Nessisary Services\n   - [ ] broker\n   - [ ] heartbeat\n   - [ ] vision\n   - [ ] eidolon-field\n   \n## Maybe services\n   - [ ] event-gateway\n   - [ ] event-hub\n   - [ ] health\n   - [ ] proxy\n\n## Notes\n\nIt may not be worth it to  migrate the proxy service.\nThe \n\n"}
{"id":"90c25667-5a2b-4c8c-9d40-5b03e54675f2","title":"Organize backend packages under services","status":"todo","priority":"p3","owner":"","labels":["framework-core","refactor"],"created":"2025-09-27T00:10:00.000Z","uuid":"90c25667-5a2b-4c8c-9d40-5b03e54675f2","created_at":"2025-09-27T00:10:00.000Z","path":"docs/agile/tasks/organize-backend-packages.md","content":"#Todo\n\n## üõ†Ô∏è Description\n\n- Track the second wave of the package reorg, relocating service and backend-oriented workspaces into a `packages/services/` namespace.\n\n## Description\n- **What changed?** Frontend packages now live in `packages/frontends/`, enabling a similar grouping for API and worker services.\n- **Where is the impact?** Applies to packages that expose Fastify or worker processes (e.g., `auth-service`, `broker`, `webcrawler-service`).\n- **Why now?** Aligns service code to a predictable path ahead of deployment automation updates.\n- **Supporting context** Follow-on slice from the frontend reorganization.\n\n## Goals\n- Inventory candidates for the services group with acceptance notes.\n- Update Nx and pnpm discovery to account for `packages/services/*`.\n- Adjust deployment scripts and docs referencing the old paths.\n\n## Requirements\n- [ ] Confirm service list and capture acceptance criteria.\n- [ ] Update workspace config and project files for moved services.\n- [ ] Validate builds/lints for the relocated packages.\n- [ ] Document migration notes.\n\n## Subtasks\n1. Collect a definitive list of service packages and categorize their runtimes.\n2. Draft migration steps and update scripts/configs accordingly.\n3. Execute the moves and regression test the builds.\n\nEstimate: 5\n\n---\n\n## üîó Related Epics\n\n- [[kanban]]\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- None\n\n## ‚õìÔ∏è Blocks\n\n- None\n\n---\n\n## üîç Relevant Links\n\n- Pending implementation notes.\n"}
{"id":"90d0e75e-0f56-4e0e-b470-20e0145ad43d","title":"Omni unified service specification and planning","status":"in_progress","priority":"P1","owner":"","labels":["omni","planning"],"created":"2025-09-21T02:40:00Z","uuid":"90d0e75e-0f56-4e0e-b470-20e0145ad43d","created_at":"2025-09-21T02:40:00Z","path":"docs/agile/tasks/omni-unified-service-spec.md","content":"## üß≠ Context\n- **What changed?**: Leadership requested a unified Omni server that fronts REST, GraphQL, WebSocket, and MCP interfaces from a single Fastify host on one domain/port.\n- **Where?**: Impacts `@promethean/smartgpt-bridge`, the new `@promethean/mcp` package, and forthcoming Omni packages/services.\n- **Why now?**: We must codify the shared protocol and roadmap before implementation so every adapter/client can ship against a stable spec.\n\n## üì• Inputs / Artifacts\n- Baseline analysis from 2025-09-21 planning session.\n- Existing bridge + MCP package source.\n- Repository agile process documentation.\n\n## ‚úÖ Definition of Done\n- [ ] Publish a protocol + architecture spec in `docs/architecture/omni/`.\n- [ ] Record a phased implementation plan with milestones and owners/T-shirt sizes.\n- [ ] Capture work breakdown into actionable tasks queued on the kanban board.\n\n## üõ£ Plan\n1. Draft Omni protocol and service specification document (scope, interfaces, shared contracts, constraints).\n2. Author implementation roadmap with phase gates, dependencies, and test strategy.\n3. Derive concrete tasks (‚â§5 points) for near-term execution; ensure they reference the spec and align with agile FSM.\n4. Link artifacts and update kanban metadata for tracking.\n\n## üîó Dependencies\n- None (planning slice).\n\n## üìé Related Notes\n- [[docs/architecture/omni/omni-protocol-spec.md]] *(to be created)*\n- [[docs/architecture/omni/omni-service-roadmap.md]] *(to be created)*\n\n## üß™ Validation Strategy\n- Peer review of spec documents.\n- Kanban board reflects new tasks in appropriate column once synced.\n"}
{"id":"911d511f-7f6d-4f01-86c0-5129765a3f8f","title":"Group tooling and CLI packages","status":"todo","priority":"p3","owner":"","labels":["framework-core","refactor"],"created":"2025-09-27T00:14:00.000Z","uuid":"911d511f-7f6d-4f01-86c0-5129765a3f8f","created_at":"2025-09-27T00:14:00.000Z","path":"docs/agile/tasks/organize-tooling-packages.md","content":"#Todo\n\n## üõ†Ô∏è Description\n\n- Capture the follow-up slice for isolating CLI and operational tooling packages under `packages/tools/` (or similar).\n\n## Description\n- **What changed?** The frontend reorg established the pattern for nested namespaces and highlighted the need to cluster CLIs separately from runtime libraries.\n- **Where is the impact?** Applies to packages that publish binaries or automation scripts (`promethean-cli`, `kanban-cli`, `pm2-helpers`, etc.).\n- **Why now?** Keeps operational tooling discoverable and enables targeted CI/test pipelines.\n- **Supporting context** Builds on the staged refactor plan agreed in the earlier discussion.\n\n## Goals\n- Define inclusion criteria for tooling vs. libraries.\n- Outline new folder structure (e.g., `packages/tools/cli`, `packages/tools/ops`).\n- Note required updates to release docs and automation scripts.\n\n## Requirements\n- [ ] Inventory CLI/tooling packages with owners and release processes.\n- [ ] Draft migration plan with acceptance criteria.\n- [ ] Ensure Nx/pnpm configs support additional nested directories.\n- [ ] Document guidelines for new tooling packages.\n\n## Subtasks\n1. Audit packages exposing `bin` entries or CLI commands.\n2. Validate proposed structure with maintainers and capture follow-up actions.\n3. Stage documentation and automation updates before moving packages.\n\nEstimate: 3\n\n---\n\n## üîó Related Epics\n\n- [[kanban]]\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- None\n\n## ‚õìÔ∏è Blocks\n\n- None\n\n---\n\n## üîç Relevant Links\n\n- Pending analysis.\n"}
{"id":"9124fbc4-92c2-4b37-a234-95dc28dd17ff","title":"Resolve Biome lint errors in compiler package","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.519Z","uuid":"9124fbc4-92c2-4b37-a234-95dc28dd17ff","created_at":"2025-09-15T02:02:58.519Z","path":"docs/agile/tasks/resolve_biome_lint_errors_in_compiler.md","content":"# Resolve Biome lint errors in compiler package\n\n## Description\n`packages/compiler` fails Biome lint with multiple errors including `useTemplate`, `noUselessUndefinedInitialization`, `noCommaOperator`, `noNonNullAssertion`, `noSwitchDeclarations`, and `noUnusedImports`.\n\n## Goals\n- Address all Biome errors so `pnpm --filter @promethean/compiler lint` passes.\n\n## Requirements\n- Refactor code to satisfy each listed Biome rule.\n- Maintain existing functionality and test coverage.\n\n## Subtasks\n- [ ] Replace string concatenation with template literals.\n- [ ] Remove unnecessary `undefined` initializations.\n- [ ] Eliminate comma operator usage.\n- [ ] Remove non-null assertions where possible.\n- [ ] Scope switch-case declarations correctly.\n- [ ] Drop unused imports.\n\n#Todo #codex-task\n\n"}
{"id":"91b19036-1e17-4668-b89f-24fa9faf062f","title":"build data structures for eidolon field codex task md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"91b19036-1e17-4668-b89f-24fa9faf062f","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/build_data_structures_for_eidolon_field_codex_task.md","content":"## üõ†Ô∏è Task: Build data structures for Eidolon field #codex-task\n\nGenerated from [../unique/2025.07.28.18.07.20.md](../unique/2025.07.28.18.07.20.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#done\n\n"}
{"id":"936b26de-61b4-4d8d-94d7-171315a56ac9","title":"Setup MCP server endpoint for kanban tooling","status":"in_progress","priority":"P2","owner":"","labels":["mcp","kanban","automation"],"created":"2025-10-02T01:47:54.474Z","uuid":"936b26de-61b4-4d8d-94d7-171315a56ac9","created_at":"2025-10-02T01:47:54.474Z","path":"docs/agile/tasks/setup-kanban-mcp-server.md","content":"#InProgress\n\n## üõ†Ô∏è Description\n\n- Wire the unified MCP server to expose tooling from the `@promethean/kanban` package.\n- Ensure the HTTP transport serves a `/kanban` endpoint with the full toolset needed for board management.\n- Provide regression tests around the kanban MCP adapters so future refactors stay safe.\n\n## Description\n- **What changed?** We need to expose kanban board automation through the MCP service instead of ad-hoc CLI scripts.\n- **Where is the impact?** Changes land in `packages/mcp` plus the shared `promethean.mcp.json` config that powers tooling.\n- **Why now?** Kanban automation is required for Codex Cloud agent workflows and should be accessible over MCP.\n- **Supporting context** `packages/mcp/src/index.ts`, `packages/mcp/src/tools`, `@promethean/kanban` library exports.\n\n## Goals\n- Deliver kanban-aware MCP tools for listing, searching, and updating board tasks.\n- Update configuration/documentation so the new endpoint is discoverable.\n- Cover the adapters with AVA tests exercising read/write flows against sample boards.\n\n## Requirements\n- [ ] test X passes: Ensure `pnpm --filter @promethean/mcp test` succeeds locally.\n- [ ] doc Y updated: Document the new kanban MCP tools in `packages/mcp/README.md`.\n- [ ] PR merged: (link to the PR) summarizing the MCP kanban endpoint work.\n- [ ] Additional constraints or non-functional requirements are addressed: Keep implementation functional and avoid shared mutable state in tool factories.\n\n## Subtasks\n1. Design tool factories that wrap `@promethean/kanban` helpers with MCP schemas.\n2. Extend the MCP config/registry so the kanban endpoint is reachable over HTTP.\n3. Backfill tests covering get/update/search flows with temporary board fixtures.\n\nEstimate: 3 (Fibonacci).\n\n---\n\n## üîó Related Epics\n\n- [[kanban]]\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- None\n\n## ‚õìÔ∏è Blocks\n\n- None\n\n---\n\n## üîç Relevant Links\n\n- `packages/mcp/src/tools/kanban.ts`\n- `promethean.mcp.json`\n"}
{"id":"94a2959e-21b1-4a41-9c1f-85787c1a0894","title":"Format auth-service README with Prettier","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"94a2959e-21b1-4a41-9c1f-85787c1a0894","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/format_auth_service_readme_with_prettier.md","content":"# Format auth-service README with Prettier\n\n## Description\nThe `auth-service` package lint step reported Prettier formatting issues in `README.md`.\n\n## Goals\n- Apply Prettier formatting to the README to satisfy the lint script.\n\n## Requirements\n- Run `prettier --write` on `packages/auth-service/README.md`.\n- Ensure lint and tests still pass.\n\n## Subtasks\n- [ ] Format `packages/auth-service/README.md` with Prettier.\n- [ ] Re-run lint for the package.\n\n#Todo #codex-task\n\n"}
{"id":"95a6c317-d9f6-4574-b2bf-80fdf56cb8f1","title":"audio processing service","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.507Z","uuid":"95a6c317-d9f6-4574-b2bf-80fdf56cb8f1","created_at":"2025-09-15T02:02:58.507Z","path":"docs/agile/tasks/audio-processing-service.md","content":"## üõ†Ô∏è Description\n\nIsolate audio manipulation (e.g., encoding, normalization, filtering) into a dedicated stateless service rather than embedding logic in agents or providers.\n\n---\n\n## üéØ Goals\n\n- Provide reusable audio processing pipeline for all agents\n- Simplify service composition by treating audio operations as standalone tasks\n\n---\n\n## üì¶ Requirements\n\n- [ ] Service accepts raw/encoded audio and returns processed output\n- [ ] Exposes RPC or broker interface for other services\n- [ ] Includes tests covering common transforms\n\n---\n\n## üìã Subtasks\n\n- [ ] Design service API and message schema\n- [ ] Implement core transforms (normalize, denoise, resample)\n- [ ] Wire service into existing agent audio flow\n- [ ] Write unit tests and usage docs\n\n---\n## üßÆ Story Points\n\n5\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#framework-core #Ready\n#ready\n\n\n"}
{"id":"95f4dfee-f348-4bc5-8046-aa5fb2395729","title":"detect contradictions in memory codex task md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.511Z","uuid":"95f4dfee-f348-4bc5-8046-aa5fb2395729","created_at":"2025-09-15T02:02:58.511Z","path":"docs/agile/tasks/detect_contradictions_in_memory_codex_task.md","content":"## üõ†Ô∏è Task: Detect contradictions in memory #codex-task\n\nGenerated from [../unique/2025.07.28.18.07.20.md](../unique/2025.07.28.18.07.20.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"96a9d9e3-69a0-4ad1-8b77-08f9d1bc97be","title":"add startchangelogprojector for any compaction lik md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.505Z","uuid":"96a9d9e3-69a0-4ad1-8b77-08f9d1bc97be","created_at":"2025-09-15T02:02:58.505Z","path":"docs/agile/tasks/add_startchangelogprojector_for_any_compaction_lik.md","content":"## üõ†Ô∏è Task: Add **startChangelogProjector** for any compaction-like topic you want live-queryable\n\nGenerated from [[2025.08.08.20.08.56]]\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"9870434b-5bf2-48ca-a588-d6769ec54796","title":"add codex layer to emacs","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.503Z","uuid":"9870434b-5bf2-48ca-a588-d6769ec54796","created_at":"2025-09-15T02:02:58.503Z","path":"docs/agile/tasks/add-codex-layer-to-emacs.md","content":"## üõ†Ô∏è Description\n\nDescribe your task\n\n## üì¶ Requirements\n- Provide an Emacs layer for interacting with Codex.\n- Include commands to send code and receive responses inside Emacs.\n- Document installation and configuration steps.\n\n## ‚úÖ Acceptance Criteria\n- Emacs layer can initiate Codex prompts from a buffer.\n- Users can execute a command to send the current buffer to Codex and view the reply.\n- Setup instructions enable others to reproduce the integration.\n\n## Tasks\n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Relevent resources\nYou might find [this] useful while working on this task\n[ChatGPT - Codex layer](https://chatgpt.com/share/68a74365-e284-8004-8911-bb2fb08b1f3e)\n\n## Comments\nUseful for agents to engage in append only conversations about this task.\n\n## Story Points\n\n- Estimate: 3\n- Assumptions: Emacs environment can load the Codex integration layer.\n- Dependencies: Existing Codex and Emacs plugin support.\n\n"}
{"id":"98d1da44-0b69-45e2-bd09-204641e87de5","title":"tool chain management system md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.522Z","uuid":"98d1da44-0b69-45e2-bd09-204641e87de5","created_at":"2025-09-15T02:02:58.522Z","path":"docs/agile/tasks/tool_chain_management_system.md","content":"## üõ†Ô∏è Description\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- What are we trying to accomplish?\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n### **Related Tasks**  \n- [[integrate-sonarqube-into-devops]] (if SonarQube integration is part of this tool chain).  \n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nLet me know if you need further details or adjustments!\n#ice-box\n\n"}
{"id":"98ddc760-e6e1-4239-92ae-db864ce5bcaf","title":"look into why the state object never seems to get updated md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.516Z","uuid":"98ddc760-e6e1-4239-92ae-db864ce5bcaf","created_at":"2025-09-15T02:02:58.516Z","path":"docs/agile/tasks/look_into_why_the_state_object_never_seems_to_get_updated.md","content":"## üõ†Ô∏è Description\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Add Ollama formally to pipeline\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n\n#archive\n\n"}
{"id":"991b27c3-0b63-49a5-acd3-d730b28e31fc","title":"Ban Python; Migrate to Hy; Compile to `./dist`","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.519Z","uuid":"991b27c3-0b63-49a5-acd3-d730b28e31fc","created_at":"2025-09-15T02:02:58.519Z","path":"docs/agile/tasks/replace-all-python-properly-with-hy-incoming.md","content":"# Ban Python; Migrate to Hy; Compile to `./dist`\n\n**Owner:** Codex / Agent\n**Status:** continue coding\n**Labels:** #architecture #lang #hy #python-ban #build #ci #tooling #promethean\n\n---\n\n## üß® Rationale (short + blunt)\n\n* One language family per runtime. If we‚Äôre in CPython land, it‚Äôll be **Lisp (Hy)**.\n* No more ‚Äúaccidental Python‚Äù sneaking into commits.\n* Reproducible build: Hy sources ‚Üí compiled Python in `./dist`, **ignored by git**.\n\n---\n\n## Requirements / Definition of Done\n\n* **No `.py` files** committed anywhere outside `./dist`.\n* All former Python services/libs are **ported to Hy** (`.hy`), with identical public APIs.\n* Hy build produces **importable Python** under `./dist` that mirrors source layout.\n* **Pre-commit hook** blocks and un-stages any tracked `.py` outside `./dist`.\n* Repo docs explicitly state **‚ÄúPython source forbidden‚Äù**; multiple touchpoints.\n* CI runs **only** from Hy sources (compiled in job), never from stray `.py`.\n* `./dist` is **git-ignored**; local/CI builds deterministic.\n\n---\n\n## High-Level Plan\n\n1. **Quarantine:** add guardrails (hook, gitignore, CI) before changes.\n2. **Module Layout:** standardize Hy source roots (mirroring TS/Sibilant).\n3. **Build:** compile Hy ‚Üí Python into `./dist` with preserved package structure.\n4. **Ports:** convert Python modules to Hy with 1:1 APIs.\n5. **Entrypoints:** run from compiled outputs (or `hy -m` during dev).\n6. **Docs & Enforcement:** loud policy + CI checks.\n\n---\n\n## üìã Tasks\n\n### 0) Repo Guardrails (do this first)\n\n* [ ] **.gitignore**\n\n  * Add/confirm:\n\n    ```\n    dist/\n    **/__pycache__/\n    **/*.pyc\n    ```\n* [ ] **Pre-commit hook** (blocks & un-stages `.py` outside `dist/`)\n  Create `.githooks/pre-commit` and wire via `.git/config` or `core.hooksPath=.githooks`:\n\n  ```bash\n  #!/usr/bin/env bash\n  set -euo pipefail\n\n  # Find staged .py files not under dist/\n  offenders=$(git diff --cached --name-only --diff-filter=ACMR | rg '\\.py$' | rg -v '^dist/')\n\n  if [[ -n \"${offenders}\" ]]; then\n    echo \"‚ùå Python source is forbidden. Unstaging offending files:\"\n    echo \"${offenders}\" | sed 's/^/   - /'\n    # Unstage each offender\n    while read -r f; do\n      [[ -z \"$f\" ]] && continue\n      git restore --staged \"$f\" || true\n    done <<< \"${offenders}\"\n    echo \"Commit blocked. Convert to .hy or place generated code under dist/.\"\n    exit 1\n  fi\n  ```\n\n  * [ ] Add `scripts/dev/install-hooks.sh`:\n\n    ```bash\n    git config core.hooksPath .githooks\n    chmod +x .githooks/pre-commit\n    ```\n  * [ ] CI job step that **fails** if any tracked `.py` exists outside `dist/`:\n\n    ```bash\n    rg -n --glob '!dist/**' --glob '!**/*.pyc' --glob '!**/__pycache__' '\\.py$' . \\\n      | grep -v '^dist/' && { echo \"Python source detected\"; exit 1; } || true\n    ```\n\n### 1) Hy Tooling + Build Skeleton\n\n* [ ] Add Hy runtime to **`pyproject.toml`** (kept because we still run on CPython):\n\n  ```toml\n  [project]\n  name = \"promethean\"\n  requires-python = \">=3.11\"\n  dependencies = [\"hy>=1.0.0\"]  # pin exact later per versioning policy\n  ```\n\n* [ ] Build script: **compile Hy ‚Üí Python files** into `./dist`. Prefer `hy2py` for readable `.py`.\n  `tools/build.hy` (invoked by Makefile/uv) should:\n\n  * [ ] Walk **Hy source roots** (see next task).\n  * [ ] For each `.hy`, run `hy2py` and write path-mirrored `.py` into `./dist`.\n  * [ ] Emit `__init__.py` in each `dist` package dir for importability.\n  * [ ] Preserve package namespace (e.g., `services/py/tts/foo.hy` ‚Üí `dist/services/py/tts/foo.py`).\n\n* [ ] Make targets:\n\n  ```Makefile\n  .PHONY: hy-build hy-clean\n  hy-build:\n  \tuv run hy tools/build.hy\n\n  hy-clean:\n  \trm -rf dist\n  ```\n\n  (Per your workflow: Makefile shells out to `hy tools/build.hy`.)\n\n### 2) Source Layout & Conventions\n\n* [ ] Create Hy roots mirroring TS/Sibilant style:\n\n  ```\n  shared/hy/            # shared libs (analogue to @shared/ts)\n  services/hy/<svc>/    # service-specific Hy\n  tools/hy/             # build/dev scripts in Hy\n  ```\n* [ ] **Import discipline:** Python modules **must not exist** under source; only `.hy`.\n* [ ] Generated `.py` lives only under `dist/` with identical package layout.\n\n### 3) Entrypoints & Runtime\n\n* [ ] For dev: allow `uv run hy -m services.hy.cephalon.main` style commands.\n* [ ] For prod/CI: run compiled entrypoints from `./dist`, e.g.:\n\n  ```bash\n  uv run python -m services.hy.cephalon.main  # with PYTHONPATH=dist\n  ```\n\n  * [ ] Add `export PYTHONPATH=dist:${PYTHONPATH}` to service runners/PM2 env.\n  * [ ] Replace any old `python path/to/*.py` with **Hy or dist imports**.\n\n### 4) Port Python ‚Üí Hy (incremental, API-stable)\n\n* [ ] **Inventory** all Python modules/services and their public APIs.\n* [ ] Port order (highest leverage first):\n\n  * [ ] `services/py/stt` (if present), `tts`, any model handlers\n  * [ ] `shared/py/**` utility libraries\n  * [ ] Remaining Python scripts under `tools/`\n* [ ] For each module:\n\n  * [ ] Re-implement in Hy with **same function/class names**, same inputs/outputs.\n  * [ ] Delete original `.py` (or move aside), compile `.hy` ‚Üí `dist/*.py`.\n  * [ ] Unit tests updated to import via package name (works for both dev/prod).\n  * [ ] No direct path imports to `.py` anywhere.\n\n### 5) Testing & CI\n\n* [ ] Update test bootstrap to set `PYTHONPATH=dist`.\n* [ ] Add a **pre-test build** step:\n\n  ```bash\n  make hy-build\n  uv run pytest -q\n  ```\n* [ ] CI pipeline:\n\n  * [ ] `make hy-build`\n  * [ ] Verify **no `.py` outside `dist/`** (same grep as in guardrail).\n  * [ ] Run tests.\n  * [ ] (Optional) publish `dist` as artifact for downstream jobs.\n\n### 6) Documentation (repeat it loudly)\n\n* [ ] `README.md`: **‚ÄúPython source is forbidden. Use Hy.‚Äù**\n* [ ] `CONTRIBUTING.md`: coding standard, examples, build, and the pre-commit rule.\n* [ ] `docs/architecture/languages.md`: why Hy, how to import, where compiled code lives.\n* [ ] Service READMEs: update run commands (dev via `hy`, prod via `dist`).\n* [ ] Codeowners/review checklist: reject PRs with `.py` outside `dist`.\n\n### 7) Linters/Formatters\n\n* [ ] Remove Python linters that parse `.py` sources in the tree.\n* [ ] Add Hy formatting guidance (Hy doesn‚Äôt have a de facto formatter; keep style doc simple).\n* [ ] Keep Ruff/Black **off** the repo or scope them to `dist/` only if needed (read-only).\n\n### 8) Final Sweep\n\n* [ ] Repo-wide search for `.py` references in scripts, docs, Dockerfiles.\n* [ ] Ensure Docker images copy **Hy sources** then build, or copy **`dist/`** if using multi-stage.\n\n---\n\n## Acceptance Criteria (checkable)\n\n* [ ] Running `rg -n '\\.py$' --glob '!dist/**'` returns **no results**.\n* [ ] `make hy-build` regenerates `./dist` with importable packages and `__init__.py` files.\n* [ ] All Python-runtime services start via **Hy or compiled outputs** with no `.py` under source.\n* [ ] Pre-commit hook blocks & un-stages `.py` placed anywhere but `dist`.\n* [ ] CI fails on any tracked `.py` outside `dist`.\n* [ ] Documentation updated in **at least** README + CONTRIBUTING + languages page.\n* [ ] At least **two** previously-Python services fully ported to Hy and green in CI.\n\n---\n\n## Step-by-Step Milestone Checklist (you asked for ‚ÄúStep 1..4‚Äù)\n\n* [ ] **Step 1 ‚Äî Guardrails:** add `.gitignore`, pre-commit hook, CI check; set `PYTHONPATH=dist`.\n* [ ] **Step 2 ‚Äî Build:** implement `tools/build.hy` + `make hy-build`; lay out `shared/hy`, `services/hy/*`.\n* [ ] **Step 3 ‚Äî Ports:** migrate highest-value Python modules to `.hy`; remove originals; fix tests.\n* [ ] **Step 4 ‚Äî Docs/Enforce:** update docs; remove Python linters; repo-wide sweep; close out.\n\n---\n\n## Relevant Resources\n\n* You might find \\[this] useful while working on this task\n  *(link your internal Hy style guide or a reference note with macros/patterns you like; keep it local to the vault/repo)*\n\n---\n\n## Comments\n\nAppend-only thread for agents. Note blockers, weird Hy interop, or macro decisions here.\n\n---\n\n## Notes / Gotchas\n\n* **hy2py limitations:** Generated Python may differ stylistically; that‚Äôs fine‚Äîit‚Äôs an artifact. Source of truth is `.hy`.\n* **Dynamic imports:** If any code used `importlib` tricks, you‚Äôll need to replicate behavior in Hy or adjust the API.\n* **Packaging:** If you distribute wheels later, you can package from `dist/` while keeping the repo source pure Hy.\n* **Performance:** Hy compiles to Python AST; runtime perf should be on par with equivalent Python once compiled.\n\n\\#tags #promethean #hy #lisp #python #build #ci #git #precommit #policy #docs\n\n#archive\n\n"}
{"id":"9926dde4-990e-4503-a057-fcae2e5bf1b1","title":"Embedding service sometimes disconnects from broker and hangs ‚Üí detect, shed, kill, recover","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.518Z","uuid":"9926dde4-990e-4503-a057-fcae2e5bf1b1","created_at":"2025-09-15T02:02:58.518Z","path":"docs/agile/tasks/periodicly-the-embedding-service-will-get-disconnected-from-the-broker-and-not-die--blocking-other-processes-who-require-embeddings.-incoming.md","content":"Here‚Äôs a surgical expansion you can drop into the board. Goal: make the **embedding service** fail fast, get killed when it‚Äôs unhealthy, and stop blocking dependents. No vibes‚Äîjust guards, telemetry, and hard interlocks.\n\n# Embedding service sometimes disconnects from broker and hangs ‚Üí detect, shed, kill, recover\n\n**Owner:** Codex / Agent\n**Status:** Planned\n**Labels:** #embeddings #broker #heartbeat #healthchecks #overload #resilience #promethean\n\n---\n\n## üõ†Ô∏è Problem recap (plain)\n\nPeriodically the **embedding** service loses its broker connection and doesn‚Äôt die. It keeps the process ‚Äúalive‚Äù enough to dodge current liveness checks, which **blocks requesters** that require embeddings. You suspect overload. If the heartbeat service worked end-to-end, it should have killed it; it didn‚Äôt.\n\n---\n\n## üéØ Outcomes\n\n* The embedding service **refuses work** under overload (admission control) instead of zombifying.\n* When **broker connectivity is lost** or the **work loop stalls**, the service **exits** (SIGTERM ‚Üí SIGKILL) quickly.\n* The **heartbeat** reliably detects unresponsive or disconnected state and kills the process within the configured window.\n* We have **metrics, logs, and repro** (chaos test) that prove this.\n\n---\n\n## üì¶ Requirements / Definition of Done (non-negotiable)\n\n* [ ] **Explicit liveness** and **readiness** semantics:\n\n  * Liveness: event loop + worker pool making forward progress in the last `LIVENESS_MAX_STALL_MS`.\n  * Readiness: **broker connected** AND **model ready** AND **queue < MAX\\_QUEUE**.\n* [ ] **Broker-aware watchdog**: if `broker.disconnected_for > DISCONNECT_KILL_MS` ‚Üí `process.exit(70)` (distinct code).\n* [ ] **Admission control**: reject new jobs once `inflight + queue ‚â• MAX_QUEUE`, emit backpressure signal (+ metrics).\n* [ ] **Heartbeat** enriched:\n\n  * Sends `connected: true|false`, `queue_depth`, `inflight`, `last_job_latency_ms`, `last_progress_ts`.\n  * Server enforces kill if `now - last_progress_ts > HEARTBEAT_TIMEOUT` OR `connected=false for > DISCONNECT_KILL_MS`.\n* [ ] **Graceful shutdown**: stop intake, drain for `DRAIN_MS`, then hard exit if still busy.\n* [ ] **Chaos test** that severs broker and floods load ‚Üí process is killed/restarted; dependents observe fail-fast (not hang).\n* [ ] **Alarms** on repeating death loops (>N restarts / 10 min) with log hint to raise capacity or lower concurrency.\n* [ ] **Docs** with clear thresholds and how to tune them.\n\n---\n\n## üìã Tasks\n\n### Step 1 ‚Äî Instrument & define health truth\n\n* [ ] Add **progress ticker** in the embed loop: update `last_progress_ts` whenever a batch completes (success or error).\n* [ ] Implement **readiness** and **liveness** endpoints (HTTP or broker health topic):\n\n  * `/health/ready` returns 200 only if `{ broker: connected, model: ready, queue < MAX_QUEUE }`.\n  * `/health/live` returns 200 only if `now - last_progress_ts < LIVENESS_MAX_STALL_MS`.\n* [ ] Extend **HeartbeatClient** payload (you already have it) to include:\n\n  ```json\n  {\n    \"pid\": 1234,\n    \"name\": \"embeddings\",\n    \"connected\": true,\n    \"queue\": 42,\n    \"inflight\": 8,\n    \"last_progress_ts\": 1724612345678,\n    \"avg_job_ms\": 37\n  }\n  ```\n* [ ] Update heartbeat server policy:\n\n  * If `connected=false` continuously for `DISCONNECT_KILL_MS`, kill.\n  * If `now - last_progress_ts > HEARTBEAT_TIMEOUT`, kill.\n  * Log structured reason: `reason=\"no_progress\"` or `reason=\"broker_disconnected\"`.\n\n### Step 2 ‚Äî Fail fast: admission control & overload hygiene\n\n* [ ] Add **MAX\\_QUEUE** and **MAX\\_INFLIGHT** envs with sane defaults (and per-model overrides).\n* [ ] On intake:\n\n  * If `queue ‚â• MAX_QUEUE` ‚Üí **reject immediately** with `overloaded=true`, `retry_after_ms`.\n  * Publish **backpressure** metric and broker signal (bridges can shed load earlier).\n* [ ] Implement **work-budget** per request (deadline in ms). If budget exceeded while waiting ‚Üí cancel.\n* [ ] Add **latency guard**: if `avg_job_ms √ó inflight > SLAM_LIMIT_MS` ‚Üí temporarily pause intake.\n\n### Step 3 ‚Äî Watchdog & exit semantics\n\n* [ ] Add a small **watchdog** in the embedding service (same process, separate timer):\n\n  * Checks every `WATCHDOG_INTERVAL_MS`:\n\n    * If broker state `disconnected` for `> DISCONNECT_KILL_MS` ‚Üí `process.exit(70)`.\n    * If `now - last_progress_ts > LIVENESS_MAX_STALL_MS` ‚Üí `process.exit(71)`.\n* [ ] Ensure **SIGTERM** handler: stop intake, set `readiness=false`, drain up to `DRAIN_MS`, then `process.exit(0)`.\n* [ ] PM2 / supervisor: restart on exit codes 70/71; **alert** if restart count threshold exceeded.\n\n### Step 4 ‚Äî Reproduce, test, and document\n\n* [ ] **Chaos script** to simulate:\n\n  1. broker disconnect (kill socket / firewall rule),\n  2. load spike (burst 10√ó queue),\n  3. slow model (inject sleep).\n     Expect: fast rejection, watchdog exit, heartbeat kill if watchdog disabled.\n* [ ] **Parity tests** for fail-fast semantics: callers get `429`/`overloaded` vs hanging.\n* [ ] Dashboards: queue depth, inflight, job latency, disconnect duration, kill reasons.\n* [ ] Docs: tuning guide (`MAX_QUEUE`, `MAX_INFLIGHT`, `HEARTBEAT_TIMEOUT`, `DISCONNECT_KILL_MS`, `DRAIN_MS`), and the ‚ÄúWhy we exit on disconnect‚Äù note.\n\n---\n\n## üîß Config (envs)\n\n```\nEMBED_MAX_INFLIGHT=8\nEMBED_MAX_QUEUE=256\nEMBED_LIVENESS_MAX_STALL_MS=30000\nEMBED_DISCONNECT_KILL_MS=10000\nEMBED_WATCHDOG_INTERVAL_MS=1000\nEMBED_DRAIN_MS=5000\nHEARTBEAT_TIMEOUT=45000\n```\n\n---\n\n## üß© Code sketches (tight & minimal)\n\n**Admission control**\n\n```ts\nfunction admit(): boolean {\n  return queue.length + inflight < EMBED_MAX_QUEUE;\n}\n\nfunction enqueue(job) {\n  if (!admit()) return failFast(job, { overloaded: true, retry_after_ms: 250 });\n  queue.push(job);\n}\n```\n\n**Progress + watchdog**\n\n```ts\nlet lastProgress = Date.now();\nbroker.on('connected', () => state.connected = true);\nbroker.on('disconnected', () => state.connected = false);\n\nasync function workLoop() {\n  while (running) {\n    const job = queue.shift();\n    if (!job) { await sleep(5); continue; }\n    inflight++;\n    try {\n      await handle(job);\n    } finally {\n      inflight--;\n      lastProgress = Date.now();\n    }\n  }\n}\n\nsetInterval(() => {\n  const now = Date.now();\n  if (!state.connected && now - state.lastDisconnectAt > EMBED_DISCONNECT_KILL_MS) {\n    log.error('watchdog', { reason: 'broker_disconnected' }); process.exit(70);\n  }\n  if (now - lastProgress > EMBED_LIVENESS_MAX_STALL_MS) {\n    log.error('watchdog', { reason: 'no_progress' }); process.exit(71);\n  }\n}, EMBED_WATCHDOG_INTERVAL_MS);\n```\n\n**Heartbeat payload**\n\n```ts\nheartbeatClient.send({\n  pid: process.pid, name: 'embeddings',\n  connected: state.connected, queue: queue.length, inflight,\n  last_progress_ts: lastProgress, avg_job_ms: metrics.avgJobMs()\n});\n```\n\n**Graceful shutdown**\n\n```ts\nprocess.on('SIGTERM', async () => {\n  ready = false; running = false; const t0 = Date.now();\n  while (inflight > 0 && Date.now() - t0 < EMBED_DRAIN_MS) await sleep(50);\n  process.exit(0);\n});\n```\n\n---\n\n## ‚úÖ Acceptance Criteria\n\n* [ ] Under forced broker disconnect, embedding service exits within **‚â§ EMBED\\_DISCONNECT\\_KILL\\_MS + WATCHDOG\\_INTERVAL\\_MS** (watchdog path).\n* [ ] If progress stalls **> EMBED\\_LIVENESS\\_MAX\\_STALL\\_MS**, service exits; heartbeat can also kill if watchdog disabled.\n* [ ] When **MAX\\_QUEUE** is reached, new requests are **rejected immediately** (no hang), and dependents observe `overloaded` with `retry_after_ms`.\n* [ ] Heartbeat dashboard shows `connected`, `queue`, `inflight`, `last_progress_ts`; kills occur when policies fire; kill reason logged.\n* [ ] Chaos test proves: no zombie state; dependents recover; system keeps breathing.\n* [ ] Docs present with tuning guidance and thresholds; PM2/supervisor configured to restart and alert on loops.\n\n---\n\n## üìÇ Proposed Files/Paths\n\n* `services/ts/embeddings/worker.ts` ‚Äî admission control + progress ticker\n* `services/ts/embeddings/watchdog.ts` ‚Äî broker/liveness watchdog\n* `services/ts/embeddings/health.ts` ‚Äî live/ready endpoints or broker health topic\n* `shared/ts/src/heartbeat/client.ts` ‚Äî payload extended (connected/queue/inflight/last\\_progress\\_ts)\n* `services/ts/heartbeat/server.ts` ‚Äî kill policies for new fields\n* `tests/chaos/embeddings.disconnect.spec.ts` ‚Äî disconnect/overload chaos tests\n* `docs/runbooks/embeddings-health.md` ‚Äî tuning, failure modes, ops actions\n\n---\n\n## üîç Debug checklist (use before/after)\n\n* Broker logs show **disconnect** ‚Üí embedding logs show **watchdog exit** within bound.\n* Heartbeat shows **stalled progress** ‚Üí kill ‚Üí supervisor restart.\n* Dependents receive **fail-fast** responses, not timeouts (inspect bridge logs).\n* No request spends > budgeted time in queue; queues drop after kill/restart.\n\n---\n\n## üßØ Pitfalls & guardrails\n\n* Don‚Äôt rely only on **socket events**‚ÄîTLS stalls can leave a half-open connection; the **progress ticker** is your truth.\n* **Slow model** ‚â† dead process: measure *progress*, not just *connection*.\n* Avoid ‚Äúgraceful drain forever‚Äù: enforce **hard DRAIN\\_MS cap**.\n* Watch for **restart storms**: cap concurrency and consider exponential backoff on reconnect.\n\n---\n\n## Step 1‚Äì4 (your minimal milestone format)\n\n* [ ] **Step 1 ‚Äî Health Semantics:** implement live/ready; progress ticker; heartbeat payloads; server kill rules.\n* [ ] **Step 2 ‚Äî Admission Control:** MAX\\_QUEUE/MAX\\_INFLIGHT + fail-fast + deadlines.\n* [ ] **Step 3 ‚Äî Watchdog & Shutdown:** broker/liveness watchdog; graceful drain; supervised restarts; alerts.\n* [ ] **Step 4 ‚Äî Chaos & Docs:** disconnect + overload chaos tests; dashboards; runbook; finalize thresholds.\n\n---\n\n## Commit template\n\n```\nfix(embeddings): kill-on-disconnect/stall + admission control + heartbeat kill policies\n\n- Add progress-based liveness and broker-aware readiness\n- Enforce MAX_QUEUE/MAX_INFLIGHT with fail-fast overload responses\n- Watchdog exits on broker disconnect or no-progress conditions\n- Heartbeat payloads & server policies updated; kill reasons logged\n- Chaos tests for disconnect/overload; docs & tuning guide\n```\n\n\\#tags #embeddings #reliability #heartbeat #watchdog #overload #backpressure #chaos #pm2 #broker #promethean\n\n## Notes\n- Tests or documentation are missing; acceptance criteria not fully met.\n- Story Points: 5\n\n#in-progress\n\n"}
{"id":"9a9e9c97-fbaa-4580-90ac-2d1f17eebbec","title":"redefine all existing lambdas with high order functions incoming","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.518Z","uuid":"9a9e9c97-fbaa-4580-90ac-2d1f17eebbec","created_at":"2025-09-15T02:02:58.518Z","path":"docs/agile/tasks/redefine-all-existing-lambdas-with-high-order-functions-incoming.md","content":"## üõ†Ô∏è Description\n\nReplace ad‚Äëhoc anonymous lambdas with well‚Äënamed higher‚Äëorder functions to clarify intent and improve reuse.\n\n---\n\n## üéØ Goals\n\n- Reduce ambiguous inline lambdas throughout the codebase\n- Establish reusable helpers that capture shared closure logic\n\n---\n\n## üì¶ Requirements\n\n- [ ] Each former lambda extracted into a named function or higher‚Äëorder wrapper\n- [ ] Added tests cover behavior of new helpers\n\n---\n\n## üìã Subtasks\n\n- [ ] Audit modules for deeply nested lambdas\n- [ ] Refactor candidate lambdas into named functions\n- [ ] Create higher‚Äëorder utilities where closures are required\n- [ ] Update references and run tests\n\n---\n## üßÆ Story Points\n\n5\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#framework-core #Ready\n\n\n"}
{"id":"9b3f1c89-9a76-4f18-92a4-38275b1bc1f0","title":"Setup MCP server for pnpm workspace management","status":"in_progress","priority":"P2","owner":"","labels":[],"created":"2025-03-09T00:00:00.000Z","uuid":"9b3f1c89-9a76-4f18-92a4-38275b1bc1f0","created_at":"2025-03-09T00:00:00.000Z","path":"docs/agile/tasks/setup-mcp-pnpm-ops.md","content":"#In-Progress\n\n## üõ†Ô∏è Description\n\n- Provide an MCP server that can install, add, and remove pnpm dependencies at the workspace root or scoped to a package filter, plus expose pnpm script execution.\n\n## Description\n- **What changed?** New requirement to automate pnpm dependency and script operations via MCP so agents can manage workspace packages.\n- **Where is the impact?** Applies to dev tooling; new server likely lives under `packages/` and touches workspace config.\n- **Why now?** Enables remote automation and scripted dependency updates for MCP clients.\n- **Supporting context** User request in current session.\n\n## Goals\n- MCP server exposes tools for `pnpm install`, `pnpm add`, `pnpm remove`, and script execution with optional `--filter` package targeting.\n- Document usage and constraints so future agents can extend capabilities safely.\n\n## Requirements\n- [ ] pnpm operations succeed for workspace root and filtered packages.\n- [ ] Tests or smoke checks cover command invocation helpers.\n- [ ] Documentation updated with usage details.\n- [ ] PR merged: (link once created)\n- [ ] Additional constraints or non-functional requirements are addressed: support GPL-3.0-only licensing and follow repo lint/build gates.\n\n## Subtasks\n1. Review existing MCP tooling packages and decide placement for new server.\n2. Implement pnpm command runner with workspace vs filter support.\n3. Register MCP tools/resources and provide docs/tests.\n\nEstimate: 5\n\n---\n\n## üîó Related Epics\n\n- [[kanban]]\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- None\n\n## ‚õìÔ∏è Blocks\n\n- None\n\n---\n\n## üîç Relevant Links\n\n- User request (current session)\n"}
{"id":"9c1a2f5c-a7e2-4f4a-bb3d-2f8a7d54f6f1","title":"DUCK ‚Äî feature flags rewrite (web + node) + docs","status":"in_progress","priority":"P1","owner":"","labels":["duck","web","node","flags","docs"],"created":"2025-10-02T00:00:00.000Z","uuid":"9c1a2f5c-a7e2-4f4a-bb3d-2f8a7d54f6f1","created_at":"2025-10-02T00:00:00.000Z","path":"docs/agile/tasks/duck-feature-flags-rewrite-web-node-docs.md","content":"#Todo\n\n## üõ†Ô∏è Description\nRewrite broken feature flag modules for duck-web and duck-utils; pure ESM TS, functional, side-effect free. Align env names; fix `FEATURE_FLAGS.md`.\n\n## Goals\n- Replace `apps/duck-web/src/flags.ts` with pure helpers (`parseBool`, constants)\n- Add `packages/duck-utils/src/flags.ts` for Node path\n- Fix docs `docs/duck/FEATURE_FLAGS.md` (names/casing, examples)\n\n## Requirements\n- [ ] Unit tests (ava) cover true/false parsing + defaults\n- [ ] Web: `import.meta.env.VITE_*` inputs only\n- [ ] Node: `process.env.*` inputs only\n- [ ] No side effects at import time\n\n## Subtasks\n1. Implement helpers and constants\n2. Wire flags in consumers (duck-web, cephalon where relevant)\n3. Update docs\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- None\n\n## ‚õìÔ∏è Blocks\n\n- Merge PR #1447\n\n---\n\n## üîç Relevant Links\n\n- PR #1447\n"}
{"id":"9c9a9289-e420-403e-9772-71fc3d7e128f","title":"make discord channel aware contextualizer md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.517Z","uuid":"9c9a9289-e420-403e-9772-71fc3d7e128f","created_at":"2025-09-15T02:02:58.517Z","path":"docs/agile/tasks/make_discord_channel_aware_contextualizer.md","content":"## üõ†Ô∏è Description\n\nSet up a mechanism to define new context creations based specifically off a discord channel with custom rules and prompting logic\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Add Ollama formally to pipeline\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n\n#archive\n\n"}
{"id":"9eed3915-19f2-4006-943c-84b274bc7f47","title":"scripts add folder readmes and usage","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.519Z","uuid":"9eed3915-19f2-4006-943c-84b274bc7f47","created_at":"2025-09-15T02:02:58.519Z","path":"docs/agile/tasks/scripts_add_folder_readmes_and_usage.md","content":"Scripts: Add READMEs and usage docs per folder\n\nGoal: Ensure each scripts subfolder has a README with purpose, dependencies, environment, and examples.\n\nScope:\n- Create READMEs for:\n  - scripts/docs/\n  - scripts/audio/\n  - scripts/indexing/\n  - scripts/misc/ (if created)\n- Link from scripts/README.md to each subfolder.\n\nExit Criteria:\n- Each folder has a concise README with clear commands and prerequisites.\n\n#incoming #scripts #docs\n\n\n"}
{"id":"9f621ec3-91d2-42cf-a8af-b1eafe7c4041","title":"review pipeline documentation in README","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-18T19:28:54Z","uuid":"9f621ec3-91d2-42cf-a8af-b1eafe7c4041","created_at":"2025-09-18T19:28:54Z","path":"docs/agile/tasks/Review pipeline documentation.md","content":"## üõ†Ô∏è Task: Review pipeline documentation in README\n\n### Context\n- The root README now lists every pipeline defined in `pipelines.json` with step summaries.\n- Future changes to `pipelines.json` need manual confirmation so the README stays accurate.\n\n### Definition of Done\n- [ ] Compare the README pipeline section against the current `pipelines.json` contents.\n- [ ] Update descriptions, environment notes, or step listings if the pipeline config changes.\n- [ ] Record any follow-up issues that arise from mismatches.\n\n### Suggested Plan\n1. Diff `pipelines.json` against the last documented state.\n2. Regenerate or adjust the README summaries as needed.\n3. Flag large behavioural changes to the automation owner.\n\n### References\n- `README.md` (Automation pipelines section)\n- `pipelines.json`\n"}
{"id":"9fbe9f3a-0c6a-472b-8f7c-b100dab6f5de","title":"replace agent automation makefile targets with pnpm scripts","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.523Z","uuid":"9fbe9f3a-0c6a-472b-8f7c-b100dab6f5de","created_at":"2025-09-15T02:02:58.523Z","path":"docs/agile/tasks/update_makefile_to_have_commands_specific_for_agents.md","content":"## üõ†Ô∏è Task: Replace Makefile agent commands with pnpm scripts\n\nThe earlier plan called for agent-specific Makefile targets such as `make start:duck`. The audit showed those never shipped,\nand the active tooling already relies on pnpm workspaces (`scripts/dev.mjs`, `pnpm --filter ‚Ä¶`). Rather than reviving the\nMakefile, we will publish pnpm scripts that mirror the desired ergonomics.\n\n## ‚úÖ Decision\n- Standardize on pnpm workspace scripts and `scripts/dev.mjs` for agent lifecycle management.\n- Remove references to `%`-style Makefile dispatch and instead document `pnpm --filter @promethean/<agent> dev|test` patterns.\n\n## üîç Findings from audit\n- No maintained agent workflows depend on `Makefile`/`Makefile.hy`.\n- Existing developers start services via `pnpm dev:all` (scripts/dev.mjs) or direct package scripts.\n- Documentation still directs contributors to non-existent Makefile targets.\n\n---\n\n## üéØ Goals\n- Provide agent-scoped start/test commands via pnpm scripts.\n- Document how to launch an individual agent without Makefile wrappers.\n- Supply a concrete example (e.g., Duck) showing the new commands.\n\n---\n\n## üì¶ Requirements\n- [ ] Add workspace scripts or README snippets mapping agents to `pnpm --filter` commands.\n- [ ] Ensure `scripts/dev.mjs` coverage is documented for multi-agent dev loops.\n- [ ] Update root onboarding docs to drop Makefile references.\n\n---\n\n## üìã Subtasks\n- [ ] Inventory existing agent packages and confirm they expose `dev`/`test` scripts.\n- [ ] Publish documented aliases (e.g., `pnpm agent:duck:dev`) if gaps exist.\n- [ ] Update README / docs to reflect pnpm usage only.\n\n---\n\n## üîó Related Epics\n#devops\n\n---\n\n## ‚õìÔ∏è Blocked By\n- [determine pm2 configuration for agents](Determine%20PM2%20configuration%20for%20agents.md)\n\n## ‚õìÔ∏è Blocks\nNothing\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n- [MIGRATION_PLAN](../MIGRATION_PLAN.md)\n- [[promethean-dev-workflow-update|promethean dev workflow update]]\n\n#devops #todo\n"}
{"id":"a069b185-7e7f-46f8-b393-a205f06f080a","title":"run model bakeoff md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.519Z","uuid":"a069b185-7e7f-46f8-b393-a205f06f080a","created_at":"2025-09-15T02:02:58.519Z","path":"docs/agile/tasks/run_model_bakeoff.md","content":"## üõ†Ô∏è Task: Run model bakeoff\n\nExecute a comparative evaluation between current and candidate models using a fixed set of tasks.\n\n---\n\n## üéØ Goals\n- Measure accuracy and latency differences\n- Decide whether to adopt, pin, or sandbox new models\n\n---\n\n## üì¶ Requirements\n- [ ] Define representative benchmark tasks\n- [ ] Freeze inputs and prompts\n- [ ] Collect metrics for each model\n\n---\n\n## ‚úÖ Acceptance Criteria\n- Bakeoff script runs and outputs comparison metrics.\n- Results highlight which model meets adoption thresholds.\n- Summary of findings is documented.\n\n---\n\n## üìã Subtasks\n- [ ] Script test harness\n- [ ] Summarize results\n\n---\n\n## üîó Related Epics\n#ops #codex-task\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\nNothing\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n#IceBox\n#ready\n\n"}
{"id":"a08e1032-c149-4060-831d-97e43d707132","title":"evaluate ollama openvino","status":"todo","priority":"p3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"a08e1032-c149-4060-831d-97e43d707132","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/evaluate-ollama-openvino.md","content":"### **Smart ChatGPT Configuration**\n```smart-chatgpt\n```\n**<hr class=\"__chatgpt_plugin\">**\n\n**<span style=\"font-size: small;\"> (llama3.2:latest)</span>**\n\n### **role::user**\n**Using this title write a reasonable task document:evaluate-ollama-openvino**\n\n\n**<hr class=\"__chatgpt_plugin\">**\n\n### **role::assistant<span style=\"font-size: small;\"> (llama3.2:latest)</span>**\n\n**Task Document**\n================\n\n**Title: Evaluate Ollama OpenVINO**\n\n**Objective:**\n----------------\n\n**Evaluate the Ollama framework and its integration with OpenVINO for deep learning inference on Edge AI devices.**\n\n**Scope:**\n-------------\n\n* **Assess the performance of Ollama on various Edge AI hardware platforms (e.g., Intel Nervana, Google Coral)**\n* **Compare Ollama's performance with existing Edge AI frameworks (e.g., TensorFlow Lite, OpenCV DNN)**\n* **Identify potential use cases for Ollama in Edge AI applications**\n* **Determine the feasibility of integrating Ollama with OpenVINO for optimized inference**\n\n**Deliverables:**\n-----------------\n\n1. **Evaluation Report: A written report detailing the evaluation process, results, and conclusions.**\n2. **Comparison Table: A table summarizing the performance comparisons between Ollama and existing Edge AI frameworks.**\n\n**Evaluation Criteria:**\n------------------------\n\n1. **Performance Metrics: Assess the accuracy, latency, and power consumption of Ollama on different hardware platforms.**\n2. **Inference Speed: Evaluate the speed of inference on various models using Ollama.**\n3. **Energy Efficiency: Compare the energy efficiency of Ollama with existing Edge AI frameworks.**\n4. **Integration with OpenVINO: Assess the feasibility of integrating Ollama with OpenVINO for optimized inference.**\n\n**Timeline:**\n--------------\n\n* **Research and Planning: 1 day**\n* **Evaluation and Testing: 3 days**\n* **Writing Report and Creating Comparison Table: 2 days**\n\n**Resources:**\n-----------------\n\n* **Access to Edge AI hardware platforms (e.g., Intel Nervana, Google Coral)**\n* **OpenVINO SDK**\n* **Ollama framework source code**\n* **Relevant research papers and documentation**\n\n**Assumptions and Dependencies:**\n-----------------------------------\n\n* **The evaluator has experience with deep learning frameworks and Edge AI applications.**\n* **The evaluator has access to necessary hardware and software resources.**\n\n**By following this task document, you should be able to evaluate the Ollama framework and its integration with OpenVINO for deep learning inference on Edge AI devices.**\n\n### **Related notes**\n```smart-connections\n{\n  \"render_markdown\": true,\n  \"show_full_path\": false,\n  \"exclude_blocks_from_source_connections\": false,\n  \"exclude_frontmatter_blocks\": true,\n  \"expanded_view\": false,\n  \"results_limit\": \"20\",\n  \"exclude_inlinks\": false,\n  \"exclude_outlinks\": false\n}\n```\n\n\n"}
{"id":"a224ab7a-70fd-4711-b499-a072c8037960","title":"pin versions in configs promethean codex md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.518Z","uuid":"a224ab7a-70fd-4711-b499-a072c8037960","created_at":"2025-09-15T02:02:58.518Z","path":"docs/agile/tasks/pin_versions_in_configs_promethean_codex.md","content":"## üõ†Ô∏è Task: Pin versions in configs (Promethean + Codex)\n\nGenerated from [../unique/2025.08.08.16.08.27.md](../unique/2025.08.08.16.08.27.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n\n#archive\n\n"}
{"id":"a31bb3f1-3b68-483a-8280-e78386abf03b","title":"cephalon backfill conversation history","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.509Z","uuid":"a31bb3f1-3b68-483a-8280-e78386abf03b","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/cephalon_backfill_conversation_history.md","content":"Cephalon: Backfill conversation history (optional)\n\nGoal: Migrate recent logs/outputs into `transcripts` and `agent_messages` to seed context for active users.\n\nWhy: After restoring persistence, agents may still loop until enough new context accumulates.\n\nScope:\n- Define a simple script to scan recent Discord messages and TTS logs (if available) to backfill collections.\n- Idempotent: avoid duplicates by keying on timestamps and message IDs.\n\nExit Criteria:\n- Backfill script present in `services/ts/cephalon/scripts/` and documented; safe to run.\n\n#incoming #cephalon #backfill #context\n\n"}
{"id":"a3dab29a-a50f-489e-8bcd-414ee6a60e4b","title":"bb tool chain","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"a3dab29a-a50f-489e-8bcd-414ee6a60e4b","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/bb-tool-chain.md","content":"#incoming\n\n"}
{"id":"a470c992-509f-48f8-9d6b-cfb6285dc67c","title":"dockerize the system","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.511Z","uuid":"a470c992-509f-48f8-9d6b-cfb6285dc67c","created_at":"2025-09-15T02:02:58.511Z","path":"docs/agile/tasks/dockerize-the-system.md","content":"## üõ†Ô∏è Description\n\nProvide Docker images and compose configuration so contributors can run the system and its services in containers for testing and deployment.\n\n---\n\n## üéØ Goals\n\n- Package core services into reproducible containers\n- Simplify local setup and pave way for hosted deployment\n\n---\n\n## üì¶ Requirements\n\n- [ ] Dockerfiles for major services and dependencies\n- [ ] `docker-compose` (or similar) to orchestrate stack\n- [ ] Documentation for building and running images\n\n---\n\n## üìã Subtasks\n\n- [ ] Create base image with shared dependencies\n- [ ] Author service-specific Dockerfiles\n- [ ] Write compose file for local development\n- [ ] Test containerized workflow end-to-end\n\n---\n\n## üîó Related Epics\n\n#devops\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#devops #Ready\n\n\n"}
{"id":"a5748afa-3e7b-49fc-b945-d0452d2adf76","title":"lisp package files","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.516Z","uuid":"a5748afa-3e7b-49fc-b945-d0452d2adf76","created_at":"2025-09-15T02:02:58.516Z","path":"docs/agile/tasks/lisp-package-files.md","content":"## üõ†Ô∏è Description\n\nDesign a package file format for the Lisp components so modules can declare dependencies and be imported consistently.\n\n---\n\n## üéØ Goals\n\n- Enable reusable Lisp packages within the monorepo\n- Provide metadata for versioning and dependency resolution\n\n---\n\n## üì¶ Requirements\n\n- [ ] Define package manifest schema\n- [ ] Loader resolves dependencies and loads modules\n- [ ] Unit tests verify package discovery and import\n\n---\n\n## üìã Subtasks\n\n- [ ] Draft manifest fields (name, version, deps)\n- [ ] Implement loader in Lisp compiler/runtime\n- [ ] Convert existing modules to new package format\n- [ ] Document package usage\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#framework-core #Ready\n\n\n"}
{"id":"a5b35121-160a-496a-ac7b-94e405294888","title":"create permission gating layer","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"a5b35121-160a-496a-ac7b-94e405294888","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/create_permission_gating_layer.md","content":"## üõ†Ô∏è Task: Create permission gating layer\n\nIntroduce a middleware layer that checks whether an action or\ninformation request is allowed before it reaches core services. This\nis based on the \"Dorian Permission Gate\" equations in our math notes.\n\n---\n\n## üéØ Goals\n\n- Prevent unauthorized commands from propagating through the system\n- Allow per-agent rule sets and default fallbacks\n- Log gate denials for auditing\n\n---\n\n## üì¶ Requirements\n\n- [ ] Implement gate logic as a Python module (`shared/py/permission_gate.py`)\n- [ ] Support weight/threshold config via YAML\n- [ ] Expose a simple `check_permission(agent, action)` API\n- [ ] Document schema expectations in [[docs/agile/agents|agents.md]]\n\n---\n\n## üìã Subtasks\n\n- [ ] Translate the Dorian equation from [[symbolic-gravity-models]]\n- [ ] Add unit tests for grant/deny cases\n- [ ] Tie into Cephalon‚Äôs command router\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- Requires agreement on permission schema in `AGENTS.md`\n\n## ‚õìÔ∏è Blocks\n\n- Future multi-user interfaces\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n## ‚ùì Questions\n\n- What format should permission rules use‚ÄîYAML or JSON?\n- Do we need real-time updates or is a static config sufficient?\n#done\n\n"}
{"id":"a5db4128-395b-4d38-8182-abef99521a5d","title":"implement transcendence cascade md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.515Z","uuid":"a5db4128-395b-4d38-8182-abef99521a5d","created_at":"2025-09-15T02:02:58.515Z","path":"docs/agile/tasks/implement_transcendence_cascade.md","content":"## üõ†Ô∏è Task: Implement transcendence cascade\n\nDesign a mechanism where an agent can escalate from ordinary\nconversation into a \"transcendent\" mode‚Äîpulling insight from multiple\ncognitive circuits and returning a synthesized response. This is an\nexperimental feature inspired by the Anankean circuit.\n\nwelp, guess we'll see \n\n---\n\n## üéØ Goals\n\n- Trigger a higher-level reasoning chain when specific cues appear\n- Fuse outputs from Cephalon and Eidolon for a single reply\n- Allow manual activation for testing\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detect trigger phrases or internal thresholds\n- [ ] Provide a pipeline step that aggregates multiple model outputs\n- [ ] Log each cascade event for analysis\n\n---\n\n## üìã Subtasks\n\n- [ ] Prototype a hook in `cephalon/src/index.ts`\n- [ ] Use `services/eidolon/` to provide emotional context\n- [ ] Return combined result via `services/tts`\n- [ ] Reference baseline metrics from [[eidolon-field-math]]\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- Requires baseline emotional data from [[eidolon-field-math|Eidolon Fields]]\n\n## ‚õìÔ∏è Blocks\n\n- Later experimental dialogue modes\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n## ‚ùì Questions\n\n- How should conflicting outputs from Cephalon and Eidolon be resolved?\n- What user-facing cue toggles the cascade mode?\n#IceBox\n\n"}
{"id":"a7b8c9d0-e1f2-4a5b-9c8d-0e1f2a3b4c5d","title":"Fix kanban config path resolution and document usage for agents","status":"todo","priority":"P2","owner":"","labels":["kanban","documentation","agents","cli","config"],"created":"2025-10-05T00:00:00.000Z","uuid":"a7b8c9d0-e1f2-4a5b-9c8d-0e1f2a3b4c5d","created_at":"2025-10-05T00:00:00.000Z","path":"docs/agile/tasks/fix-kanban-config-path-resolution-and-documentation.md","content":"\n## üõ†Ô∏è Task: Fix kanban config path resolution and document usage for agents\n\n## üêõ Problem Statement\n\nThe kanban CLI has a path resolution bug where when called from subdirectories, it finds the correct config file but resolves output paths relative to the current working directory instead of the config file location. This causes agents to think they need to navigate to specific directories to use the kanban tool.\n\n## üéØ Desired Outcome\n\n### Code Fixes:\n1. **Fix path resolution** in `packages/kanban/src/board/config/sources.ts` and related files\n2. **Config paths should resolve relative to config file location**, not current working directory\n3. **Maintain backward compatibility** for existing usage patterns\n\n### Documentation Updates:\n1. **README.md** - Add comprehensive kanban usage section\n2. **AGENTS** - Document kanban tool usage guidelines for AI agents\n3. **CLAUDE** - Add kanban usage instructions for Claude interactions\n4. **Package documentation** - Update kanban package README\n\n## üìã Requirements\n\n### Code Changes:\n- [ ] Update `findConfigPath` function to resolve relative paths from config directory\n- [ ] Ensure `boardFile`, `tasksDir`, and `indexFile` paths resolve correctly\n- [ ] Add tests for path resolution from various subdirectories\n- [ ] Maintain existing CLI argument overrides functionality\n- [ ] Ensure environment variable overrides still work\n\n### Documentation:\n- [ ] Update main README.md with kanban usage section\n- [ ] Update AGENTS file with kanban tool guidelines\n- [ ] Update CLAUDE file with kanban usage instructions\n- [ ] Add kanban package documentation examples\n- [ ] Document path resolution behavior\n\n### Agent Guidelines:\n- [ ] Clarify that kanban commands work from any directory in the repo\n- [ ] Document common kanban workflows for agents\n- [ ] Provide examples of proper kanban usage\n- [ ] Explain when to use flags vs config file\n\n## üèóÔ∏è Implementation Plan\n\n### Phase 1: Code Fix\n1. **Analyze current path resolution logic** in `packages/kanban/src/board/config/`\n2. **Update `resolveWithBase` function** to use config directory as base for relative paths\n3. **Add comprehensive tests** for subdirectory usage scenarios\n4. **Ensure backward compatibility** for existing workflows\n\n### Phase 2: Documentation\n1. **Audit current documentation** across README, AGENTS, CLAUDE files\n2. **Write comprehensive kanban usage section** for README\n3. **Create agent-specific guidelines** for AGENTS file\n4. **Add Claude-specific instructions** to CLAUDE file\n5. **Update kanban package documentation** with examples\n\n### Phase 3: Agent Guidelines\n1. **Document best practices** for agent kanban usage\n2. **Provide common workflow examples**\n3. **Explain when navigation is needed vs when it's not**\n4. **Add troubleshooting guidance** for common agent issues\n\n## üìö Documentation Content Outline\n\n### README.md - Kanban Section\n- Overview of kanban system\n- Basic usage examples\n- Configuration explanation\n- Common workflows\n- Agent usage guidelines\n\n### AGENTS File\n- Kanban tool access patterns\n- When to use flags vs config\n- Common agent workflows\n- Best practices and gotchas\n\n### CLAUDE File\n- Claude-specific kanban interactions\n- Task management workflows\n- Board generation and updates\n- Troubleshooting common issues\n\n## ‚õìÔ∏è Dependencies\n\n- Code analysis of kanban package path resolution\n- Understanding of current config system\n- Documentation audit across multiple files\n\n## üîó Related Resources\n\n- Current kanban config: `promethean.kanban.json`\n- Kanban package: `packages/kanban/`\n- Config sources: `packages/kanban/src/board/config/sources.ts`\n- Documentation files: `README.md`, `AGENTS`, `CLAUDE`\n\n## ‚úÖ Acceptance Criteria\n\n1. **Code Fix**: Kanban commands work correctly from any subdirectory\n2. **Backward Compatibility**: Existing workflows continue to work\n3. **Documentation**: All target files updated with kanban guidance\n4. **Agent Guidelines**: Clear instructions for AI agent usage\n5. **Testing**: Comprehensive test coverage for path resolution\n6. **Examples**: Practical usage examples in documentation\n\n## üìù Notes\n\n- This affects both human and agent users of the kanban system\n- Priority is making it \"just work\" from any directory\n- Documentation is as important as the code fix\n- Consider adding debug/logging for path resolution troubleshooting"}
{"id":"a8067d01-573b-4a24-9d2e-6d9981db563d","title":"migrating relevant modules from riatzukiza github md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.516Z","uuid":"a8067d01-573b-4a24-9d2e-6d9981db563d","created_at":"2025-09-15T02:02:58.516Z","path":"docs/agile/tasks/migrating_relevant_modules_from_riatzukiza_github.md","content":"## üõ†Ô∏è Task: Migrating relevant modules from `riatzukiza.github.io` to `/sites/` and `/docs/`\n\nGenerated from [../unique/2025.07.28.18.07.20.md](../unique/2025.07.28.18.07.20.md)\n\n---\n\n## üéØ Goals\n\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n\n- [ ] TODO\n\n---\n\n#accepted\n\n"}
{"id":"a84c4e37-311f-4991-abe3-473fa1ee869a","title":"scripts group docs utilities","status":"todo","priority":"P3","owner":"","labels":["stabilization"],"created":"","uuid":"a84c4e37-311f-4991-abe3-473fa1ee869a","path":"docs/agile/tasks/scripts_group_docs_utilities.md","content":"Scripts: Group documentation utilities under scripts/docs\n\nGoal: Move all documentation-maintenance utilities into `scripts/docs/` with a README and stable usage.\n\nScope:\n- Create `scripts/docs/` and move:\n  - convert_markdown_links_to_wiki.py\n  - convert_wikilinks.py\n  - lowercase_links.py\n  - generate_orphan_docs.py\n  - chunk_unique_docs.py\n  - strip-file-extensions.ts\n- Add `scripts/docs/README.md` describing each tool, inputs/outputs, and examples.\n- Update any references in docs/ and CI.\n\nExit Criteria:\n- All docs utilities live under `scripts/docs/` and run from repo root.\n- README exists with usage and env notes.\n\n#incoming #scripts #docs #organization\n\n\n"}
{"id":"a9ea7854-0c6f-46e8-9230-1055795a1a23","title":"cephalon tests for persistence and ecs","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.509Z","uuid":"a9ea7854-0c6f-46e8-9230-1055795a1a23","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/cephalon_tests_for_persistence_and_ecs.md","content":"Cephalon: Tests for persistence and ECS flow\n\nGoal: Add tests to ensure transcripts and agent messages are stored during ECS-driven conversations.\n\nWhy: Prevent regressions where the agent monologues/loops without context.\n\nScope:\n- Unit test: simulate `agent.llm.result` and assert `agent_messages` insert via a stubbed `ContextManager`.\n- Integration test: start `start-dialog` flow with mocked LLM + TTS, emit `transcriptEnd`, validate both transcript and agent message persistence.\n\nExit Criteria:\n- Tests pass and fail if persistence calls are removed.\n\n#incoming #cephalon #tests #persistence #ecs\n\n"}
{"id":"aa135028-70f3-4569-a969-915bf857ecaf","title":"Task: ChatGPT Export Ingest with De‚Äëdup Index & Hashes","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.509Z","uuid":"aa135028-70f3-4569-a969-915bf857ecaf","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/chatgpt-export-injest-with-dedupe-index-and-hashes.md","content":"# Task: ChatGPT Export Ingest with De‚Äëdup Index & Hashes\n**Status:** blocked\n\n## Objective\n\nIngest full ChatGPT export archives repeatedly without reprocessing previously seen items. Maintain a durable index of processed message/thread IDs and content hashes to support fast de‚Äëduplication, resumability, and idempotent re‚Äëruns.\n\n---\n\n## Definition of Done (DoD)\n\n* [ ] CLI and/or service endpoint accepts a path to a ChatGPT export (zip or folder).\n* [ ] Parser extracts conversations, messages, attachments, and metadata from the export.\n* [ ] A persistent **IngestIndex** is created/updated with:\n\n  * Stable `conversation_id`, `message_id` keys where available.\n  * Content hash (SHA‚Äë256) of canonicalized payload per message and attachment.\n  * First‚Äëseen timestamp and last‚Äëprocessed timestamp.\n  * Processing status (new/updated/skipped/failed) and error details if any.\n* [ ] Re‚Äërunning against the same export yields **0 reprocessed messages** (all skipped) and finishes quickly.\n* [ ] If a message‚Äôs content changes (hash diff), it is reprocessed exactly once and index is updated.\n* [ ] Idempotent upsert to downstream stores (e.g., Mongo + Chroma) guarded by hash.\n* [ ] Metrics + summary report printed: counts of new/updated/skipped/failed.\n* [ ] Unit tests cover: hashing, canonicalization, index upsert logic, skip logic, resume from partial run.\n\n---\n\n## Inputs\n\n* `EXPORT_PATH`: path to unzipped ChatGPT export directory (or a `.zip` handled by pre‚Äëstep).\n* Optional filters: `--since <ISO8601>`, `--only-conv <id>`, `--dry-run`.\n\n## Outputs\n\n* Updated **IngestIndex** collection/table.\n* Optional: line‚Äëdelimited JSON of newly processed items for piping.\n* Summary stats in stdout.\n\n---\n\n## Data Model (proposed)\n\n```json\n// Collection: ingest_index_chatgpt\n{\n  \"_id\": \"<conversation_id>:<message_id>\",\n  \"conversation_id\": \"string\",\n  \"message_id\": \"string\",\n  \"parent_id\": \"string|null\",\n  \"role\": \"system|user|assistant|tool\",\n  \"created_at\": \"ISO8601\",\n  \"canonical_text\": \"string\", // normalized text used for hashing\n  \"content_hash\": \"sha256-hex\",\n  \"attachments\": [\n    {\n      \"name\": \"string\",\n      \"mimetype\": \"string\",\n      \"bytes\": \"int\",\n      \"hash\": \"sha256-hex\"\n    }\n  ],\n  \"first_seen_at\": \"ISO8601\",\n  \"last_processed_at\": \"ISO8601\",\n  \"status\": \"new|updated|skipped|failed\",\n  \"error\": {\n    \"message\": \"string\",\n    \"stack\": \"string\"\n  },\n  \"version\": 1\n}\n```\n\n> Note: If export lacks stable message IDs, derive `message_id` = `sha256(conversation_id + index_in_thread + timestamp + canonical_text[:128])`.\n\n---\n\n## Canonicalization Rules\n\n* Normalize line endings to `\\n`.\n* Trim trailing spaces; collapse 2+ blank lines to 1.\n* Strip non‚Äësemantic whitespace in markdown code fences.\n* Normalize JSON within code blocks by parsing and stringifying with stable key order when possible.\n* For attachments, hash the raw bytes; for images inside export, include EXIF‚Äëstripped bytes if present.\n\n---\n\n## Hashing Strategy\n\n* Message hash = `sha256(role + \"\\n\" + canonical_text + \"\\n\" + join(att_hashes, \",\"))`.\n* Thread hash (optional) = merkle root of ordered message hashes.\n* Store both message‚Äëlevel and thread‚Äëlevel hashes to detect edits and reorderings.\n\n---\n\n## High‚ÄëLevel Flow\n\n```mermaid\nflowchart TD\n    A[Start ingest] --> B{Is ZIP?}\n    B -- yes --> C[Unzip to temp dir]\n    B -- no --> D[Use as dir]\n    C --> D\n    D --> E[Scan conversations.json & messages]\n    E --> F[Canonicalize message]\n    F --> G[Compute SHA‚Äë256 hash]\n    G --> H{Index has _id?}\n    H -- no --> I[Mark NEW; process & upsert]\n    H -- yes --> J{Hash changed?}\n    J -- no --> K[Skip; update last_processed_at]\n    J -- yes --> L[Mark UPDATED; reprocess]\n    I --> M[Write to stores]\n    L --> M\n    M --> N[Update IngestIndex]\n    K --> N\n    N --> O[Metrics & summary]\n    O --> P[Done]\n```\n\n---\n\n## Pseudocode (Sibilant‚Äëstyle)\n\n```lisp\n(defn process-export [export-path opts]\n  (let [root (ensure-dir export-path)\n        convs (read-conversations root)]\n    (for-each (lambda (conv)\n                (process-conversation conv opts))\n              convs)\n    (print-summary)))\n\n(defn process-conversation [conv opts]\n  (for-each (lambda (msg idx)\n              (let* [canon (canonicalize (get msg \"text\"))\n                     att-hashes (map sha256-bytes (attachments-bytes msg))\n                     hash (sha256 (join \"\\n\" [(get msg \"role\") canon (join \",\" att-hashes)]))\n                     id (stable-id conv msg idx)\n                     rec (ingest-index-get id)]\n                (cond\n                  (nil? rec) (ingest-new id conv msg hash canon att-hashes)\n                  (= hash (get rec \"content_hash\")) (ingest-skip id)\n                  :else (ingest-update id conv msg hash canon att-hashes))))\n            (enumerate (get conv \"messages\"))))\n```\n\n---\n\n## Implementation Tasks\n\n1. **Scaffold**\n\n   * [ ] Add `services/ts/chatgpt-ingest` (or integrate into existing ingest service).\n   * [ ] CLI via `pnpm tsx src/cli.ts` with args: `--export <path> [--since ISO] [--only-conv id] [--dry-run]`.\n\n2. **Parser**\n\n   * [ ] Support official ChatGPT export structure (`conversations.json`, `messages/`, `attachments/`).\n   * [ ] Extract: conversation title, id, create time, per‚Äëmessage role, text, code blocks, attachments.\n\n3. **Canonicalizer & Hashing**\n\n   * [ ] Implement text canonicalization module with tests.\n   * [ ] Implement SHA‚Äë256 helper for strings + files.\n   * [ ] Optional: Merkle builder for thread hash.\n\n4. **Index Store**\n\n   * [ ] Create `ingest_index_chatgpt` in Mongo (or DualStore abstraction).\n   * [ ] Upsert by `_id = conversation_id:message_id`.\n   * [ ] Maintain `first_seen_at`, `last_processed_at`, `status`, `content_hash`.\n\n5. **Processing Pipeline**\n\n   * [ ] For NEW/UPDATED: emit to downstream sinks (Mongo docs, Chroma embeddings, file store) behind a hash guard.\n   * [ ] For SKIPPED: only touch `last_processed_at`.\n   * [ ] `--dry-run` prints plan without writes.\n\n6. **Resumability & Partial Runs**\n\n   * [ ] Maintain a run log with `run_id`, totals, and per‚Äëconv cursors.\n   * [ ] On crash, re‚Äëinvoke continues where left off.\n\n7. **Reporting & Metrics**\n\n   * [ ] At end: print counts (new/updated/skipped/failed) and timing.\n   * [ ] Export `ingest-summary-YYYYMMDDTHHMMSS.json`.\n\n8. **Tests**\n\n   * [ ] Unit: canonicalize, hash, ID derivation, index upsert, skip logic.\n   * [ ] Integration: run twice; second run should skip all.\n\n---\n\n## CLI Sketch\n\n```bash\npnpm tsx src/cli.ts --export ~/Downloads/chatgpt-export \\\n  --since 2025-01-01T00:00:00Z --dry-run\n```\n\n---\n\n## Failure Modes & Guards\n\n* Export schema variations ‚Üí feature‚Äëflag parser versions; validate with JSON schema.\n* Missing IDs ‚Üí fall back to derived IDs; store `id_source = derived|native`.\n* Large exports ‚Üí stream read files; batch writes; backoff on DB.\n* Hash collisions (practically negligible with SHA‚Äë256) ‚Üí also compare length + role.\n* Time skew ‚Üí prefer content‚Äëbased hashes over timestamps for change detection.\n\n---\n\n## Optional Enhancements\n\n* Bloom filter of seen message hashes for ultra‚Äëfast prechecks in memory.\n* Secondary index on `content_hash` to detect duplicates across conversations.\n* Attachment dedup store keyed by file hash with ref‚Äëcounts.\n* Metrics to heartbeat service.\n\n---\n\n## Acceptance Checklist (Quick)\n\n* [ ] Second run over same export yields only SKIPPED messages.\n* [ ] Editing one message in export results in exactly one UPDATED.\n* [ ] Index survives process restarts and supports resuming.\n* [ ] Report artifact written with accurate counts.\n\n## Blockers\n- No active owner or unclear scope\n\n#breakdown\n\n"}
{"id":"ad2b6f21-1a73-4682-96bb-f7ea7dd09cfa","title":"wrap event hub publish path with withschemavalidat md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.523Z","uuid":"ad2b6f21-1a73-4682-96bb-f7ea7dd09cfa","created_at":"2025-09-15T02:02:58.523Z","path":"docs/agile/tasks/wrap_event_hub_publish_path_with_withschemavalidat.md","content":"## üõ†Ô∏è Task: Wrap `event-hub` publish path with **withSchemaValidation**; fail fast on bad payloads\n\nGenerated from [../unique/2025.08.08.20.08.56.md](../unique/2025.08.08.20.08.56.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"ad8abaf0-5abc-407f-bb20-0f0557066452","title":"write end to end tests md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.524Z","uuid":"ad8abaf0-5abc-407f-bb20-0f0557066452","created_at":"2025-09-15T02:02:58.524Z","path":"docs/agile/tasks/write_end_to_end_tests.md","content":"## üõ†Ô∏è Description\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Add Ollama formally to pipeline\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#accepted\n\n"}
{"id":"ae52ea20-929e-41f6-982c-b1b2720d0e3b","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"ae52ea20-929e-41f6-982c-b1b2720d0e3b","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/decouple-audio-processing-logic-from-discord.md","content":"# Description\n\nDescribe your task\n\n## Related Epics\n- [[Import other projects.md]]\n## Requirements/Definition of done\n\n- If it doesn't have this, we can't accept it\n\n## Tasks \n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n"}
{"id":"af127b29-3031-4f13-a82b-7d7042581e35","title":"ecs component schemas core","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"af127b29-3031-4f13-a82b-7d7042581e35","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/ecs_component_schemas_core.md","content":"ECS Component Schemas (Core Conversation)\n\nGoal: Define canonical ECS components (with JSON Schemas) for conversation and IO.\n\nComponents:\n- Conversation entities: `UserMessage`, `AgentMessage`, `Turn`, `TranscriptFinal`, `Utterance`, `ContextWindow`.\n- IO/state: `RawVAD`, `VAD`, `AudioRef`, `AudioRes`, `VisionFrame`, `VisionRing`, `Policy` (existing).\n- LLM: `LLMRequest`, `LLMResult` (link to message/turn via ids).\n\nDeliverables:\n- `shared/ts/src/agent-ecs/schemas/*.json` for component schemas.\n- Mapping doc: component ‚Üí Mongo collection, indexes, retention policy.\n\nExit Criteria:\n- Schemas published and referenced by adapter & services.\n\n#incoming #ecs #schemas #conversation\n\n"}
{"id":"af87b6ca-6f04-4380-9e5b-54adcf7bf254","title":"update cephalon to use custom embedding function md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.523Z","uuid":"af87b6ca-6f04-4380-9e5b-54adcf7bf254","created_at":"2025-09-15T02:02:58.523Z","path":"docs/agile/tasks/update_cephalon_to_use_custom_embedding_function.md","content":"## üõ†Ô∏è Task: Update cephalon to use custom embedding function\n\nDesign notes point toward replacing the default Chroma embeddings with a lightweight Python service. Cephalon should call this service when generating context vectors.\n\nChroma was causing issues for our testing environments so we need to be able to decouple this service from it to do CI/CD efficiently. It seemed like a dependency of a dependency of it was requiring GPU drivers.\n\nWe won't necessarily be using this function unless it doesn't meaningfully impact performance.\n\nAt this time, we are just focused on reproducibility. Testability, and clarity.\n\nPremature optimization is bad.\n\n\n---\n\n## üéØ Goals\n- Decouple embedding logic from ChromaDB\n- Allow easy end to end testing in CI/CD\n- Allow remote agents to run code when they don't have access to GPUs\n- Allow experimentation with alternative models\n\n---\n\n## üì¶ Requirements\n- [ ] Implement a new embedding service with out low level dependencies\n- [ ] Implement API calls from other services to the embedding service\n- [ ] The option to use chroma remains\n- [ ] There is a shared class that wraps the API call for easy use in other services.\n- [ ] Decouple services from chroma\n\n---\n\n## üìã Subtasks\n- [ ] Define request/response schema in `bridge/protocols`\n- [ ] Write tests for new embeddings service\n- [ ] Unit tests\n- [ ] e2e\n- [ ] integration\n- [ ] Write unit tests for the new helper\n- [ ] Implement service\n- [ ] implement wrapper class\n- [ ] Update `cephalon/src` to fetch embeddings asynchronously using wrapper class from shared\n- [ ] Update `discord-embedder/src` to fetch embeddings asynchronously using wrapper class from shared\n\n---\n\n## üîó Related Epics\n#framework-core #cephalon #discord #embedding #typescript\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\nNothing\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n- [pseudo/eidolon-field-scratchpad.lisp](../../pseudo/eidolon-field-scratchpad.lisp)\n#in-progress\n\n## Blockers\n- Embedding service implementation not linked.\n- Unit and integration tests for wrapper and service missing.\n- Documentation for the new embedding workflow is absent.\n\n"}
{"id":"af99c867-867a-420d-a436-73676a2ec7ce","title":"discord image attachment indexer md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.511Z","uuid":"af99c867-867a-420d-a436-73676a2ec7ce","created_at":"2025-09-15T02:02:58.511Z","path":"docs/agile/tasks/discord_image_attachment_indexer.md","content":"## üõ†Ô∏è Description\n\n\nI at least started this, I think  it's working,  but I don't think it's being used for anything.\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Add Ollama formally to pipeline\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core #discord #images #attachments #indexing  #memory\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#done\n\n"}
{"id":"b0d32736-46a9-42af-aab8-211b8ae43109","title":"Reason for rejection","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.519Z","uuid":"b0d32736-46a9-42af-aab8-211b8ae43109","created_at":"2025-09-15T02:02:58.519Z","path":"docs/agile/tasks/run_bakeoff_see_below.md","content":"## üõ†Ô∏è Task: Run bakeoff (see below)\n\nGenerated from [../unique/2025.08.08.16.08.27.md](../unique/2025.08.08.16.08.27.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n# Reason for rejection\n\nDuplicate of [[run_model_bakeoff_md|Run model bakeoff]].\n\n#rejected\n\n"}
{"id":"b2048125-45fd-4c09-aaeb-3359a4840c0b","title":"Placeholder ‚Äî will be overwritten by nitpack","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.518Z","uuid":"b2048125-45fd-4c09-aaeb-3359a4840c0b","created_at":"2025-09-15T02:02:58.518Z","path":"docs/agile/tasks/pr-688-nitpack-policy.md","content":"# Placeholder ‚Äî will be overwritten by nitpack\n\n"}
{"id":"b27a270a-d257-40ca-bfc2-438afeeca829","title":"audit makefile.hy remnants and confirm deprecation","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"b27a270a-d257-40ca-bfc2-438afeeca829","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/breakdown-makefile.hy.md","content":"## üõ†Ô∏è Description\n**Status:** planning\n\n`Makefile.hy` previously orchestrated builds but is now unmaintained. The audit confirms no active workflows call into it, yet\nseveral documents still reference a pending breakdown. This task formalizes the retirement plan and documents any remaining\nreferences that must be redirected to pnpm / TypeScript tooling.\n\n## ‚úÖ Decision\n- Treat `Makefile.hy` as legacy; do not resurrect Hy-based generators.\n- Capture any residual references so they can be re-pointed to pnpm scripts.\n\n---\n\n## üéØ Goals\n- Inventory lingering links to `Makefile.hy` and classify them (documentation vs. tooling).\n- Provide recommendations for replacing each reference with pnpm/TS equivalents.\n- Deliver a short deprecation memo so future tasks do not reintroduce the Hy pipeline.\n\n---\n\n## üì¶ Requirements\n- [ ] List every reference discovered during the audit (boards, tasks, docs).\n- [ ] Note the pnpm/TypeScript alternative for each workflow.\n- [ ] Open follow-up tasks (if needed) for high-effort migrations.\n\n---\n\n## üìã Subtasks\n- [ ] Search repo for `Makefile.hy` mentions and classify them.\n- [ ] Update docs/tasks inline when the fix is trivial; file follow-ups otherwise.\n- [ ] Summarize findings in `docs/notes/` alongside the Makefile retirement log.\n\n---\n\n## üîó Related Epics\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\n- [[update_github_actions_to_use_makefile_md_md|update github actions automation to pnpm scripts]]\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n- [Legacy Makefile audit note](../notes/promethean-dev-workflow-update.md)\n\n#framework-core #todo\n"}
{"id":"b2929e05-8da5-477c-b537-16757882e39c","title":"switch critical readers to subscribenormalized md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.522Z","uuid":"b2929e05-8da5-477c-b537-16757882e39c","created_at":"2025-09-15T02:02:58.522Z","path":"docs/agile/tasks/switch_critical_readers_to_subscribenormalized.md","content":"## üõ†Ô∏è Task: Switch critical readers to **subscribeNormalized**\n\nGenerated from [../unique/2025.08.08.20.08.52.md](../unique/2025.08.08.20.08.52.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"b2d63853-2a8e-4e5c-a196-3fc69299d876","title":"WebSocket Broker API Parity with GPT Bridge","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"b2d63853-2a8e-4e5c-a196-3fc69299d876","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/create-broker-services-that-can-handle-all-the-same-tasks-as-the-gpt-bridge.md","content":"# WebSocket Broker API Parity with GPT Bridge\n\n**Owner:** Codex / Agent\n**Status:** continue coding\n**Labels:** #broker #ws #api #gptbridge #parity #promethean #ops\n\n---\n\n## üõ†Ô∏è Description\n\nBring the **WebSocket broker API** up to full parity with the existing **GPT Bridge HTTP interface**. Once parity is achieved, the GPT Bridge becomes a **thin wrapper** that proxies requests into the broker (adding load control, queueing, and overload protection). This ensures that the broker, which already manages system health and backpressure, becomes the **source of truth** for all messaging and tool calls.\n\n---\n\n## üéØ Goals\n\n* Match **all GPT Bridge features** in the WebSocket broker (chat, tool calls, streaming, metadata, etc.).\n* Expose a **stable, documented schema** for broker events and actions.\n* Ensure GPT Bridge wrappers proxy to broker seamlessly, adding only:\n\n  * **Rate limiting**\n  * **Load shedding**\n  * **Request shaping** (timeouts, retries, queue management)\n* Make **broker ‚Üí service ‚Üí client** the canonical path for all communication.\n* Guarantee CI verifies feature parity between broker and GPT Bridge.\n\n---\n\n## üì¶ Requirements / Definition of Done\n\n* [ ] Broker supports **all GPT Bridge endpoints** as WebSocket actions/events.\n* [ ] JSON schema defined for broker requests/responses (validated at runtime).\n* [ ] GPT Bridge code reduced to a wrapper around broker client.\n* [ ] Load shedding (queue caps, timeouts, backpressure) **only** happens in GPT Bridge.\n* [ ] Broker metrics exported (active connections, queue depth, dropped, latency).\n* [ ] CI runs **side-by-side tests** ensuring parity between GPT Bridge and broker responses.\n* [ ] Documentation updated: GPT Bridge labeled **deprecated entrypoint** in future, with WS broker as canonical API.\n\n---\n\n## üìã Tasks\n\n### Step 1 ‚Äî Inventory & Schema\n\n* [ ] List **all GPT Bridge APIs** currently in use:\n\n  * Chat completions (sync/stream)\n  * Embedding calls\n  * Tool calling / structured outputs\n  * File queries (if any)\n  * Health/metrics endpoints\n* [ ] Define **WebSocket equivalents** for each:\n\n  * `action: \"chat.completion\"`\n  * `action: \"embedding.query\"`\n  * `action: \"tool.invoke\"`\n  * `action: \"system.health\"`\n* [ ] Create JSON schema contracts (`shared/ts/src/broker/schemas/*.json`).\n* [ ] Add runtime validation (Ajv or zod).\n\n### Step 2 ‚Äî Broker Implementation\n\n* [ ] Implement **WebSocket handlers** for each GPT Bridge feature.\n* [ ] Add **request IDs** + correlation so responses can be tracked.\n* [ ] Support **streaming events** (`delta` chunks for chat).\n* [ ] Integrate with existing broker publish/subscribe system (reuse topics).\n* [ ] Add **backpressure** handling in broker (pause/resume signals, per-connection limits).\n* [ ] Export broker metrics:\n\n  * Connected clients\n  * Requests/sec\n  * Queue depth\n  * Dropped / errored\n\n### Step 3 ‚Äî GPT Bridge Wrapper\n\n* [ ] Replace GPT Bridge implementations with a **broker client**.\n* [ ] Add **overload protection**:\n\n  * Connection pooling\n  * Per-user rate limits\n  * Queue caps with fail-fast\n* [ ] Keep identical API surface (so clients don‚Äôt break).\n* [ ] Mark GPT Bridge code paths as **deprecated** in docs, but keep for now.\n\n### Step 4 ‚Äî Testing, CI, and Docs\n\n* [ ] Write **side-by-side parity tests**:\n\n  * Given same input ‚Üí broker vs GPT Bridge ‚Üí outputs match.\n  * Streaming responses chunk identically.\n* [ ] CI runs parity suite on every PR.\n* [ ] Add **integration tests** with actual services (chat, embeddings, tool calls).\n* [ ] Document:\n\n  * Broker API schemas (with examples).\n  * Migration path: ‚Äúnew clients should connect directly to broker.‚Äù\n  * GPT Bridge purpose: ‚Äúthin wrapper for load shedding only.‚Äù\n\n---\n\n## üîß Example Broker Messages\n\n**Request:**\n\n```json\n{\n  \"id\": \"req-123\",\n  \"action\": \"chat.completion\",\n  \"payload\": {\n    \"model\": \"qwen3:14b-instruct\",\n    \"messages\": [\n      { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n      { \"role\": \"user\", \"content\": \"Explain Promethean.\" }\n    ],\n    \"stream\": true\n  }\n}\n```\n\n**Response (streaming chunks):**\n\n```json\n{\n  \"id\": \"req-123\",\n  \"event\": \"chat.delta\",\n  \"delta\": \"Promethean is a modular, multi-agent system...\"\n}\n```\n\n**Final Response:**\n\n```json\n{\n  \"id\": \"req-123\",\n  \"event\": \"chat.complete\",\n  \"output\": {\n    \"text\": \"Promethean is a modular...\"\n  },\n  \"usage\": { \"prompt_tokens\": 42, \"completion_tokens\": 133, \"total_tokens\": 175 }\n}\n```\n\n---\n\n## ‚úÖ Acceptance Criteria\n\n* [ ] Every GPT Bridge endpoint has a **1:1 broker action**.\n* [ ] GPT Bridge wrapper proxies requests to broker only, adding load shedding.\n* [ ] Parity suite passes in CI (outputs identical across both paths).\n* [ ] Docs state broker is **canonical API**.\n* [ ] Metrics visible for broker connections and GPT Bridge load shedding.\n\n---\n\n## üìÇ Proposed Files/Paths\n\n* `shared/ts/src/broker/schemas/*.json` ‚Äî JSON schemas\n* `shared/ts/src/broker/client.ts` ‚Äî WS client wrapper\n* `services/ts/broker/handlers/*.ts` ‚Äî action handlers\n* `services/ts/gptbridge/main.ts` ‚Äî reduced wrapper\n* `tests/parity/broker-vs-bridge.spec.ts` ‚Äî parity suite\n* `docs/api/broker.md` ‚Äî schema + examples\n* `docs/migration/broker-vs-bridge.md` ‚Äî migration notes\n\n---\n\n## üß± Step 1‚Äì4 Milestones\n\n* [ ] **Step 1 ‚Äî Inventory & Schema**: List GPT Bridge APIs, define broker equivalents, write schemas.\n* [ ] **Step 2 ‚Äî Broker Implementation**: Implement handlers, events, metrics.\n* [ ] **Step 3 ‚Äî GPT Bridge Wrapper**: Proxy to broker client, add load shedding, keep surface.\n* [ ] **Step 4 ‚Äî Testing & Docs**: Parity suite, CI, documentation, migration path.\n\n---\n\n## üîç Relevant Resources\n\n* You might find \\[this] useful while working on this task\n  *(link internal GPT Bridge API reference + broker design docs)*\n\n---\n\n## Comments\n\nAppend-only thread for agents to log discovered gaps, schema changes, or overload handling notes.\n\n#ready\n\n"}
{"id":"b4f3da22-f345-4ea2-84ec-7277ca9e0735","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.514Z","uuid":"b4f3da22-f345-4ea2-84ec-7277ca9e0735","created_at":"2025-09-15T02:02:58.514Z","path":"docs/agile/tasks/harden-precommit-hooks.md","content":"# Description\n**Status:** blocked\n\nWe want to make it as hard as possible for bad code and bad documentation to be committed to the git repository.\n\n## Requirements/Definition of done\n\n- `make generate-makefile` regenerates the Makefile after updating hooks.\n- Commits are blocked when tests fail.\n- Commits are blocked if coverage drops below 80%.\n- Pre-commit clearly reports failing steps to the user.\n\n## Tasks\n\n- [ ] Add pre-commit step to run `make test` and verify pass.\n- [ ] Integrate coverage report and enforce ‚â•80% threshold.\n- [ ] Regenerate `Makefile` by running `make generate-makefile`.\n- [ ] Document hook behavior in developer docs.\n\n## Relevant resources\n\nYou might find [this](https://pre-commit.com/) useful while working on this task.\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n\n#breakdown\n\n## Blockers\n- No active owner or unclear scope\n\n"}
{"id":"b543e648-991c-4365-8d76-280c60c2c31e","title":"decouple from ollama md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"b543e648-991c-4365-8d76-280c60c2c31e","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/decouple_from_ollama.md","content":"## üõ†Ô∏è Task: Add Ollama formally to pipeline\n\nIt's difficult to get the system running for codex and CI/CD for them to properly run integration tests, or  just to sanity  test that it starts.\n\n\n\n---\n\n## üéØ Goals\n\n- Allow the system to source models from a diverse set of LLM providers\n- Open up the option to train custom LLM more easily for the system\n\n---\n\n## üì¶ Requirements\n\n- System Can use any model from:\n\t- ollama\n\t- HuggingFace\n\t- ...npu/open vino?\n\n---\n\n## üìã Subtasks\n\n- [ ] Write driver classes for llm service\n- [ ] \n\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#ready\n\n"}
{"id":"b67ff78e-f1b8-4cdf-ba1c-d2ed622a4882","title":"replace polyglot makefile with pnpm-first workflow docs","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.520Z","uuid":"b67ff78e-f1b8-4cdf-ba1c-d2ed622a4882","created_at":"2025-09-15T02:02:58.520Z","path":"docs/agile/tasks/set_up_makefile_for_python_js_build_test_dev.md","content":"## üõ†Ô∏è Task: Replace polyglot Makefile with pnpm-first workflow docs\n\nThe repo already uses pnpm workspaces, TypeScript scripts, and `scripts/dev.mjs`. Instead of maintaining a cross-language\nMakefile, capture the pnpm-based workflow and retire the old automation checklist.\n\n## ‚úÖ Decision\n- Treat this initiative as documentation/cleanup‚Äîno new Makefile to be written.\n- Publish guidance showing how pnpm covers setup, service start, testing, and cleanup.\n\n---\n\n## üéØ Goals\n- Provide a pnpm-focused quickstart for setting up JS/TS + any remaining Python helpers.\n- Highlight existing scripts (`pnpm install`, `pnpm test:all`, `pnpm run clean`, `scripts/dev.mjs`).\n- Record that the Makefile experiment is archived and no longer required.\n\n---\n\n## üì¶ Requirements\n- [ ] Update onboarding docs with pnpm workflow tables.\n- [ ] Call out how to execute Python tooling (if any) directly, without Makefile wrappers.\n- [ ] Ensure the old Makefile checklist is marked as superseded.\n\n---\n\n## üìã Subtasks\n- [ ] Draft documentation section in README or docs/notes.\n- [ ] Remove stale references to `make setup` / `make test` from guides.\n- [ ] Link to the Makefile retirement note in `docs/notes/`.\n\n---\n\n## üîó Related Epics\n#cicd #buildtools #devtools #devops\n\n---\n\n## ‚õìÔ∏è Blocked By\n- [[update_makefile_to_have_commands_specific_for_agents_md|replace agent automation makefile targets with pnpm scripts]]\n\n## ‚õìÔ∏è Blocks\nNothing\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n- [Makefile retirement note](../notes/promethean-dev-workflow-update.md)\n\n#cicd #buildtools #devtools #devops #archive\n"}
{"id":"bbd54acd-a5ea-4c0c-a20c-9608f0e898ed","title":"add unit tests for date tools py md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.506Z","uuid":"bbd54acd-a5ea-4c0c-a20c-9608f0e898ed","created_at":"2025-09-15T02:02:58.506Z","path":"docs/agile/tasks/add_unit_tests_for_date_tools_py.md","content":"## üõ†Ô∏è Task: Add unit tests for date_tools.py\n\nWrite pytest tests covering functions in `shared/py/date_tools.py`.\n\n---\n\n## üéØ Goals\n- Achieve at least 90% coverage for this module\n- Ensure edge cases for `time_ago` and related utilities are tested\n\n---\n\n## üì¶ Requirements\n- [ ] Use pytest with clear assertions\n- [ ] Include small datetime fixtures\n\n---\n\n## üìã Subtasks\n- [ ] Test `time_ago` relative formatting\n- [ ] Test `days_until` calculation\n\n---\n\n## üîó Related Epics\n#codex-task #testing\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\nNothing\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n- [Untested modules](../../untested-code.md)\n#done\n\n"}
{"id":"bf41be60-e9bf-4a19-95da-a453a32b9f2e","title":"fully convert js ts projects to pnpm incoming","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.514Z","uuid":"bf41be60-e9bf-4a19-95da-a453a32b9f2e","created_at":"2025-09-15T02:02:58.514Z","path":"docs/agile/tasks/fully-convert-js-ts-projects-to-pnpm-incoming.md","content":"## üõ†Ô∏è Description\n\n\nIs this done already? I think... mainly we need to look for every reference to npm and replace it so the agents don't get confused. They somehow keep finding references, then they add new records.\n\n---\n\n## üéØ Goals\n\n- What are we trying to accomplish?\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#archive\n\n"}
{"id":"c0de5fe3-a795-4795-a368-50ce36f2a705","title":"add tokenbucket to ws server conn per topic md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.506Z","uuid":"c0de5fe3-a795-4795-a368-50ce36f2a705","created_at":"2025-09-15T02:02:58.506Z","path":"docs/agile/tasks/add_tokenbucket_to_ws_server_conn_per_topic.md","content":"## üõ†Ô∏è Task: Add `TokenBucket` to WS server (conn + per-topic)\n\nGenerated from [../unique/2025.08.08.19.08.25.md](../unique/2025.08.08.19.08.25.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"c20e39a1-acb8-400c-8d6f-5228f15811ad","title":"migrating relevant modules from riatzukiza github io to site and docs md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.516Z","uuid":"c20e39a1-acb8-400c-8d6f-5228f15811ad","created_at":"2025-09-15T02:02:58.516Z","path":"docs/agile/tasks/migrating_relevant_modules_from_riatzukiza_github_io_to_-site-_and_-docs-.md","content":"## üõ†Ô∏è Task: Migrating relevant modules from riatzukiza.github.io to -site- and -docs-\n\nMove documentation and site pieces from the legacy `riatzukiza.github.io` repository into this project.\n\n---\n\n## üéØ Goals\n\n- Preserve useful information from the old repository\n- Keep the project website and docs in a single place\n- Remove outdated or redundant files\n\n---\n\n## üì¶ Requirements\n\n- [ ] Review the old repo and list files worth keeping\n- [ ] Copy markdown docs into `docs/`\n- [ ] Copy website assets into `sites/`\n- [ ] Update internal links to match the new structure\n\n---\n\n## üìã Subtasks\n\n- [ ] [Extract docs from riatzukiza.github.io](Extract%20docs%20from%20riatzukiza.github.io.md)\n- [ ] [Extract site modules from riatzukiza.github.io](Extract%20site%20modules%20from%20riatzukiza.github.io.md)\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\n- Future website styling tasks\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n  #ready\n\n"}
{"id":"c2ee671b-c84a-4461-b71a-8b598470aef4","title":"wire runoutboxdrainer in event hub md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.523Z","uuid":"c2ee671b-c84a-4461-b71a-8b598470aef4","created_at":"2025-09-15T02:02:58.523Z","path":"docs/agile/tasks/wire_runoutboxdrainer_in_event_hub.md","content":"## üõ†Ô∏è Task: Wire `runOutboxDrainer` in event-hub\n\nGenerated from [../unique/2025.08.08.19.08.49.md](../unique/2025.08.08.19.08.49.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"c3366097-0d29-4c8d-b7e1-32de4cf8072e","title":"setup kanban ui for kanban package","status":"in_progress","priority":"P3","owner":"","labels":["kanban","ui","framework-core"],"created":"2025-10-03T07:25:00.000Z","uuid":"c3366097-0d29-4c8d-b7e1-32de4cf8072e","created_at":"2025-10-03T07:25:00.000Z","path":"docs/agile/tasks/Setup Kanban UI for Kanban package.md","content":"## üéØ Desired Outcome\nA lightweight web UI served from `@promethean/kanban` that visualises the current board using the package's existing loaders, so agents can inspect column WIP without opening Obsidian.\n\n## üìè Acceptance Criteria\n- `pnpm kanban ui` (or equivalent) serves an HTML dashboard reachable locally.\n- The UI renders all board columns, task counts, and task metadata (title, priority, labels, created timestamp).\n- Data refresh works without reloading the page (manual refresh button and automatic polling acceptable).\n- Coverage via unit tests for the render pipeline and a smoke test for the server endpoint.\n- README documents how to launch the UI.\n\n## üõ†Ô∏è Implementation Notes\n- Reuse `loadBoard` to obtain data; expose it through a small HTTP server in `packages/kanban`.\n- Implement UI as a module under `src/frontend/` with zero external dependencies (vanilla web components or DOM helpers).\n- Ship CSS with the UI so it is readable without further tooling.\n- Add AVA tests for HTML rendering and server response.\n\n## ‚ùì Open Questions / Risks\n- Ensure CLI wiring keeps backward compatibility with existing commands.\n- Confirm that serving compiled frontend assets from `dist/frontend` works under Nx build output.\n"}
{"id":"c39b94ff-9e97-4fba-99d9-5cc713e09e5d","title":"file explorer","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"c39b94ff-9e97-4fba-99d9-5cc713e09e5d","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/file-explorer.md","content":"## üõ†Ô∏è Description\n\nProvide a lightweight file explorer for navigating repository contents through the agent interface.\n\n---\n\n## üéØ Goals\n\n- Browse directories and open file previews.\n- Search for files by name within allowed scope.\n\n---\n\n## üì¶ Requirements\n\n- [x] Read-only navigation respecting permission rules.\n- [x] Search functionality with fuzzy matching.\n- [x] Clear error messages for restricted paths.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline UX and permission constraints.\n- [x] Implement backend API for listing and reading files.\n- [x] Add UI component or chat command for navigation. ‚úÖ 2025-09-13\n- [x] Include tests covering permission and search cases.\n- [x] Document usage with examples.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n- [[c39b94ff-file-explorer|Review findings (2025-09-30)]]\n\n\n## Notes\n- Tests or documentation are missing; acceptance criteria not fully met.\n- Story Points: 5\n\n#ready\n\n"}
{"id":"c5ebef15-dfbb-4ab4-a477-dd8584201758","title":"Placeholder ‚Äî will be overwritten by nitpack","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.518Z","uuid":"c5ebef15-dfbb-4ab4-a477-dd8584201758","created_at":"2025-09-15T02:02:58.518Z","path":"docs/agile/tasks/pr-688-nitpack-codemods.md","content":"# Placeholder ‚Äî will be overwritten by nitpack\n\n"}
{"id":"c6139027-bd0a-4f44-a200-c04829e35220","title":"structure vault to mirror services agents docs md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.522Z","uuid":"c6139027-bd0a-4f44-a200-c04829e35220","created_at":"2025-09-15T02:02:58.522Z","path":"docs/agile/tasks/structure_vault_to_mirror_services_agents_docs.md","content":"## üõ†Ô∏è Task: Document-Driven Development for Service Scripts\n**Status:** blocked\n\nWe are following a **document-driven development** process.  \nThis means **each program file must have a corresponding documentation file** that fully describes its behavior, purpose, and relationships‚Äî*before or alongside implementation*.\n\nOur documentation structure should **mirror the code structure** one-to-one.  \nThese docs should be so complete and precise that any developer (or LLM) could rebuild the system from documentation alone.\n\n---\n\n## üéØ Goals\n\n- Every program file has a corresponding documentation file\n- Each document fully describes:\n  - The script's purpose\n  - Its internal logic and structure\n  - Its role in the larger system\n- Documents include **bi-directional links** to relevant dependencies and dependents\n- Enables better AI pair programming, onboarding, and debugging\n\n---\n\n## üì¶ Requirements\n\n- [ ] Folder structure for docs matches `services/`, `shared/`, etc.\n- [ ] Every source file has a `README.md`, `DOC.md`, or `.note.md` style doc\n- [ ] Docs are written in Obsidian-compatible markdown\n- [ ] Links to dependencies and dependents use [wiki-style](wiki-style.md) links\n- [ ] Documents include commentary on design intent, not just implementation\n- [ ] Each file should follow the [[file.doc.template]] structure.\n\n---\n\n## üìã Subtasks\n\n- [ ] Create matching folder structure in `docs/` or `notes/` (or use vault-wide)\n- [ ] For each existing script:\n  - [ ] Create corresponding markdown file\n  - [ ] Describe its internal logic clearly\n  - [ ] Explain its role in the larger system\n  - [ ] Link to:\n    - [ ] Files it *depends on*\n    - [ ] Files that *depend on it*\n- [ ] Add tags for epics, services, and layers (e.g. #cephalon, #layer1)\n\n---\n\n## üîó Related Epics\n\n#cicd #buildtools #devtools #devops  \n#documentation #knowledge-graph #docdrivendev\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\n- Automated documentation generation\n- LLM prompt generation from doc tree\n- CI/CD config templating\n\n---\n\n## üîç Relevant Links\n\n- [promethean-doc-structure](promethean-doc-structure.md)\n- [project-file-structure](project-file-structure.md)\n- [doc-template](doc-template.md)\n- [docdrivendev-principles](docdrivendev-principles.md)\n#ice-box\n\n## Blockers\n- No active owner or unclear scope\n\n#breakdown\n\n"}
{"id":"c71b790d-b9d6-45ee-b3e7-517ca66ff4aa","title":"mirror shared utils with language specific doc folders md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.517Z","uuid":"c71b790d-b9d6-45ee-b3e7-517ca66ff4aa","created_at":"2025-09-15T02:02:58.517Z","path":"docs/agile/tasks/mirror_shared_utils_with_language-specific_doc_folders.md","content":"## üõ†Ô∏è Task: Mirror shared utils with language-specific doc folders\n\nCreate documentation folders that mirror the utilities in `shared/`, grouped by their implementation language. This will make it easier to reference cross-language helpers in the design docs.\n\n---\n\n## üéØ Goals\n\n- Align documentation structure with the `shared/` directory\n- Provide examples for Python, Hy, and Sibilant/JS utilities\n- Improve discoverability of common helper functions\n\n---\n\n## üì¶ Requirements\n\n- [ ] Audit `shared/` to list existing utility modules\n- [ ] Create matching folders under `docs/shared/` for each language\n- [ ] Generate README stubs summarizing available utilities\n\n---\n\n## üìã Subtasks\n\n- [ ] Draft initial README for Python utilities\n- [ ] Draft initial README for Sibilant/JS utilities\n- [ ] Link these docs from `docs/file-structure.md`\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [file structure](../file-structure.md)\n- [[kanban]]\n#done\n\n"}
{"id":"c80af308-7b65-4dbf-92b9-d464308b4565","title":"docops pipeline","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.500Z","uuid":"c80af308-7b65-4dbf-92b9-d464308b4565","created_at":"2025-09-15T02:02:58.500Z","path":"docs/agile/tasks/Docops pipeline.md","content":"## üõ†Ô∏è Task: Complete Docops pipeline (synthesis agent)\n\nRun the synthesis agent over the files in `docs/unique/` to generate more polished documentation. The agent should create drafts in the appropriate design subfolders.\n\n---\n\n## üéØ Goals\n\n- Transform raw notes in `unique/` into coherent design docs\n- Keep generated drafts separate from hand-written notes\n- Provide a repeatable command for future runs\n\n---\n\n## üì¶ Requirements\n\n- [ ] Identify which notes should be processed\n- [ ] Output drafts to `docs/design/drafts/` or similar\n- [ ] Document the workflow in `docs/Process.md`\n\n---\n\n## üìã Subtasks\n\n- [x] Configure synthesis-agent to read from `labeled/` (staged via `inbox/` ‚Üí `doc-stage`) ‚úÖ 2025-09-18\n- [x] Export cleaned markdown to new folder ‚úÖ 2025-09-26\n- [ ] Link resulting docs on the kanban board\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[process]]\n- [[kanban]]\n#IceBox\n#ready\n\n"}
{"id":"c858464b-c43b-4521-80b9-12fa3d5cd464","title":"llm service must accept tool calls","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.516Z","uuid":"c858464b-c43b-4521-80b9-12fa3d5cd464","created_at":"2025-09-15T02:02:58.516Z","path":"docs/agile/tasks/llm-service-must-accept-tool-calls.md","content":"## üõ†Ô∏è Description\n\nExtend the LLM service to support tool/function calls so agents can request structured actions.\n\n---\n\n## üéØ Goals\n\n- Allow clients to register callable tools with schema\n- Route tool invocation requests through broker to appropriate service\n\n---\n\n## üì¶ Requirements\n\n- [ ] API accepts OpenAI-style tool definitions\n- [ ] Service returns tool call payloads when requested\n- [ ] Executed tool results fed back into conversation flow\n- [ ] Tests cover tool registration and invocation\n\n---\n\n## üìã Subtasks\n\n- [ ] Define tool-call schema in bridge and LLM service\n- [ ] Implement tool selection logic in LLM adapter\n- [ ] Integrate broker messaging for tool execution\n- [ ] Add end-to-end tests for sample tool\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#framework-core #Ready #ready\n\n"}
{"id":"c9ef64a0-d3f6-49e0-911e-4ce722f9e057","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"c9ef64a0-d3f6-49e0-911e-4ce722f9e057","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/consolidate-all-configs-into---config--as--.edn--files.md","content":"# Description\n\nDescribe your task\n\n## Requirements/Definition of done\n\n- If it doesn't have this, we can't accept it\n\n## Tasks \n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Relevent resources\n\nYou might find [this] useful while working on this task\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n"}
{"id":"cb45d915-0601-4df8-997d-71ca913ad9f6","title":"cephalon persist llm replies to agent messages","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.509Z","uuid":"cb45d915-0601-4df8-997d-71ca913ad9f6","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/cephalon_persist_llm_replies_to_agent_messages.md","content":"Cephalon: Persist LLM replies to `agent_messages`\n\nGoal: When the ECS orchestrator receives an LLM reply (current handler in `services/ts/cephalon/src/bot.ts` subscribing to `agent.llm.result`), persist the reply text and metadata to the `agent_messages` collection via `ContextManager`/`CollectionManager`.\n\nWhy: The agent currently speaks without storing their utterances, breaking conversational continuity and context reconstruction.\n\nScope:\n- Add a persistence hook right before enqueuing the utterance (preferred) or when the audio finishes.\n- Metadata: `{ userName: AGENT_NAME, agentMessage: true, channel, recipient, startTime, endTime? }`.\n- Ensure `ContextManager` has `agent_messages` created before use (already in `Bot.start()`).\n\nExit Criteria:\n- LLM reply text stored in `agent_messages` on each response.\n- Unit test simulating `agent.llm.result` that asserts Mongo insert and Chroma add.\n\n#incoming #cephalon #persistence #agent_messages\n\n"}
{"id":"cbc70baa-3202-4eb0-a4ef-aec84d3e3bbf","title":"Task: Structural code editing AI tool (tree-diffs, not text-diffs) ‚Äî with immediate validation","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.521Z","uuid":"cbc70baa-3202-4eb0-a4ef-aec84d3e3bbf","created_at":"2025-09-15T02:02:58.521Z","path":"docs/agile/tasks/structural-code-editing-ai-tool-that-uses-tree-diffs-instead-of-string-based-universal-diffs--ensuring-generated-results-are-always-immediately-validatable.md","content":"# Task: Structural code editing AI tool (tree-diffs, not text-diffs) ‚Äî with immediate validation\n\n**Slug:** `structural-code-editing-ai-tool-that-uses-tree-diffs-instead-of-string-based-universal-diffs--ensuring-generated-results-are-always-immediately-validatable`\n**Type:** Feature / R\\&D spike ‚Üí MVP\n**Status:** Proposed\n**Owner:** Err\n**Priority:** High\n**Context:** We need robust, deterministic edits that don‚Äôt explode at compile time. Stop letting agents ‚Äúguess-and-pray‚Äù with string diffs. Operate on ASTs, emit structural patches, and refuse to output anything that can‚Äôt be validated immediately.\n\n---\n\n## Problem statement\n\nString-based diffs are brittle. They break on formatting, comments, or tiny layout changes, and they‚Äôre impossible to validate mid-generation. We want an AI-driven **structural editor** that (1) plans edits at the AST level, (2) applies them via language-aware transforms, (3) formats, **type-checks**, and **test-checks** the result, and (4) only returns a patch if it‚Äôs **immediately validatable** (parse clean, passes formatter, passes lints, and at least compiles/types). If validation fails, the tool iterates or aborts with a structured failure report.\n\n---\n\n## Outcome / definition of done\n\n* A **CLI + library** (Node/TS) that:\n\n  * Accepts an **edit intent** (JSON) or natural-language instruction.\n  * Produces an **AST patch plan** (insert/replace/move/rename) using language adapters.\n  * Applies the patch to a repo **in an overlay/worktree**, then runs:\n\n    * Parser round-trip (parse ‚Üí print ‚Üí parse) = stable.\n    * Prettier format pass = clean (no diff after second run).\n    * ESLint (configurable) = no new errors beyond baseline.\n    * TypeScript `tsc --noEmit` = no new type errors beyond baseline.\n    * Optional: test shard selection (e.g., Vitest/Jest on impacted files) = green or no worse than baseline.\n  * Emits:\n\n    * A **structural diff** (AST ops, JSON).\n    * A **text patch** (git-applyable).\n    * A **validation report** (machine-readable + human summary).\n* MVP supports **TypeScript/JavaScript first** with **ts-morph**/**TypeScript Compiler API**. Hooks exist for adding Python (LibCST), and others later.\n* All generated outputs are **deterministic** given the same repo state + instruction.\n* Clear **failure modes**: when validation fails, the CLI exits non-zero and prints a minimal repro (why it failed, where).\n\n---\n\n## Non-goals (for MVP)\n\n* Full multi-language parity. Start with TS/JS.\n* Fancy refactors spanning mixed-language graphs. Keep boundaries clean.\n* LSP server implementation. (We can integrate with one later.)\n* In-IDE UX. Ship a CLI/library first.\n\n---\n\n## Approach (skeptical + practical)\n\n1. **Language adapter (TS/JS)**\n\n   * Use **ts-morph** for AST ops (rename symbol, move declaration, change signature, add import/export, insert call, wrap/unwrap node).\n   * Export a minimal **AST Patch Schema** with idempotent ops:\n\n     * `InsertNode(parentSelector, position, nodeSpec)`\n     * `ReplaceNode(selector, nodeSpec)`\n     * `DeleteNode(selector)`\n     * `MoveNode(selector, newParentSelector, position)`\n     * `RenameSymbol(symbolSelector, newName)`\n     * `UpdateImport(from, add: [], remove: [])`\n   * **Selectors** are structural: by symbol name + file + kind + path (e.g., file glob + top-level function named X). Avoid brittle character offsets.\n\n2. **Patch planner**\n\n   * Natural-language ‚Üí constrained plan via small rules engine:\n\n     * Map common intents (‚Äúrename function,‚Äù ‚Äúextract method,‚Äù ‚Äúmove to file,‚Äù ‚Äúconvert default export to named‚Äù) into **canonical AST ops**.\n   * Refuse ambiguous plans: if multiple targets match a selector, abort with disambiguation hints.\n\n3. **Application + round-trip**\n\n   * Apply ops to an **ephemeral worktree** (git worktree or memfs ‚Üí write to temp dir).\n   * **Print ‚Üí parse ‚Üí print** check on touched files to ensure AST stability.\n\n4. **Validation pipeline (must-pass gates)**\n\n   * Prettier ‚Üí eslint (no new errors) ‚Üí `tsc --noEmit` ‚Üí impacted tests.\n   * Gate policy configurable, but default is strict: fail-fast on any new error.\n\n5. **Outputs**\n\n   * `*.astpatch.json` (structural ops)\n   * `*.diff` (git patch)\n   * `validation.json` + short human summary\n   * Optional: **explain** file: why these ops, affected symbols, risk notes.\n\n6. **Safety rails**\n\n   * Never touch working tree by default. Require `--apply` to write real changes.\n   * Provide `--baseline` capture, then compare post-state to baseline.\n\n7. **Metrics**\n\n   * Track plan attempts, validation failures, time per gate, final outcome. Useful for improving the planner.\n\n---\n\n## CLI sketch\n\n```\nsceet  # structural code editing tool\n\nUsage:\n  sceet plan --repo . --intent intent.json --out outdir\n  sceet apply --repo . --patch outdir/plan.astpatch.json --validate\n  sceet validate --repo . --since HEAD --report outdir/validation.json\n  sceet all --repo . --intent intent.json --emit-patch --validate\n```\n\n* Flags:\n\n  * `--strict` (default): fail on any new lint/type error.\n  * `--tests impacted|none|all` (default: impacted).\n  * `--formatter prettier` (default).\n  * `--overlay worktree|tempdir` (default: tempdir).\n  * `--emit {text|structural|both}`.\n\n---\n\n## Minimal AST Patch Schema (JSON)\n\n```json\n{\n  \"version\": \"1\",\n  \"language\": \"typescript\",\n  \"ops\": [\n    {\n      \"op\": \"RenameSymbol\",\n      \"selector\": { \"file\": \"src/utils/math.ts\", \"kind\": \"Function\", \"name\": \"sum\" },\n      \"newName\": \"add\"\n    },\n    {\n      \"op\": \"UpdateImport\",\n      \"file\": \"src/index.ts\",\n      \"from\": \"./utils/math\",\n      \"add\": [\"add\"],\n      \"remove\": [\"sum\"]\n    }\n  ]\n}\n```\n\n* **Invariant:** After applying ops, parse+format+typecheck must succeed or the plan is rejected.\n\n---\n\n## Acceptance criteria (must all pass)\n\n* [ ] Running `sceet all` on a sample repo to **rename a public function** across multiple files:\n\n  * Emits a structural patch with `RenameSymbol` + `UpdateImport` ops.\n  * Text patch applies cleanly with `git apply`.\n  * `prettier --check` shows no changes after a second run.\n  * `eslint` yields **no new** errors vs baseline.\n  * `tsc --noEmit` yields **no new** errors vs baseline.\n  * Impacted tests pass (or unchanged vs baseline).\n* [ ] A second scenario: **move function to a new module** (update imports/exports, adjust references) passes the same gates.\n* [ ] A third scenario: **wrap function return type** (e.g., add `Result<T, E>`) demonstrates failure handling with a clear validation report when downstream types aren‚Äôt updated.\n* [ ] Clear **failure report** includes selectors and suggested next steps.\n* [ ] Tool never mutates the working tree unless `--apply` is provided.\n\n---\n\n## Milestones\n\n1. **MVP TS/JS adapter**\n\n   * ts-morph transforms (rename/move/update imports/exports)\n   * AST Patch Schema v1\n   * Overlay filesystem + round-trip parse check\n\n2. **Validation pipeline**\n\n   * Prettier + ESLint + TSC baseline comparison\n   * Impacted test detection (simple: glob by import graph via tsconfig paths)\n\n3. **Planner v0**\n\n   * Map a fixed set of intents to ops\n   * Ambiguity detection + fail with hints\n\n4. **CLI + reports**\n\n   * Human + JSON reports\n   * Emit both structural + text patches\n\n5. **Hardening**\n\n   * Idempotency tests, snapshot tests, repo-agnostic E2E suite\n\n---\n\n## Risks & mitigations\n\n* **Selector brittleness:** If names collide, selectors match too much.\n  ‚Üí Add disambiguators (file path, symbol kind, export-ness) and abort on multi-match.\n* **Formatter churn hides real diffs:**\n  ‚Üí Enforce format-first, diff after format.\n* **Type cascades make small edits ‚Äúbig‚Äù:**\n  ‚Üí Start with low-blast-radius ops (rename/move/import updates). Gate more invasive ops behind a flag.\n* **Cross-project TS config weirdness (path aliases, project refs):**\n  ‚Üí Resolve via the TS compiler API using the repo‚Äôs real `tsconfig.json` to mirror build.\n\n---\n\n## Deliverables\n\n* `/tools/sceet/` (TypeScript package)\n\n  * `src/adapter/ts/‚Ä¶` (ts-morph)\n  * `src/planner/‚Ä¶`\n  * `src/patch/‚Ä¶` (schema + apply)\n  * `src/validate/‚Ä¶` (prettier, eslint, tsc, tests)\n  * `src/cli.ts`\n* `examples/` repo with 3 scenarios and golden snapshots.\n* `docs/STRUCTURAL_DIFFS.md` (schema + invariants + examples).\n\n---\n\n## Nice-to-haves (post-MVP)\n\n* Python adapter with **LibCST** (safe codemods).\n* Language-server bridge (expose as MCP/LSP tool).\n* Graph-aware impact analysis (tsserver, project references).\n* ‚ÄúSelf-repair‚Äù loop: collect validator errors ‚Üí plan additional ops ‚Üí re-validate (bounded retries).\n\n---\n\n## How we‚Äôll judge success (blunt criteria)\n\n* **Zero** new parser/type/lint errors on successful runs.\n* **Deterministic**: same input ‚Üí same patch.\n* **Human-reviewable**: structural diff readable, text patch minimal.\n* **Abort early** on ambiguity or validation failure, with useful hints.\n\n---\n\n## Example intents (for planner tests)\n\n* ‚ÄúRename function `sum` to `add` across the project; keep exports intact.‚Äù\n* ‚ÄúMove `parseConfig` from `src/util/config.ts` to `src/config/index.ts` and update imports.‚Äù\n* ‚ÄúConvert default export in `src/api/client.ts` to named export `createClient` and update consumers.‚Äù\n\n---\n\n## Integration hooks\n\n* Pre-commit optional: `sceet validate --since HEAD`\n* CI job: run on PRs to block text-only AI diffs without structural equivalence.\n\n---\n\n#promethean #cephalon #devtools #codemod #ast #typescript #ts-morph #validation #prettier #eslint #tsc #testing #mcp #cli #task #architecture #refactor #agent-mode\n\n```smart-connections\n{\n  \"expanded_view\": false,\n  \"render_markdown\": true,\n  \"show_full_path\": false,\n  \"exclude_blocks_from_source_connections\": false,\n  \"exclude_frontmatter_blocks\": true,\n  \"results_limit\": \"20\",\n  \"exclude_inlinks\": false,\n  \"exclude_outlinks\": false\n}\n```\n\n"}
{"id":"cc373a25-f288-4def-8ced-b824cc72c06a","title":"Remove `any` types across packages","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.518Z","uuid":"cc373a25-f288-4def-8ced-b824cc72c06a","created_at":"2025-09-15T02:02:58.518Z","path":"docs/agile/tasks/remove_any_types_across_packages.md","content":"# Remove `any` types across packages\n\n## Description\n`pnpm -r lint` reports many `noExplicitAny` warnings in packages like `agent`, `codex-context`, and `compiler`.\n\n## Goals\n- Improve type safety by replacing `any` with explicit types.\n\n## Requirements\n- Identify all `noExplicitAny` lint warnings.\n- Refactor code to use specific types or generics.\n\n## Subtasks\n- [ ] Replace `any` usages in `packages/agent`.\n- [ ] Replace `any` usages in `packages/codex-context` tests and types.\n- [ ] Replace `any` usages in `packages/compiler`.\n\n#Todo #codex-task\n\n"}
{"id":"cd2f96f3-bd40-410b-94f2-f7d0dbce4da4","title":"update github actions automation to pnpm scripts","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.523Z","uuid":"cd2f96f3-bd40-410b-94f2-f7d0dbce4da4","created_at":"2025-09-15T02:02:58.523Z","path":"docs/agile/tasks/update_github_actions_to_use_makefile.md","content":"## üõ†Ô∏è Task: Update GitHub Actions to use pnpm scripts\n\nThe Makefile targets referenced by the original plan never stabilized. CI already leans on pnpm (`pnpm lint`, `pnpm test:all`),\nso workflows should call the same scripts instead of Makefile wrappers.\n\n## ‚úÖ Decision\n- Replace all Makefile references with pnpm workspace commands.\n- Keep workflow steps close to the developer experience (`pnpm lint`, `pnpm test:all`, `pnpm --filter ‚Ä¶ build`).\n\n---\n\n## üéØ Goals\n- Invoke pnpm scripts for lint/build/test in every workflow.\n- Remove Makefile-specific setup instructions from CI docs.\n- Ensure package-scoped tests run via pnpm filters where relevant.\n\n---\n\n## üì¶ Requirements\n- [ ] Audit current workflow steps for Makefile usage.\n- [ ] Swap `make lint|test|build` with pnpm equivalents.\n- [ ] Align documentation/README references with pnpm tooling.\n- [ ] Verify environment variables and caching remain intact after changes.\n\n---\n\n## üìã Subtasks\n- [ ] Identify pnpm scripts that replace each Makefile target.\n- [ ] Update YAML workflows accordingly.\n- [ ] Update contributing or CI docs describing the pipeline.\n\n---\n\n## üîó Related Epics\n#cicd #devops #framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n- [[breakdown-makefile.hy|audit makefile.hy remnants and confirm deprecation]]\n\n## ‚õìÔ∏è Blocks\n- [[update_makefile_to_have_commands_specific_for_agents_md|replace agent automation makefile targets with pnpm scripts]]\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n- [[process]]\n- [scripts/dev.mjs](../../scripts/dev.mjs)\n\n#cicd #devops #framework-core #todo\n"}
{"id":"ce8a026c-7d8c-41fe-9f48-e1b6a36e5dc0","title":"design vision pipeline mvp","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.511Z","uuid":"ce8a026c-7d8c-41fe-9f48-e1b6a36e5dc0","created_at":"2025-09-15T02:02:58.511Z","path":"docs/agile/tasks/design-vision-pipeline-mvp.md","content":"<hr class=\"__chatgpt_plugin\">\n\n### role::assistant<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n\n---\npromptId: smart_task_templater_md\nname: \"{{title}}\"\ndescription: A task on the kanban of the promethean system.\nauthor: Aaron Beavers\ntags:\n  - prompt-refinement\n  - promptcompiler\n  - \"#metaprogramming\"\n  - \"#metacompiler\"\nversion: 0.0.1\ndisableProvider: false\nprovider: ollama\ncommands: generate\nmode: replace\nstreaming: \"true\"\nmodel: ollama@llama3.2:latest\nprompt: \"{{title}}\"\ntask-id: TASK-{{date}}\npriority: p3\nsystem_commands: ['You are a helpful assistant.']\nfrequency_penalty: 0\nmax_tokens: 400\npresence_penalty: 0\nstream: true\ntemperature: 0.7\n---\n<hr class=\"__chatgpt_plugin\">\n\n<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n### role::assistant\n\nHere's a suggested revision of your context section:\n\n## Context\n### Changes and Updates\n- **What changed?**: [Describe the key changes that have occurred, e.g., updated requirements, new feature added]\n- **Where?**: [Specify the location or scope where these changes are relevant, e.g., specific project, department]\n- **Why now?**: [Explain why these changes are necessary at this time, e.g., due to deadline, feedback from stakeholders]\n\n## Inputs / Artifacts\n- ([link or path] to relevant documentation, data, or assets)\n\n## Definition of Done\n- [ ] test X passes: [ Briefly describe the testing scenario]\n- [ ] doc Y updated: [Mention the specific documentation or resource that has been updated]\n- [ ] PR merged: ([link to the PR] with a brief summary of changes)\n\n## Plan\n1. ‚Ä¶ [ Outline the high-level steps for completing this task, including any dependencies or resources required]\n2. ‚Ä¶\n\n## Relevant Resources\n\nYou might find [this](link) useful while working on this task.\n\n### Smart Connections Configuration\n```smart-connections\n{\n  \"render_markdown\": true,\n  \"show_full_path\": false,\n  \"exclude_blocks_from_source_connections\": false,\n  \"exclude_frontmatter_blocks\": true,\n  \"expanded_view\": false,\n  \"results_limit\": \"20\",\n  \"exclude_inlinks\": false,\n  \"exclude_outlinks\": false\n}\n```\n\n### Smart ChatGPT Configuration\n```smart-chatgpt\n```\n<hr class=\"__chatgpt_plugin\">\n\n\n### role::user\n\n### role::assistant <span style=\"font-size: small;\"> (llama3.2:latest)</span>\n\n\n### role::user\n\nUsing this title write a reasonable task kanban task document:\ndesign-vision-pipeline-mvp\n\ntrue\n\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\n\n"}
{"id":"d158980f-ef8c-4236-85be-c494c4aa9159","title":"define codex baseg agent md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"d158980f-ef8c-4236-85be-c494c4aa9159","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/define_codex_baseg_agent.md","content":"## üõ†Ô∏è Description\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- What are we trying to accomplish?\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#IceBox\n\n"}
{"id":"d1efce9d-cff7-4f99-baeb-143b9edf981a","title":"migrate server side sibilant libs to promethean architecture md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.516Z","uuid":"d1efce9d-cff7-4f99-baeb-143b9edf981a","created_at":"2025-09-15T02:02:58.516Z","path":"docs/agile/tasks/migrate_server_side_sibilant_libs_to_promethean_architecture.md","content":"## üõ†Ô∏è Task: Migrate server side sibilant libs to promethean architecture\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Migrate server side sibilant libs to promethean architecture\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#ready\n\n"}
{"id":"d21a00c1-6f9d-43d6-803c-71ab6134b85f","title":"add ollama formally to pipeline md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.505Z","uuid":"d21a00c1-6f9d-43d6-803c-71ab6134b85f","created_at":"2025-09-15T02:02:58.505Z","path":"docs/agile/tasks/add_ollama_formally_to_pipeline.md","content":"## üõ†Ô∏è Task: Add Ollama formally to pipeline\n\nGenerated from [../unique/2025.07.28.18.07.20.md](../unique/2025.07.28.18.07.20.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"d35d985b-44c2-4b67-9228-bc2cb912df24","title":"setup code in wsl md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.521Z","uuid":"d35d985b-44c2-4b67-9228-bc2cb912df24","created_at":"2025-09-15T02:02:58.521Z","path":"docs/agile/tasks/setup_code_in_wsl.md","content":"## üõ†Ô∏è Description\n\nThis will give me a chance to test  out the  quick setup methods  in an  environment similar to the codex agent, and smooth  out some  issues we have   in development\n\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Add Ollama formally to pipeline\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#accepted\n\n"}
{"id":"d3e6cca5-2b2f-4cd9-8131-db74ebe7c8f9","title":"Clean up useless regex escape in agent package","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.509Z","uuid":"d3e6cca5-2b2f-4cd9-8131-db74ebe7c8f9","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/cleanup_useless_regex_escape_in_agent_package.md","content":"# Clean up useless regex escape in agent package\n\n## Description\nThe `noUselessEscapeInRegex` rule flagged an unnecessary escape in `packages/agent/src/policy.ts`.\n\n## Goals\n- Remove redundant escape sequences to simplify the regex.\n\n## Requirements\n- Adjust the pattern in `globToRegExp` to avoid escaping characters that don't require it.\n- Verify lint passes after the change.\n\n## Subtasks\n- [ ] Update regex in `packages/agent/src/policy.ts`.\n- [ ] Ensure associated tests still pass.\n\n#Todo #codex-task\n\n"}
{"id":"d4574bbb-98a3-40ae-a335-0ff2123b4c57","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"d4574bbb-98a3-40ae-a335-0ff2123b4c57","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/connect-reddit.md","content":"# Description\n\nIntegrate Reddit so agents can read and post subreddit content using the Reddit REST API with OAuth2.\n\n## Target APIs and Authentication\n\n- **API**: [Reddit API](https://www.reddit.com/dev/api)\n- **Authentication**: OAuth2 via client ID/secret and refresh token stored in service configuration\n\n## Data Flow & Rate Limiting\n\n- Agent queues request ‚ûú OAuth client ‚ûú Reddit endpoint ‚ûú internal broker/storage\n- Respect Reddit rate limits (~60 requests/minute per app) and backoff on `429` or `Retry-After` headers\n\n## Requirements/Definition of done\n## Requirements\n\n- Authenticated requests can fetch and post subreddit data\n- Data flows through the broker into internal storage\n- Rate limiting and backoff logic prevents API throttling\n\n## Tasks\n## Definition of Done\n\n- [ ] Register Reddit application and configure OAuth2 credentials\n- [ ] Implement OAuth client and endpoint wrappers\n- [ ] Stream or poll subreddit content into internal queue\n- [ ] Implement rate limit handler with exponential backoff\n- [ ] Integration test covering happy path and 429 handling\n- [ ] Unit tests for data mapping and error cases\n- [ ] Reddit API client authenticates using configured credentials\n- [ ] Posts can be fetched from a target subreddit\n- [ ] Basic unit test covers fetching and storing a post\n\n## Story Points\n\n3\n\n## Tasks\n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Dependencies\n\n- Reddit API access and credentials\n- OAuth2 authentication library\n\n## Rough Scope\n\n- Set up API client with OAuth2 flow\n- Implement fetch and post operations for target subreddits\n- Integrate results with existing context pipeline\n\n## Estimate\n\n- Story points: 3\n\n## Relevent resources\n\nYou might find [Reddit API docs](https://www.reddit.com/dev/api) useful while working on this task\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n\n#Ready\n\nNo blockers.\n\n#Ready\n#ready\n\n"}
{"id":"d474baf2-e4ff-4771-9e51-11775c186fe6","title":"each service registers a pid with a heartbeat service if they do not successfully check in terminate the process using the pid md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"d474baf2-e4ff-4771-9e51-11775c186fe6","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/each_service_registers_a_pid_with_a_heartbeat_service_if_they_do_not_successfully_check_in_terminate_the_process_using_the_pid.md","content":"## üõ†Ô∏è Description\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- What are we trying to accomplish?\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n\n#archive\n\n"}
{"id":"d51d697f-2c46-42f1-a402-694f9f376bca","title":"add starter notes eidolon fields cephalon inner monologue 1 md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.505Z","uuid":"d51d697f-2c46-42f1-a402-694f9f376bca","created_at":"2025-09-15T02:02:58.505Z","path":"docs/agile/tasks/add_starter_notes_-_eidolon_fields_cephalon_inner_monologue_1.md","content":"## üõ†Ô∏è Task: Add starter notes - eidolon_fields, cephalon_inner_monologue\n\nCreate initial documentation for two key areas of the emotional model.\n`eidolon_fields.md` should outline the concept of the **Eidolon Field** and\ndescribe each layer‚Äôs purpose. `cephalon_inner_monologue.md` introduces the\nCephalon agent‚Äôs reasoning loop and how fragments are stored for reflection.\nWrite introductory documentation for two core concepts: **eidolon_fields** and\n**cephalon_inner_monologue**. These notes will serve as a reference for future\nimplementation work in the Eidolon and Cephalon services. Place the notes in\n`docs/notes/` so they are available to the vault and link them from the\n`docs/unique/index.md` file.\n\n---\n\n## üéØ Goals\n\n- Establish clear introductory docs for the emotional framework\n- Highlight how the Eidolon Field layers map to emotional dimensions\n- Explain Cephalon's internal monologue cycle at a high level\n- Provide a high level description of what **eidolon_fields** represent and how\nthey influence emotional state.\n- Document the purpose of **cephalon_inner_monologue** and how it interacts with\nthe Eidolon field during agent reasoning.\n- Establish a canonical location in `docs/notes/` for further deep dives.\n\n---\n\n## üì¶ Requirements\n- [ ] Create `docs/notes/eidolon_fields.md` with an overview section and bullet\nlist of key properties.\n- [ ] Create `docs/notes/cephalon_inner_monologue.md` describing the monologue\nloop and its inputs/outputs.\n- [ ] Link both notes from `docs/unique/index.md` so they appear in the\nknowledge base.\n- [ ] Keep each note under 2 pages of markdown with clear headings.\n\n- [ ] Include reference links to `pseudo/eidolon-field-scratchpad.lisp`\n- [ ] Use [wikilinks](wikilinks.md) so notes connect to existing math docs\n\n---\n\n## üìã Subtasks\n- [ ] Draft each note in the vault using [wikilinks](wikilinks.md) for related concepts.\n- [ ] Convert wikilinks to standard markdown before committing.\n- [ ] Commit notes and update the index file.\n\n- [ ] Draft bullet-point summaries for each layer (8 total)\n- [ ] Describe Cephalon's reflection loop and memory storage\n- [ ] Commit the new notes and link them from relevant tasks\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#done\n\n"}
{"id":"d524ff64-8f12-46bb-8c28-103e13743fa1","title":"schedule alignment meeting with stakeholders md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.519Z","uuid":"d524ff64-8f12-46bb-8c28-103e13743fa1","created_at":"2025-09-15T02:02:58.519Z","path":"docs/agile/tasks/schedule_alignment_meeting_with_stakeholders.md","content":"## üõ†Ô∏è Task: Schedule alignment meeting with stakeholders\n\nOrganize a meeting to discuss the open questions about Promethean and confirm project goals.\n\n---\n\n## üéØ Goals\n- Confirm availability of key stakeholders\n- Share agenda with collected questions\n- Record meeting outcomes\n\n---\n\n## üì¶ Requirements\n- [ ] Meeting invite sent to all stakeholders\n- [ ] Agenda posted to `docs/notes/meeting-agendas/`\n\n---\n\n## üìã Subtasks\n- [ ] Poll stakeholders for available times\n- [ ] Finalize and distribute meeting agenda\n- [ ] Document meeting notes\n\n---\n\n## üîó Related Epics\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n- Gather open questions about system direction\n\n## ‚õìÔ∏è Blocks\n- Summarize clarified priorities for next sprint\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n#ice-box\n\n"}
{"id":"d54f5640-9a46-4643-a76c-020603439c39","title":"set up proper openai custom gpt compatable oauth login flow","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.520Z","uuid":"d54f5640-9a46-4643-a76c-020603439c39","created_at":"2025-09-15T02:02:58.520Z","path":"docs/agile/tasks/set-up-proper-openai-custom-gpt-compatable-oauth-login-flow.md","content":"## üõ†Ô∏è Description\n**Status:** blocked\n\nImplement a secure OAuth flow for OpenAI custom GPT integrations, including redirect handling and token management.\n\n---\n\n## üéØ Goals\n\n- Allow users to authenticate with OpenAI via OAuth.\n- Ensure minimal scopes and safe token storage.\n\n---\n\n## üì¶ Requirements\n\n- [ ] OAuth client with configurable redirect URI.\n- [ ] Use PKCE or similar protection.\n- [ ] Persist and refresh tokens securely.\n- [ ] Document setup and environment variables.\n\n---\n\n## üìã Subtasks\n\n- [ ] Research OpenAI OAuth endpoints and required scopes.\n- [ ] Implement authorization request and callback handler.\n- [ ] Store tokens in vault or encrypted config.\n- [ ] Validate flow end-to-end with a demo client.\n- [ ] Write README instructions for configuring the flow.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#accepted\n\n## Blockers\n- No active owner or unclear scope\n\n#breakdown\n\n"}
{"id":"d6204735-5a96-4670-a361-6f6ef7c5cdb6","title":"implement fragment ingestion with activation vecto md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.515Z","uuid":"d6204735-5a96-4670-a361-6f6ef7c5cdb6","created_at":"2025-09-15T02:02:58.515Z","path":"docs/agile/tasks/implement_fragment_ingestion_with_activation_vecto.md","content":"## üõ†Ô∏è Task: Implement fragment ingestion with activation vectors\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Implement fragment ingestion with activation vectors\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#ice-box\n\n"}
{"id":"d70ed8c5-4885-450f-b1bc-9f8d6b0e94f2","title":"docker compose edn template system","status":"todo","priority":"p3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"d70ed8c5-4885-450f-b1bc-9f8d6b0e94f2","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/docker-compose-edn-template-system.md","content":"<hr class=\"__chatgpt_plugin\">\n\n<span style=\"font-size: small;\"> (llama3.2:latest)</span>\n### role::assistant\n\nHere's a suggested revision of your context section:\n\n## Context\n### Changes and Updates\n- **What changed?**: [Describe the key changes that have occurred, e.g., updated requirements, new feature added]\n- **Where?**: [Specify the location or scope where these changes are relevant, e.g., specific project, department]\n- **Why now?**: [Explain why these changes are necessary at this time, e.g., due to deadline, feedback from stakeholders]\n\n## Inputs / Artifacts\n- ([link or path] to relevant documentation, data, or assets)\n\n## Definition of Done\n- [ ] test X passes: [ Briefly describe the testing scenario]\n- [ ] doc Y updated: [Mention the specific documentation or resource that has been updated]\n- [ ] PR merged: ([link to the PR] with a brief summary of changes)\n\n## Plan\n1. ‚Ä¶ [ Outline the high-level steps for completing this task, including any dependencies or resources required]\n2. ‚Ä¶\n\n## Relevant Resources\n\nYou might find [this](link) useful while working on this task.\n\n### Smart Connections Configuration\n```smart-connections\n{\n  \"render_markdown\": true,\n  \"show_full_path\": false,\n  \"exclude_blocks_from_source_connections\": false,\n  \"exclude_frontmatter_blocks\": true,\n  \"expanded_view\": false,\n  \"results_limit\": \"20\",\n  \"exclude_inlinks\": false,\n  \"exclude_outlinks\": false\n}\n```\n\n### Smart ChatGPT Configuration\n```smart-chatgpt\n```\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\nUsing this title write a reasonable task kanban task document:\ndocker-compose-edn-template-system\n\ntrue \n\n\n<hr class=\"__chatgpt_plugin\">\n\n### role::user\n\n\n"}
{"id":"d83c4f0e-336d-4957-9f6e-7aff774e7eb9","title":"rewrite build tools with bb","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.519Z","uuid":"d83c4f0e-336d-4957-9f6e-7aff774e7eb9","created_at":"2025-09-15T02:02:58.519Z","path":"docs/agile/tasks/rewrite-build-tools-with-bb.md","content":"#incoming\n\n"}
{"id":"d91c26e9-a20a-4e96-a7e9-913bda7c54d1","title":"clean up notes into design docs md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.509Z","uuid":"d91c26e9-a20a-4e96-a7e9-913bda7c54d1","created_at":"2025-09-15T02:02:58.509Z","path":"docs/agile/tasks/clean_up_notes_into_design_docs.md","content":"## üõ†Ô∏è Description\n\nConvert the DocOps/Ollama working notes into canonical design documentation. Source material lives in:\n- [[docs/inbox/2025.09.19.16.04.44|DocOps inbox dump (2025-09-19)]]\n- [[docops-feature-updates|DocOps feature updates]]\n- [[promethean-documentation-pipeline-overview|Promethean documentation pipeline overview]]\n- [[functional-embedding-pipeline-refactor|Functional embedding pipeline refactor]]\n\nUse the shared design doc template at [[file.doc.template|docs/templates/file.doc.template.md]] when drafting updates. As content is migrated, annotate the [[unique-info-dump-index|unique info dump index]] so we keep track of what has been normalized.\n\nTarget destinations for the cleaned material:\n- [DocOps pipeline design doc](../../design/docops-pipeline.md)\n- [Ollama pipeline architecture doc](../../architecture/ollama-pipeline.md)\n\nCoordinate deliverables with the follow-up task [[add_ollama_formally_to_pipeline_md_md|add ollama formally to pipeline md]] so downstream dependencies stay aligned with the refreshed docs.\n\n---\n\n## üéØ Goals\n\n- Capture DocOps pipeline requirements and current state in `docs/design/docops-pipeline.md` using the shared template framing.\n- Document Ollama integration decisions and interfaces in `docs/architecture/ollama-pipeline.md` sourced from the referenced notes.\n- Ensure the follow-up \"add ollama formally to pipeline\" task has clear entry points and dependencies documented.\n\n---\n\n## üì¶ Requirements\n\n- [ ] Restructure each target design doc to match [[file.doc.template|docs/templates/file.doc.template.md]] while incorporating insights from all four source notes.\n- [ ] Update [[unique-info-dump-index|docs/notes/unique-info-dump-index.md]] with links back to the normalized sections to prevent duplicate conversions.\n- [ ] Flag any additional work needed directly in [[add_ollama_formally_to_pipeline_md_md|add ollama formally to pipeline md]] so the dependency reflects the refreshed documentation.\n\n---\n\n## üìã Subtasks\n\n- [ ] Inventory overlaps and deltas across the four source notes to determine canonical sections for each design doc.\n- [ ] Draft updates to `docs/design/docops-pipeline.md` and `docs/architecture/ollama-pipeline.md` using the shared template, citing the specific note paragraphs pulled forward.\n- [ ] Cross-link the updated design docs from the unique info dump index and notify the follow-up task of the new structure.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#agent-thinking\n#accepted\n"}
{"id":"db083776-2697-432a-8aba-3e4639648867","title":"gather baseline emotion metrics for eidolon field 1 md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.514Z","uuid":"db083776-2697-432a-8aba-3e4639648867","created_at":"2025-09-15T02:02:58.514Z","path":"docs/agile/tasks/gather_baseline_emotion_metrics_for_eidolon_field_1.md","content":"## üõ†Ô∏è Task: Gather baseline emotion metrics for Eidolon field\n\nCollect initial emotional state data to seed analysis and reward calculations.\nThese numbers provide a reference point for future reinforcement logic.\nThe dataset should provide a snapshot of typical emotional vectors generated by\nthe Eidolon service under neutral conditions. Future evaluation tasks will use\nthis baseline to detect deviations and compute rewards.\n\n---\n\n## üéØ Goals\n\n- Provide a reproducible baseline dataset of emotion measurements\n- Document collection methodology in `notes/math/eidolon-field-math.md`\n- Store results in `data/eidolon/baseline.csv`\n\n---\n\n## üì¶ Requirements\n\n- [ ] Implement a script `services/eidolon/scripts/collect_baseline.py`\n- [ ] Log raw vectors to `data/eidolon/raw/`\n- [ ] Summarize averages and ranges in a short report\n- [ ] Decide on sampling frequency and duration (e.g., 1 Hz for 5 minutes)\n- [ ] Implement `log_metrics.py` in `services/eidolon/` to capture vectors\n- [ ] Save raw values to `data/eidolon/baseline.csv`\n- [ ] Generate a small markdown report summarizing mean and variance\n\n---\n\n## üìã Subtasks\n\n- [ ] Run initial collection using current Eidolon service (or mock data)\n- [ ] Export cleaned results to `data/eidolon/baseline.csv`\n- [ ] Update doc `eidolon-field-math.md` with methodology and assumptions\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\n- [Evaluate and reward flow satisfaction](Evaluate%20and%20reward%20flow%20satisfaction.md)\n- [Implement transcendence cascade](Implement%20transcendence%20cascade.md)\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n## ‚ùì Questions\n\n- What instrumentation already exists in Eidolon for metric export?\n- How much historical data is needed for a meaningful baseline?\n#IceBox\n#rejected\n\n"}
{"id":"db64f552-fa40-4176-a27b-44551201b64f","title":"suggest metaprogramming updates md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.521Z","uuid":"db64f552-fa40-4176-a27b-44551201b64f","created_at":"2025-09-15T02:02:58.521Z","path":"docs/agile/tasks/suggest_metaprogramming_updates.md","content":"## üõ†Ô∏è Task: Suggest metaprogramming updates\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Suggest metaprogramming updates\".\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#ice-box\n\n"}
{"id":"dcdb2ed7-78d0-495a-b2a4-4fadd871ae3a","title":"separate all testing pipelines in github actions md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.520Z","uuid":"dcdb2ed7-78d0-495a-b2a4-4fadd871ae3a","created_at":"2025-09-15T02:02:58.520Z","path":"docs/agile/tasks/separate_all_testing_pipelines_in_github_actions.md","content":"## üõ†Ô∏è Task: seperate all testing pipelines in GitHub Actions\n\nDesign docs suggest isolating service tests. Each service should have its own workflow file so failures don't block unrelated code.\n\nCurrently a single workflow runs all tests together, which slows feedback and\nmakes failures hard to trace. Split the CI configuration so each service has\nits own test job.\n\n---\n\n## üéØ Goals\n- Independent CI jobs per service\n- Faster feedback on failing components\n- Allow independent failure reporting per service.\n- Speed up CI by running jobs in parallel.\n- Provide a clear pattern for adding new services in the future.\n\n---\n\n## üì¶ Requirements\n- [ ] Create a workflow file under `.github/workflows` for each service\n- [ ] Ensure shared setup steps use the Makefile\n- [ ] Create a workflow file that runs actions from a matrix containing each service  under `.github/workflows`.\n- [ ] Trigger the appropriate workflow only when files under that service change.\n\n---\n\n## üìã Subtasks\n- [ ] Draft workflow template\n- [ ] Apply to Python services\n- [ ] Apply to Node services\n- [ ] Audit existing `test.yml` workflow.\n- [ ] Configure path filters for each workflow.\n- [ ] Update documentation on CI usage.\n\n---\n\n## üîó Related Epics\n#cicd \n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\n- Move all testing to individual services\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n- [ci](../ci.md)\n#done\n\n"}
{"id":"dd4105ca-a9d4-423b-820d-f5b506d847cf","title":"write meaningful tests for cephalon md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.524Z","uuid":"dd4105ca-a9d4-423b-820d-f5b506d847cf","created_at":"2025-09-15T02:02:58.524Z","path":"docs/agile/tasks/write_meaningful_tests_for_cephalon.md","content":"## üõ†Ô∏è Task: Write meaningful tests for Cephalon\n\nCover core functions of the Cephalon service with automated tests.\n\n---\n\n## üéØ Goals\n- Validate message routing and embedding logic\n- Protect against regression in conversation flow\n\n---\n\n## üì¶ Requirements\n- [ ] Use pytest\n- [ ] Mock dependencies like embeddings and STT/TTS calls\n\n---\n\n## üìã Subtasks\n- [ ] Test command router behaviour\n- [ ] Test memory update logic\n\n---\n\n## üîó Related Epics\n#codex-task #testing\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\nNothing\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n#done\n\n"}
{"id":"ddad0e6a-9019-4cc8-900e-93549b5ee845","title":"obsidian replacement md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.518Z","uuid":"ddad0e6a-9019-4cc8-900e-93549b5ee845","created_at":"2025-09-15T02:02:58.518Z","path":"docs/agile/tasks/obsidian_replacement.md","content":"## üõ†Ô∏è Description\n\nI can do better than this lag monster\n\n---\n\n## üéØ Goals\n\n- What are we trying to accomplish?\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## ‚úÖ Acceptance Criteria\n- Prototype loads notes with lower latency than Obsidian.\n- Users can create, edit, and link notes.\n- Migration path from existing vault is documented.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#ice-box\n\n"}
{"id":"df445b14-f6c7-457e-88c4-872477f8c6e6","title":"add twitch chat integration md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.506Z","uuid":"df445b14-f6c7-457e-88c4-872477f8c6e6","created_at":"2025-09-15T02:02:58.506Z","path":"docs/agile/tasks/add_twitch_chat_integration.md","content":"## üõ†Ô∏è Description\n**Status:** blocked\n\nIntegrate Twitch chat so agents can read and respond to messages during live streams.\n\n---\n\n## üéØ Goals\n\n- Mirror Twitch channel messages into the system.\n- Allow agents to post replies or trigger actions from chat.\n\n---\n\n## üì¶ Requirements\n\n- [ ] Connect using Twitch IRC or EventSub with OAuth.\n- [ ] Respect Twitch rate limits and moderation settings.\n- [ ] Map chat messages to internal event format.\n\n---\n\n## üìã Subtasks\n\n- [ ] Register a Twitch application and obtain credentials.\n- [ ] Implement connection handler for subscribing to chat events.\n- [ ] Translate chat messages into broker events.\n- [ ] Enable optional agent responses back to Twitch.\n- [ ] Add tests for message flow and rate limit handling.\n- [ ] Document required environment variables and usage.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#breakdown\n\n## Blockers\n- No active owner or unclear scope\n\n"}
{"id":"df456c9c-e34b-438f-9e59-d0b2bf014dba","title":"write vault config readme md for obsidian vault on md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.524Z","uuid":"df456c9c-e34b-438f-9e59-d0b2bf014dba","created_at":"2025-09-15T02:02:58.524Z","path":"docs/agile/tasks/write_vault_config_readme_md_for_obsidian_vault_on.md","content":"## üõ†Ô∏è Task: Write vault-config README.md for Obsidian vault onboarding\n\nPlaceholder task stub generated from kanban board.\n\n---\n\n## üéØ Goals\n\n- Define clear objectives for \"Write vault-config README.md for Obsidian vault onboarding\".\n\n---\n\n## üì¶ Requirements\n\n- [x] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [x] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n## Comments\n\n`vault-config/README.md` now documents the minimal configuration and explains\nhow to open the repository as an Obsidian vault.\n#done\n\n"}
{"id":"e02f9c17-8d25-48bf-bad9-bd145a14c9a3","title":"Task: Context Layer Between Codex and Local LLMs","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.517Z","uuid":"e02f9c17-8d25-48bf-bad9-bd145a14c9a3","created_at":"2025-09-15T02:02:58.517Z","path":"docs/agile/tasks/openai-compatable-api.md","content":"# Task: Context Layer Between Codex and Local LLMs\n\n## Description\n\nWe need to design and implement a **context-building service** under `services/ts/` that sits between Codex and local language models (e.g., Ollama). This service must **expose an OpenAI-compatible API** so that Codex and other OpenAI SDK-based clients can interact with it without modification.\n\nThe service will:\n\n* Accept requests via OpenAI-compatible REST endpoints (`/v1/chat/completions`, `/v1/completions`, etc.).\n* Retrieve relevant Promethean monorepo context (functions, agents, configs, docs) using the SmartGPT Bridge (`/search`, `/grep`, `/symbols`).\n* Authorize all requests to the SmartGPT Bridge with a static token.\n* Dynamically enrich model prompts with this context before forwarding to Ollama (first backend) or other local LLMs.\n* Provide standardized outputs in OpenAI response format, while also persisting enriched context into Obsidian artifacts under `docs/`.\n\n## Requirements / Definition of Done ‚úÖ\n\n* [ ] Service implemented under `services/ts/codex-context/`.\n* [ ] Exposes OpenAI-compatible API endpoints (`/v1/chat/completions`, `/v1/completions`, etc.).\n* [ ] Accepts OpenAI-style requests (same JSON schema) and returns OpenAI-style responses.\n* [ ] Codex requests transparently pass through this service.\n* [ ] Relevant repo context (code, AGENTS.md, docs) is retrieved via SmartGPT endpoints.\n* [ ] SmartGPT requests must be authorized with a static token.\n* [ ] Prompt construction supports append-only structured augmentation (preserves provenance & citations).\n* [ ] Configurable for different LLM backends (Ollama first, extend later).\n* [ ] Outputs are standardized into:\n\n  * OpenAI API response (developer-facing).\n  * Obsidian artifacts under `docs/` (structured markdown with citations).\n* [ ] Tests are written for:\n\n  * OpenAI API compatibility (request schema, response schema).\n  * CLI interception and request routing.\n  * SmartGPT Bridge request authorization.\n  * Context assembly and augmentation logic.\n  * Output format validation (CLI + Obsidian).\n* [ ] Verified against at least one coding workflow (e.g., agent development guided by AGENTS.md).\n\n## Tasks üõ†Ô∏è\n\n* [ ] Step 1: Define OpenAI-compatible API surface (`/v1/chat/completions`, `/v1/completions`).\n* [ ] Step 2: Build `services/ts/codex-context/` with service scaffolding.\n* [ ] Step 3: Implement CLI interception layer (Codex ‚Üí Context Service ‚Üí LLM).\n* [ ] Step 4: Integrate SmartGPT Bridge retrieval (`/search`, `/grep`, `/symbols`) with static token auth.\n* [ ] Step 5: Implement context-aware prompt builder (citations, provenance, structured outputs).\n* [ ] Step 6: Add backend configurability (Ollama first, extend later).\n* [ ] Step 7: Write full test suite (API compatibility, routing, token auth, prompt assembly, outputs).\n* [ ] Step 8: Test with a sample workflow (e.g., `agents/duck/AGENTS.md`).\n* [ ] Step 9: Export results into `docs/architecture/codex-context.md` with Obsidian graphs.\n\n## Relevant Resources üìö\n\n* [ChatGPT - Using Codex with Ollama](https://chatgpt.com/share/68a74084-31fc-8004-b327-68ea8a979fab)\n* [AI-assisted Coding with Cody: Lessons from Context Retrieval and Evaluation for Code Recommendations](https://arxiv.org/html/2408.05344v1)\n\n## Comments üí¨\n\nAgents can use this task to maintain append-only discussions on design decisions, tradeoffs, and testing.\n\n\n"}
{"id":"e0a6842d-031a-4d1c-bee1-563bbda8cf80","title":"Extract shared services into @promethean/omni-core","status":"in_progress","priority":"P1","owner":"","labels":["omni","refactor"],"created":"2025-09-21T02:46:00Z","uuid":"e0a6842d-031a-4d1c-bee1-563bbda8cf80","created_at":"2025-09-21T02:46:00Z","path":"docs/agile/tasks/extract-omni-core-services.md","content":"## üéØ Outcome\nRelocate SmartGPT bridge domain logic (files/search/sinks/indexer/agents/exec) into a reusable `@promethean/omni-core` package that implements the Omni protocol interfaces.\n\n## üì• Inputs\n- [[docs/architecture/omni/omni-service-roadmap.md]]\n- `bridge/src/routes/v1/*`\n- `packages/mcp/src/tools/github/*`\n\n## ‚úÖ Definition of Done\n- [ ] New package `packages/omni-core/` with build/test scripts.\n- [ ] Core services expose protocol-compliant methods consuming existing stores and supervisors.\n- [ ] MCP file + GitHub helpers delegate to omni-core implementations.\n- [ ] Existing SmartGPT bridge re-exports continue working via thin adapters.\n- [ ] Regression tests (AVA) run against omni-core services.\n\n## ü™ú Steps\n1. Inventory dependencies (Mongo, Chroma, config) and define injection points.\n2. Move file/search/sink/indexer/agent/exec modules into omni-core with minimal surface changes.\n3. Update MCP package to consume omni-core services.\n4. Provide Fastify request context builder hooking into auth/RBAC.\n5. Run affected package tests; update docs + changelog.\n\n## üîó Dependencies\n- [[docs/agile/tasks/author-omni-protocol-package.md]]\n- [[docs/agile/tasks/omni-unified-service-spec.md]]\n"}
{"id":"e1591a4b-1a4f-48c9-ab6a-01fe11c524f3","title":"llm service must allow streamed responses","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.516Z","uuid":"e1591a4b-1a4f-48c9-ab6a-01fe11c524f3","created_at":"2025-09-15T02:02:58.516Z","path":"docs/agile/tasks/llm-service-must-allow-streamed-responses.md","content":"## üõ†Ô∏è Description\n\nDescribe your task\n\n## üì¶ Requirements\n- Add streaming endpoint to the LLM service for partial responses.\n- Update client libraries to consume streamed tokens.\n- Document how to enable and use the streaming API.\n\n## ‚úÖ Acceptance Criteria\n- Clients receive incremental outputs from the LLM service.\n- Example integration demonstrates a streaming completion.\n- Documentation covers streaming usage.\n\n## Tasks\n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Relevent resources\nYou might find [this] useful while working on this task\n\n## Comments\nUseful for agents to engage in append only conversations about this task.\n\n## Story Points\n\n- Estimate: 5\n- Assumptions: Underlying LLM and transport layer support streaming responses.\n- Dependencies: Streaming protocol implementation and client compatibility.\n#ready\n\n"}
{"id":"e2059e9f-f2a2-4bc4-bcc7-eece59c16f88","title":"ecs query api gateway","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"e2059e9f-f2a2-4bc4-bcc7-eece59c16f88","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/ecs_query_api_gateway.md","content":"ECS Query API Gateway\n\nGoal: Expose a small HTTP API for reading ECS-backed data (Mongo) for dashboards, agents, and tools.\n\nEndpoints (examples):\n- `GET /agents/:id/state` ‚Äì AgentStateSnapshot\n- `GET /conversations/:channelId/recent?limit=50` ‚Äì ConversationLog\n- `GET /turns/:id` ‚Äì turn details and related messages\n\nExit Criteria:\n- Service scaffolding + 2‚Äì3 endpoints with tests.\n\n#incoming #ecs #api #mongo\n\n"}
{"id":"e3fc2eed-0da2-4aa9-b407-4dda0de4488d","title":"ensure openapi specs are automaticly updated when an endpoint is changed","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"e3fc2eed-0da2-4aa9-b407-4dda0de4488d","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/ensure-openapi-specs-are-automaticly-updated-when-an-endpoint-is-changed.md","content":"## üõ†Ô∏è Description\n\nThis should just work with fastify... but I am actually not sure. I've just started to learn fastify.\n\n\n---\n\n## üéØ Goals\n\n- What are we trying to accomplish?\n\n---\n\n## üì¶ Requirements\n\n- [ ] Detail requirements.\n\n---\n\n## üìã Subtasks\n\n- [ ] Outline steps to implement.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#done\n\n"}
{"id":"e4b85f6b-4112-4c52-84d2-17695ae5e67c","title":"auth shared clients and middleware","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"e4b85f6b-4112-4c52-84d2-17695ae5e67c","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/auth_shared_clients_and_middleware.md","content":"Auth: Shared clients and middleware (TS + Python)\n\nGoal: Provide small libraries to fetch/verify tokens and inject auth into HTTP/WebSocket requests.\n\nScope:\n- TS: `shared/ts/src/auth/` token verifier (JWKS cache, kid rotation), HTTP middleware for Express/Fastify, helper to fetch client_credentials token.\n- Python: `shared/py/auth/` verifier (PyJWT + JWKS cache), FastAPI/Starlette middleware, helper to fetch client_credentials token.\n- Config: `AUTH_ISSUER`, `AUTH_JWKS_URL`, `AUTH_AUDIENCE`, `AUTH_CLIENT_ID`, `AUTH_CLIENT_SECRET`.\n- Docs + examples for bridging to SmartGPT Bridge, file-watcher, cephalon.\n\nExit Criteria:\n- Unit tests verifying signature, expiry, audience, and scope checks.\n- Example snippets integrated in service READMEs.\n\n#incoming #auth #libraries #ts #python\n\n\n"}
{"id":"e4ec91c1-9c83-42a8-8f95-f3f15d8e8499","title":"expose metrics on an express app and scrape with p md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"e4ec91c1-9c83-42a8-8f95-f3f15d8e8499","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/expose_metrics_on_an_express_app_and_scrape_with_p.md","content":"## üõ†Ô∏è Task: Expose `/metrics` on an express app and scrape with Prom\n\nGenerated from [../unique/2025.08.08.15.08.47.md](../unique/2025.08.08.15.08.47.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"e6a16b59-a67d-4df9-babc-9faaaad3f9c9","title":"Remove CommonJS artifacts from repository","status":"document","priority":"P2","owner":"","labels":["#codex-task","#doc-this"],"created":"2025-09-20T21:30:00.000Z","uuid":"e6a16b59-a67d-4df9-babc-9faaaad3f9c9","created_at":"2025-09-20T21:30:00.000Z","path":"docs/agile/tasks/remove_commonjs_artifacts_repo_wide.md","content":"# Remove CommonJS artifacts from repository\n\n## Problem\nThe codebase mixes ES module tooling with lingering CommonJS patterns (`.cjs` configs, `require()` usage, and \"commonjs\" annotations).\nThese stragglers break consistency, complicate bundling, and violate the repo rule banning `require`.\n\n## Outcome\nThe repository should rely solely on ES module syntax and file extensions, replacing CommonJS artifacts with equivalent ESM/TypeScript constructs.\n\n## Proposed steps\n- Inventory all `.cjs` files, `require()` calls, and documentation references.\n- Determine replacement patterns (e.g., convert configs to `.ts` or `.mjs`, swap `require` for `import`).\n- Update build/test tooling to consume the new module shapes.\n- Document any exceptions or migration blockers.\n\n## Definition of done\n- No source-controlled file or documentation references `.cjs`, `CommonJS`, or `require()` semantics unless explicitly recorded as a future task.\n- Tooling and tests succeed using the updated module formats.\n- Migration notes highlight remaining blockers if a full conversion is not yet possible.\n\n# Status history\n- 2025-09-20: Moved from **Incoming** to **Accepted** after clarifying scope and confirming repository-wide coverage expectation.\n- 2025-09-20: Advanced to **Breakdown** to inventory `.cjs` modules, `require()` usage, and documentation references.\n- 2025-09-20: Transitioned to **Ready** after carving executable slices for the migration.\n- 2025-09-20: Pulled into **To Do** for execution of the initial inventory/documentation slice.\n- 2025-09-20: Moved to **In Progress** while compiling exhaustive inventories and migration strategy.\n- 2025-09-20: Entered **In Review** with a repository-wide audit, slice plan, and outstanding work log.\n- 2025-09-20: Shifted to **Document** after capturing evidence, counts, and open follow-ups for handoff.\n\n## Current inventory\n\n| Category | Count | Notes |\n| --- | --- | --- |\n| Package-level `.eslintrc.cjs` files | 33 | Each extends `config/.eslintrc.base.cjs`; should migrate to a shared ESM preset. |\n| CommonJS lint scripts | 2 | `scripts/check-changelog.cjs` and `scripts/check-changelog-fragments.cjs`, consumed by AVA tests. |\n| PM2 ecosystem fixtures | 2 | Located at `packages/heartbeat/fixtures/ecosystem.fixture.config.cjs` and `services/js/heartbeat/tests/test-ecosystem.config.cjs`. |\n| `package.json` files exporting `dist/index.cjs` | 45 | `main` and `exports.require` rely on CommonJS build artifacts across workspace packages. |\n| Tooling & docs references | ‚â•6 | Includes `.pnpmfile.cjs`, `.depcruise.cjs`, and troubleshooting guides that teach `.cjs` usage. |\n\n## Risks & decisions\n- Lint/test harnesses may assume CommonJS module shape; replacing with `.ts`/`.mjs` requires ensuring consumption sites support `import`.\n- Package build pipelines must emit `.mjs` or `.js` ESM bundles; confirm `tsup`/`tsc` configs before changing published entry points.\n- Some docs intentionally mention `.cjs` for historical instructions‚Äîneed policy on archival content vs active guidance.\n\n## Upcoming slices\n1. Convert shared ESLint config to ESM/TypeScript and update package-level `.eslintrc.cjs` files.\n2. Replace CommonJS lint scripts with `.mjs` equivalents and adjust invoking tests.\n3. Reconfigure package build outputs (`package.json` exports & build tooling) away from `dist/index.cjs`.\n4. Update docs/tooling references and record any intentionally retained `.cjs` fixtures.\n\n## Evidence gathered\n\n```bash\nfind packages -maxdepth 2 -name '.eslintrc.cjs' | wc -l\n# ‚Üí 33\n\npython - <<'PY'\nfrom pathlib import Path\nimport json\ncount = 0\nfor pkg in Path('packages').glob('*/package.json'):\n    data = json.loads(pkg.read_text())\n    if data.get('main') == 'dist/index.cjs' or any(\n        isinstance(v, dict) and v.get('require') == './dist/index.cjs'\n        for v in (data.get('exports') or {}).values()\n        if isinstance((data.get('exports') or {}), dict)\n    ):\n        count += 1\nprint(count)\nPY\n# ‚Üí 45\n\nfind . -name '*.cjs' | sort | head -n 40\n```\n\n## Outstanding work\n- Replace the 33 package-level `.eslintrc.cjs` files after introducing an ESM-compatible shared config.\n- Decide on migration path for 45 package entry points that currently emit `dist/index.cjs` bundles.\n- Port CommonJS automation scripts (`check-changelog*.cjs`, PM2 fixtures) to ESM while keeping test harnesses functional.\n- Sweep documentation/tooling references for `.cjs` instructions and either modernize or archive them.\n\n#Document #doc-this #codex-task\n"}
{"id":"e6b0e9e7-5967-4b97-bded-0ac50646bb29","title":"create a generic markdown helper module","status":"todo","priority":"P5","owner":"","labels":[],"created":"","uuid":"e6b0e9e7-5967-4b97-bded-0ac50646bb29","path":"docs/agile/tasks/create-a-generic-markdown-helper-module.md","content":"## üõ†Ô∏è Create a Generic Markdown Helper Module\n\nDevelop a shared module for reading, writing, and manipulating markdown files used across documentation scripts.\n\n---\n\n## üéØ Goals\n\n- Provide utilities for common markdown transformations\n- Reduce duplicated parsing logic in scripts\n\n---\n\n## üì¶ Requirements\n\n- [ ] Module exposes parse and render helpers\n- [x] Handles frontmatter, links, and task lists\n- [x] Includes unit tests\n\n---\n\n## üìã Subtasks\n\n- [ ] Define helper API surface\n- [ ] Implement core parsing and formatting functions\n- [ ] Write tests covering typical file operations\n- [ ] Document usage in scripts README\n\n---\n\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\nhttps://github.com/riatzukiza/promethean/pull/1122\nhttps://github.com/riatzukiza/promethean/pull/1123\nhttps://github.com/riatzukiza/promethean/pull/1125\n\n#framework-core #Ready\n\n\n"}
{"id":"e6bbd2be-7191-4b32-ba5c-d82f1771b520","title":"determine pm2 configuration for agents 1 md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.511Z","uuid":"e6bbd2be-7191-4b32-ba5c-d82f1771b520","created_at":"2025-09-15T02:02:58.511Z","path":"docs/agile/tasks/determine_pm2_configuration_for_agents_1.md","content":"## üõ†Ô∏è Task: Determine PM2 configuration for agents\n\nEstablish a shared PM2 ecosystem or alternative process manager setup that all\nagents can rely on. Document the chosen approach in `MIGRATION_PLAN.md` and\ninclude startup instructions in each service README.\n\n---\n\n## üéØ Goals\n\n- Decide on the default process manager (PM2 vs custom)\n- Provide example ecosystem files for at least one agent\n- Outline how to start/stop all services consistently\n\n---\n\n## üì¶ Requirements\n\n- [ ] Review existing Makefile or npm scripts\n- [ ] Draft a sample `ecosystem.config.js`\n- [ ] Document instructions in `docs/MIGRATION_PLAN.md`\n- [ ] Show how environment variables are passed to each service\n- [ ] Include environment variable examples for development vs production\n- [ ] Ensure log files are rotated to `logs/pm2/*.log`\n\n---\n\n## üìã Subtasks\n\n- [ ] Prototype with Duck services\n- [ ] Validate restart behavior and log handling\n- [ ] Add `npm run start:all` that loads the ecosystem file\n- [ ] Share the config with the team for feedback\n- [ ] Add a Makefile target `make pm2-start` for convenience\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\n- [write simple ecosystem declaration library for new agents](write%20simple%20ecosystem%20declaration%20library%20for%20new%20agents.md)\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n## ‚ùì Questions\n\n- Should we consider lightweight alternatives to PM2 for local dev?\n- feel like PM2 *is* the lightweight alternative MR AI, but if you have other ideas, I'm all ears. It's definitely lighter weight than docker\n- And there is only gonna be local dev... this is a local tool\n- as much as I am sure your greedy owners would like to use this in the cloud, it's designed to not scale that way\n- unless you can afford well... all the people you're trying to automate's time to spend a life time training 1 model that only works for them.\n- Then please.\n- How will service logs be aggregated when using PM2?\n#done\n\n"}
{"id":"e72a8ed9-7efa-4817-bf55-0b65fd84e5d2","title":"create and push a feature branch","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"e72a8ed9-7efa-4817-bf55-0b65fd84e5d2","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/describe_github_branching_workflow.md","content":"## üõ†Ô∏è Description\n\nMerging straight into `main` caused instability. This task documents a\nstructured Git branching workflow where features land on `dev`, are\npromoted to `staging`, and finally reach `main` once validated. The\ndocument also specifies branch naming conventions, merge gates, and CI\nexpectations.\n\n### Branch Naming Conventions\n\n- `feature/<summary>` ‚Äì new features\n- `fix/<summary>` ‚Äì bug fixes\n- `docs/<summary>` ‚Äì documentation changes\n- `chore/<summary>` ‚Äì refactors or maintenance\n\n### Merge Requirements\n\n1. **Feature branch ‚Üí `dev`**\n   - Rebase on latest `origin/dev`.\n   - `make format lint test` must succeed before opening a PR.\n2. **`dev` ‚Üí `staging`**\n   - All `dev` CI checks are green.\n   - Run `make build` and any integration tests.\n3. **`staging` ‚Üí `main`**\n   - Staging has been manually verified.\n   - Full CI suite passes (`make lint test build`).\n\n### CI Expectations\n\n- **`dev`**: linting and unit tests.\n- **`staging`**: linting, unit tests, integration tests, and build.\n- **`main`**: full test suite plus manual verification.\n\n### Example Commands\n\n```bash\n# create and push a feature branch\ngit checkout -b feature/add-auth\nmake format lint test\ngit push origin feature/add-auth\n\n# promote through the environments\ngit checkout staging\ngit merge dev && git push origin staging\n\ngit checkout main\ngit merge staging && git push origin main\n```\n\nSee the [[README|contribution guidelines]] for general workflow\ndetails.\n\n---\n\n## üéØ Goals\n\n- Document branch naming conventions.\n- Define merge requirements from `dev` ‚Üí `staging` ‚Üí `main`.\n- Clarify CI expectations for each stage.\n- Provide example commands and link to contribution docs.\n\n---\n\n## üì¶ Requirements\n\n- [ ] Branch names follow `<type>/<summary>`.\n- [ ] `make format lint test` passes before merging to `dev`.\n- [ ] `make build` and integration tests run before merging to `staging`.\n- [ ] Full CI suite passes before merging to `main`.\n\n---\n\n## üìã Subtasks\n\n- [ ] Draft branch naming section.\n- [ ] Document merge gates and CI requirements.\n- [ ] Add example git commands.\n- [ ] Link to contribution docs.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n#agent-thinking\n#Breakdown\n\n\n"}
{"id":"e7b32074-d418-456e-a874-2e61432ec04e","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.523Z","uuid":"e7b32074-d418-456e-a874-2e61432ec04e","created_at":"2025-09-15T02:02:58.523Z","path":"docs/agile/tasks/universal-package-manager.md","content":"# Description\n\nDescribe your task\n\n## Requirements/Definition of done\n\n- If it doesn't have this, we can't accept it\n\n## Tasks \n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Relevent resources\n\nYou might find [this](https://github.com/replit/upm?tab=readme-ov-file) useful while working on this task\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n\n#rejected\n\n# Reason for rejection\n\nShort answer: **don‚Äôt move your whole system to UPM.** It‚Äôs a thin wrapper over each language‚Äôs real package manager. That‚Äôs handy for Replit-scale convenience, but it won‚Äôt solve your monorepo/workspaces, cache, hermeticity, or CI determinism problems‚Äîand it can add another layer of abstraction to debug.\n\n### What UPM actually is (not marketing)\n\n* A single static binary that **detects a language** in the folder and **delegates** to that language‚Äôs native manager (pip/uv/poetry, npm/pnpm/yarn, bundler, cask, etc.). It keeps to the specfile/lockfile of the underlying tool. ([GitHub][1])\n* Supports core ops (`add/remove/lock/install/list`), registry lookups, and a ‚Äúguess‚Äù feature that scans imports to suggest deps. It **does not** replace or unify lockfile formats; it runs the native tool and reads its files. ([GitHub][1])\n* It‚Äôs what Replit uses to smooth over many languages in their IDE; Replit docs explicitly point there and still endorse using native CLIs directly if you prefer. ([docs.replit.com][2])\n\n### Upsides (for small, single-language projects)\n\n* One command surface across languages.\n* ‚ÄúGuess deps‚Äù can be nice for quick spikes or throwaway demos. ([GitHub][1])\n* Easy to install (brew/scoop/deb/rpm/docker) and zero runtime deps. ([GitHub][1])\n\n### Red flags for **Promethean** (your reality)\n\n* **No unified lock model.** Reproducibility still lives in *per-language* lockfiles; UPM won‚Äôt add checksums/signature guarantees across ecosystems. If reproducibility matters, you still need best-in-class lock discipline per language. (General lockfile security rationale.) ([Semgrep][3], [reproducible-builds.org][4])\n* **No monorepo/workspace story.** It won‚Äôt manage pnpm/yarn workspaces, TS project references, Python multi-env matrices, or shared virtualenv constraints. It simply calls those tools. ([GitHub][1])\n* **Adds an extra failure surface.** When something breaks you now debug UPM ‚Üí native manager ‚Üí registry. You already fight complexity; another shim isn‚Äôt leverage.\n* **Not a replacement for toolchains.** You still need version pinning for Node/Python toolchains (mise/asdf/direnv/nix) and per-service isolation.\n\n### What I‚Äôd do instead (practical, systems-minded)\n\n1. **Keep best-in-class per ecosystem, make them scriptable:**\n\n   * **JS/TS:** `pnpm` + workspaces + `pnpm-lock.yaml`. Deterministic, fast, good monorepo support.\n   * **Python:** **uv** (or Poetry if you must) with `pyproject.toml` + `uv.lock` (uv is fast and moving toward excellent lock + sync flows). Track build deps explicitly. ([GitHub][5])\n   * **Rust/Go/etc.:** their native tools are already hermetic enough (Cargo/Go modules).\n2. **Unify at the orchestration layer, not at the package layer.**\n   You already moved toward **bb/EDN** manifests. Define a **single EDN ‚Äúdeps manifest‚Äù** that maps service ‚Üí `{lang, manager, path, cmds}` and have `bb` drive the native commands:\n\n   * `bb deps:add :service foo :pkg bar` ‚Üí dispatch to `pnpm -w add -r ‚Ä¶` or `uv add ‚Ä¶`\n   * `bb deps:lock`, `bb deps:install`, `bb deps:audit`, etc.\n     You get the *UPM ergonomics* without surrendering control.\n3. **Hermetic builds + toolchains:**\n   Layer **mise/asdf + direnv** or **Nix flakes** to pin tool versions per project. UPM won‚Äôt help here; Nix/mise will.\n4. **Policy & CI:**\n\n   * Enforce ‚Äúlockfile present & clean‚Äù precommit.\n   * CI job per language runs native `install` from lock only (no network if you can cache).\n   * Central audits: `pnpm audit --json`, `uv pip audit`, `cargo audit` ‚Üí merged report.\n\n### If you still want to try UPM\n\nRun a **contained experiment** on a single non-critical service and measure:\n\n```bash\n# install upm (example mac)\nbrew install replit/tap/upm\n\n# in a small python svc\nupm -l python add requests\nupm list           # reads pyproject/poetry.lock it just made\nupm lock           # delegates to poetry/uv lock\nupm install        # installs from lock\n```\n\nSuccess criteria: fewer commands to teach agents, zero regressions vs native CLIs, and no surprises in CI. If it‚Äôs not clearly better in a week, drop it. ([GitHub][1])\n\n### Bottom line\n\nUPM is clever glue. You don‚Äôt need glue; you need **determinism, workspaces, and toolchain pinning** across a polyglot monorepo. Keep native managers, **standardize their invocation behind your EDN/bb layer**, and reserve UPM (if at all) for on-ramps or tiny side-scripts.\n\n\\#tags\n\\#promethean #monorepo #dependency-management #pnpm #uv #poetry #nix #mise #asdf #direnv #ci #lockfiles #reproducible-builds #architectural-decisions\n\n[1]: https://github.com/replit/upm \"GitHub - replit/upm: ‚†ï Universal Package Manager - Python, Node.js, Ruby, Emacs Lisp.\"\n[2]: https://docs.replit.com/replit-workspace/dependency-management?utm_source=chatgpt.com \"Replit's dependency management\"\n[3]: https://semgrep.dev/blog/2022/the-best-free-open-source-supply-chain-tool-the-lockfile?utm_source=chatgpt.com \"The best free, open-source supply-chain security tool? ...\"\n[4]: https://reproducible-builds.org/?utm_source=chatgpt.com \"Reproducible Builds ‚Äî a set of software development ...\"\n[5]: https://github.com/astral-sh/uv/issues/5190?utm_source=chatgpt.com \"Locking of build dependencies ¬∑ Issue #5190 ¬∑ astral-sh/uv\"\n\n"}
{"id":"edaa3887-de50-422c-8c8f-6d624329465b","title":"twitch stream title generator md md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.523Z","uuid":"edaa3887-de50-422c-8c8f-6d624329465b","created_at":"2025-09-15T02:02:58.523Z","path":"docs/agile/tasks/twitch_stream_title_generator.md","content":"Here‚Äôs a refined version of your task that keeps it tied to your streaming workflow but makes it concrete enough to move forward:\n\n---\n\n## üõ†Ô∏è Description\n\nImplement a system component that automatically generates new stream titles using real-time and historical context from the pipeline.\nInitially, the context source will be **Discord transcripts**, but the scope should expand to include additional context sources (e.g., game events, chat logs, metadata from streaming platforms).\n\nThe goal is for the system to produce engaging, accurate, and relevant stream titles **without manual intervention**, using Ollama for text generation as a formally integrated step in the pipeline.\n\n---\n\n## üéØ Goals\n\n* Formally integrate **Ollama** into the existing pipeline as the generation engine for stream titles.\n* Ensure the title generation process pulls from the **latest available context** (Discord transcripts initially).\n* Enable **flexible context source expansion** so additional data streams can be plugged in later without redesigning the pipeline.\n* Make output titles **engaging for viewers** while being representative of the actual stream content.\n\n---\n\n## üì¶ Requirements\n\n* [ ] Define the **API interface** between context sources and Ollama for prompt construction.\n* [ ] Add **pipeline step** for title generation and storage.\n* [ ] Support **real-time** generation triggered by new context availability.\n* [ ] Store generated titles in a retrievable format for later review/editing.\n* [ ] Implement safeguards against inappropriate or irrelevant titles.\n* [ ] Support expansion to **multiple context sources** (chat logs, metadata, etc.).\n\n---\n\n## üìã Subtasks\n\n* [ ] Identify the initial context format from Discord transcripts.\n* [ ] Design a **prompt template** for Ollama that uses this context to generate titles.\n* [ ] Implement a **pipeline integration module** that sends prompts to Ollama and receives generated titles.\n* [ ] Add **trigger mechanism** for generation (e.g., after N messages or time interval).\n* [ ] Create a storage layer or use existing logging system for generated titles.\n* [ ] Add a review/edit UI or CLI command for manual override.\n* [ ] Plan for expansion to additional context types.\n\n---\n\n## üîó Related Epics\n\n\\#framework-core\n\\#ollama-integration\n\\#stream-automation\n\n---\n\nIf you want, I can also make you a **mermaid diagram** showing how the context flows from Discord transcripts ‚Üí Ollama ‚Üí storage ‚Üí stream platform so we can see where it sits in the pipeline.\nThat would make expansion easier later.\n\n## Notes\n- Tests or documentation are missing; acceptance criteria not fully met.\n- Story Points: 3\n\n#in-progress\n\n"}
{"id":"ee61990a-388c-469f-8397-92073bebb9e8","title":"auth service rfc and architecture","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.524Z","uuid":"ee61990a-388c-469f-8397-92073bebb9e8","created_at":"2025-09-15T02:02:58.524Z","path":"docs/agile/tasks/auth_service_rfc_and_architecture.md","content":"Auth Service: RFC and Architecture\n\nGoal: Design a centralized internal auth provider for Promethean services.\n\nWhy: 19+ services require consistent, secure service-to-service auth. Current static tokens are ad hoc.\n\nKey Decisions:\n- Protocol: OAuth2.1 Client Credentials (service-to-service) with JWT (JWS, RS256) access tokens.\n- Endpoints: `POST /oauth/token` (client_credentials), `GET /.well-known/openid-configuration`, `GET /.well-known/jwks.json`, `POST /introspect` (optional), `POST /rotate` (admin-only), `GET /health`.\n- Token Claims: `iss`, `sub` (service id), `aud` (service/group), `exp`, `iat`, `scope` (space-delimited), `kid`.\n- Storage: MongoDB (clients collection with hashed secret, scopes, status; keys collection for JWKS, rotation metadata).\n- Signing: Asymmetric (RS256). Maintain key set with rotation; publish JWKS.\n- Scopes: Align with permission gate actions and bridge/topics (e.g., `publish:agent.llm.result`, `indexer:write`, `heartbeat:read`).\n- Trust Model: Internal-only exposure, but standards-based to allow future external federation.\n\nDeliverables:\n- Markdown RFC with sequence diagrams, data models, example tokens, error codes, and rotation strategy (kid overlap window).\n- Alignment doc mapping existing `checkPermission`/permission gate to OAuth scopes.\n\nExit Criteria:\n- Approved RFC; tickets created for scaffold, clients, and service integration.\n\n#incoming #auth #oauth #architecture\n\n\n"}
{"id":"ee9f46e7-9d65-437b-8897-2a7e11756f4f","title":"snapshot prompts specs to repo","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.520Z","uuid":"ee9f46e7-9d65-437b-8897-2a7e11756f4f","created_at":"2025-09-15T02:02:58.520Z","path":"docs/agile/tasks/snapshot_prompts_specs_to_repo.md","content":"## üõ†Ô∏è Task: Snapshot prompts specs to repo\n\nCapture existing prompt and specification documents into the repository so they are version-controlled and easily referenced.\n\n---\n\n## üéØ Goals\n\n- Ensure prompt specifications live alongside code\n- Provide baseline for tracking future changes\n\n---\n\n## üì¶ Requirements\n\n- [ ] Collect current prompt and spec documents\n- [ ] Store them under `docs/prompts/`\n- [ ] Verify references on the Kanban board\n\n---\n\n## üìã Subtasks\n\n- [ ] Identify all existing prompt/spec sources\n- [ ] Add missing files to repository\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#IceBox\n#ready\n\n"}
{"id":"eebffb87-73cd-4945-a53a-f60eaa852eed","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"eebffb87-73cd-4945-a53a-f60eaa852eed","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/curate-code-from-personal-repository.md","content":"# Description\n\nI've written a lot of code over the years that I want to be a part of this system.\nNot even nessisarily to run it, though that would be nice.\nJust having it in the repo gives my system context\n\n## Requirements/Definition of done\n\n- List of the repos we think are valuable\n\n## Tasks \n\n- [ ] Look through old code for gems\n\n## Relevent resources\n\n[ChatGPT - Config.toml guide](https://chatgpt.com/share/68a74210-8c1c-8004-9a3f-e41a94ba6ffa)\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n"}
{"id":"eed137f9-8a84-43c4-99d3-5ec338da6f49","title":"spin up ws gateway ws port 8090 ws token devtoken md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.521Z","uuid":"eed137f9-8a84-43c4-99d3-5ec338da6f49","created_at":"2025-09-15T02:02:58.521Z","path":"docs/agile/tasks/spin_up_ws_gateway_ws_port_8090_ws_token_devtoken.md","content":"## üõ†Ô∏è Task: Spin up WS gateway (`WS_PORT=8090 WS_TOKEN=devtoken node index.js`)\n\nGenerated from [../unique/2025.08.08.15.08.47.md](../unique/2025.08.08.15.08.47.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"ef909fd1-19f0-43d4-b022-35d29ec053c7","title":"auth service scaffold and endpoints","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"ef909fd1-19f0-43d4-b022-35d29ec053c7","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/auth_service_scaffold_and_endpoints.md","content":"Auth Service: Scaffold + Endpoints\n\nGoal: Implement a minimal OAuth2.1 service for internal use.\n\nStack:\n- TypeScript (Node), Fastify or Express. Path: `services/ts/auth/`.\n- MongoDB for client + keys. Env-configurable connection.\n\nScope (MVP):\n- `POST /oauth/token` (client_credentials): Validate client_id/secret, issue RS256 JWT with scopes + 5‚Äì15m TTL.\n- `GET /.well-known/jwks.json`: Publish public keys (kid, kty, e, n, alg, use).\n- `GET /.well-known/openid-configuration`: Minimal metadata (issuer, jwks_uri, token_endpoint).\n- `GET /health`: Liveness/readiness.\n- Key rotation worker + admin CLI to generate new keypair and mark old key retiring (overlap window).\n\nExit Criteria:\n- Service starts locally, issues + verifies tokens (via JWKS) end-to-end.\n- Basic tests for token issuance, expiration, invalid secret, and JWKS fetch.\n\n#incoming #auth #oauth #service\n\n\n"}
{"id":"f082b303-f317-4459-9692-21e26eb70d0d","title":"add stt service tests md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.505Z","uuid":"f082b303-f317-4459-9692-21e26eb70d0d","created_at":"2025-09-15T02:02:58.505Z","path":"docs/agile/tasks/add_stt_service_tests.md","content":"## üõ†Ô∏è Task: Add STT service tests\n\nCreate unit tests for the speech-to-text service in `services/stt/`.\n\n---\n\n## üéØ Goals\n- Verify `transcribe_file` produces expected text\n- Lay groundwork for integration tests\n\n---\n\n## üì¶ Requirements\n- [ ] Use small audio samples\n- [ ] Mock external model calls where possible\n\n---\n\n## üìã Subtasks\n- [ ] Test CLI transcription\n- [ ] Test API endpoint behaviour if available\n\n---\n\n## üîó Related Epics\n#codex-task #testing\n\n---\n\n## ‚õìÔ∏è Blocked By\nNothing\n\n## ‚õìÔ∏è Blocks\nNothing\n\n---\n\n## üîç Relevant Links\n- [[kanban]]\n#done\n\n"}
{"id":"f0fe8d30-5833-4791-9e31-9c2292db03bc","title":"üõ†Ô∏è Task: Implement `defun` in Lisp Compiler","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.515Z","uuid":"f0fe8d30-5833-4791-9e31-9c2292db03bc","created_at":"2025-09-15T02:02:58.515Z","path":"docs/agile/tasks/implement-defun-in-compiler-lisp-incoming.md","content":"# üõ†Ô∏è Task: Implement `defun` in Lisp Compiler\n\n## üìñ Description\nIntroduce support for the `defun` special form in the Promethean Lisp compiler. This will allow defining named functions with arguments and bodies, storing them in the environment, and enabling later calls.\n\n---\n\n## ‚úÖ Requirements / Definition of Done\n- [ ] Parser recognizes the `defun` keyword and constructs an appropriate AST node.\n- [ ] Compiler/Evaluator can create a new function binding in the environment.\n- [ ] Functions defined with `defun` can be invoked with arguments.\n- [ ] Support for recursion (`defun fact (n) ...`).\n- [ ] Unit tests pass for simple and recursive cases.\n- [ ] Documentation updated to include `defun` usage.\n\n---\n\n## üìã Subtasks\n- [ ] Update parser to handle `(defun <name> (<args>...) <body>)`.\n- [ ] Extend AST with a `DefunNode`.\n- [ ] Extend environment to support named function bindings.\n- [ ] Update evaluator to compile/evaluate `DefunNode` into callable functions.\n- [ ] Write unit tests for function definition, invocation, and recursion.\n- [ ] Update Lisp docs with `defun` examples.\n\n---\n\n## üîó Dependencies\n- Relates to: `implement classes in compiler lisp incoming` (functions must exist before classes/methods).\n- Builds on: `redefine all existing lambdas with high order functions incoming` (shared function infrastructure).\n\n---\n\n## üìö Relevant Resources\n- Common Lisp HyperSpec: [`defun`](http://www.lispworks.com/documentation/HyperSpec/Body/m_defun.htm)\n- Hy Language: [`defn`](https://docs.hylang.org/en/stable/api.html#defn)\n\n---\n\n## üí¨ Comments\nAppend-only thread for discussion by agents working on this task.\n\n## Notes\n- Tests or documentation are missing; acceptance criteria not fully met.\n- Story Points: 5\n\n#in-progress\n\n"}
{"id":"f2a4af09-e073-4773-9322-3acff1778142","title":"Evaluate and integrate docker NPU image into workflows","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.513Z","uuid":"f2a4af09-e073-4773-9322-3acff1778142","created_at":"2025-09-15T02:02:58.513Z","path":"docs/agile/tasks/evaluate-docker-npu-compatability.md","content":"# Evaluate and integrate docker NPU image into workflows\n\nI recently added an Intel OpenVINO GenAI Model Server instance to our docker compose configuration. They should just work, but we have to make sure that the models are available to them. We will have to write a script to grab them.\n\nI'd like to be able to experiment with different models, but that is outside of the scope of this project. We have an [[ovm.json]] file with the models we're going to start out with.\n"}
{"id":"f40d381b-9c48-4dfc-9f89-51577c10c024","title":"<verb> <thing> <qualifier>","status":"todo","priority":"p3","owner":"","labels":["board:auto","lang:ts"],"created":"2025-09-15T02:02:58.517Z","uuid":"f40d381b-9c48-4dfc-9f89-51577c10c024","created_at":"2025-09-15T02:02:58.517Z","path":"docs/agile/tasks/post-move-stablization--compat-layer---hook-diet-.md","content":"# Task: Post-move stabilization (compat layer + hook diet)\n\n## Objective\n\nGet `pre-commit run -a` green **without** undoing the refactor, by:\n\n1. adding a thin compatibility layer for moved modules,\n2. tightening hook scope (stop linting logs/fixtures),\n3. pinning TypeScript/Python import roots,\n4. fixing the two real code issues surfaced.\n\n---\n\n## What to do (step-by-step)\n\n### 0) Create a stabilization branch\n\n* `git switch -c refactor/stabilize-precommit`\n\n### 1) Generate a move map (so you‚Äôre fixing facts, not vibes)\n\n* `git show --name-status HEAD > /tmp/move-map.txt`\n* skim for `R100/R9x` lines ‚Üí that‚Äôs your old ‚Üí new paths.\n* keep this file; you‚Äôll use it to build shims.\n\n### 2) TypeScript: lock imports to workspace roots\n\n* In the **root** `tsconfig.base.json` (or `tsconfig.json`):\n\n  * ensure:\n\n    ```json\n    {\n      \"compilerOptions\": {\n        \"baseUrl\": \".\",\n        \"paths\": {\n          \"@shared/*\": [\"shared/ts/*\"],\n          \"@services/*\": [\"services/js/*\"]\n        }\n      }\n    }\n    ```\n* Add **temporary re-export shims** for the most commonly imported moved modules (from the move map). Example:\n\n  * If `shared/ts/voice/src/voice-recorder.ts` moved to `shared/ts/voice/src/recorder/index.ts`, create:\n\n    ```\n    shared/ts/voice/src/voice-recorder.ts\n    -------------------------------------\n    export * from \"./recorder/index\";\n    ```\n  * Do *not* overdo it; hit only what the build/tests currently import.\n\n### 3) Prettier/ESLint: stop linting the trash heap\n\n* Create/update `.prettierignore` at repo root:\n\n  ```\n  # generated & logs\n  **/dist/**\n  **/build/**\n  **/coverage/**\n  **/.output/**\n  shared/ts/smartgpt-bridge/logs/**\n  **/*.lock\n  # intentionally broken fixture(s)\n  shared/ts/smartgpt-bridge/src/tests/fixtures/broken.ts\n  ```\n* In `.pre-commit-config.yaml`, scope prettier/eslint hooks with `files:`/`exclude:` so they don‚Äôt traverse logs/fixtures:\n\n  ```yaml\n  - id: prettier\n    files: '\\.(ts|tsx|js|mjs|cjs|json|md|yaml|yml)$'\n    exclude: '(logs/|fixtures/broken\\.ts$)'\n  ```\n\n### 4) Kill Hy from hook paths (this was the top failure)\n\n* Your `tsc-no-emit` and `lint-topics` hooks are shelling into `make` ‚Üí `Makefile.hy` ‚Üí `mk.generator` (now missing).\n* In `.pre-commit-config.yaml`, rewrite those hooks to call tools **directly**:\n\n  ```yaml\n  - id: tsc-no-emit\n    name: TypeScript compile check (no emit)\n    entry: bash -lc 'pnpm -w -r exec tsc --noEmit'\n    language: system\n    pass_filenames: false\n\n  - id: lint-topics\n    name: Topic/Schema lints\n    entry: node scripts/lint-topics.mjs\n    language: system\n    pass_filenames: false\n  ```\n* In your `Makefile`, change `ts-type-check` to the same `pnpm ‚Ä¶ tsc --noEmit` and **remove** any Hy imports from that target. Don‚Äôt delete Hy globally yet‚Äîjust stop calling it.\n\n### 5) Python: stop import-time explosions\n\n* Your import chain dies because `shared/py/__init__.py` eagerly imports a module that moved (`permissions`).\n* Make `shared/py/__init__.py` minimal (no eager imports):\n\n  ```python\n  # shared/py/__init__.py\n  __all__ = []\n  ```\n* Add a **compat shim** so callers don‚Äôt faceplant:\n\n  * Create `shared/py/permissions.py`:\n\n    ```python\n    # Temporary forwarder after moves. Remove once call sites updated.\n    try:\n        # point this to the new location:\n        from shared.py.auth.permissions import PermissionLayer, PermissionRule  # noqa: F401\n    except Exception as e:  # pragma: no cover\n        raise ImportError(\n            \"shared.py.permissions moved; update imports to shared.py.auth.permissions\"\n        ) from e\n    ```\n  * If the new path is different, adjust the import accordingly.\n* Fix missing names flagged by flake8:\n\n  * `shared/py/ml/hf_embeddings.py`: `from sentence_transformers import SentenceTransformer`\n  * `shared/py/speech/transcriber.py`:\n    `from scipy.io import wavfile`; `from urllib.parse import urlencode`; `import requests`\n  * `shared/py/speech/voice_synth.py`: `import requests`\n\n### 6) Requirements check: align with constraints & reality\n\n* The hook complains about `requirements.{gpu,cpu}.in` paths that don‚Äôt exist. Options:\n\n  * **Preferred:** update the checker config to only validate files that exist.\n  * **Pragmatic:** add stub files with only a constraints include:\n\n    ```\n    -c ../../../constraints.txt\n    ```\n* Remove any `-r` shared includes; per your policy, each service lists **direct** deps + `-c constraints.txt`.\n\n### 7) Re-run hooks in isolation (quick loops)\n\n```sh\npre-commit clean && pre-commit install\n\npre-commit run tsc-no-emit -a\npre-commit run flake8 -a\npre-commit run pytest -a     # should collect; if tests fail, fine‚Äîpre-commit just needs to run them\npre-commit run prettier -a\npre-commit run check-requirements-includes -a\npre-commit run lint-topics -a\n\npre-commit run -a            # full sweep\n```\n\n### 8) If TS still screams, it‚Äôs path fallout‚Äînot Hy\n\n* Use TS to tell you which imports are broken now that Hy is out of the way.\n* For any high-fanout import that moved, prefer **one** re-export shim (Step 2) over churning 200 call sites in this PR.\n\n### 9) Commit + PR\n\n* Commit message:\n\n  ```\n  chore(stabilize): pre-commit green after repo moves\n  - drop Hy from hook path\n  - add TS path mapping & re-export shims\n  - python compat shim for permissions\n  - tighten prettier scope; ignore logs/fixtures\n  - fix flake8 missing imports\n  - reconcile requirements includes with constraints\n  ```\n\n---\n\n## Follow-ups (new tasks, separate PRs)\n\n* **Rip shims**: migrate call sites to new paths, then delete TS/Python shims.\n* **Delete Hy** if it‚Äôs truly dead.\n* **CI gate**: add a required job that runs `pre-commit run -a` on clean checkout.\n* **Codemod**: use a simple jscodeshift/ts-morph + a Python script to rewrite imports using your move map (keep it opt-in).\n\n---\n\n## philosophy (because this is how it stays fixed)\n\n* Don‚Äôt chase every broken import today‚Äî**stabilize** the tooling so it stops fighting you.\n* Use **compat shims** to decouple refactor velocity from hook health.\n* Keep hooks pointed at **first-party tools** (pnpm/node/python), not Makefile wrappers that can drag in dead ecosystems.\n\n#precommit #refactor #typescript #python #imports #compat #monorepo #tooling #stabilization\n#accepted\n\n"}
{"id":"f446af1c-3616-478b-8bef-4e682d3ef17c","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.510Z","uuid":"f446af1c-3616-478b-8bef-4e682d3ef17c","created_at":"2025-09-15T02:02:58.510Z","path":"docs/agile/tasks/connect-wikipedia.md","content":"# Description\n\nIntegrate Wikipedia article lookup using the MediaWiki API for knowledge retrieval.\n\n## Target APIs and Authentication\n\n- **API**: [MediaWiki API](https://www.mediawiki.org/wiki/API:Main_page)\n- **Authentication**: Read requests require no auth but must send a descriptive `User-Agent`; OAuth is optional for high-volume or write operations\n\n## Data Flow & Rate Limiting\n\n- Agent issues search/query ‚ûú MediaWiki API ‚ûú parse summary ‚ûú internal knowledge store or broker\n- Follow Wikimedia guidelines: throttle to ~1 request/second and honor `Retry-After`/`maxlag` responses\n\n## Requirements/Definition of done\n## Requirements\n\n- Agents can query and retrieve article summaries\n- Data is routed through broker or storage services\n- Requests respect rate limits and include required headers\n\n## Tasks\n## Definition of Done\n\n- [ ] Implement client for search and page retrieval\n- [ ] Add User-Agent and optional OAuth credentials\n- [ ] Queue results into knowledge store or broker\n- [ ] Add throttling and `maxlag` handling\n- [ ] Integration tests for search and retrieval\n- [ ] Unit tests for rate-limit/backoff logic\n- [ ] Wikipedia API can be queried for article summaries\n- [ ] Retrieved content stored or returned in expected format\n- [ ] Unit test verifies lookup of a sample article\n\n## Story Points\n\n5\n\n## Tasks\n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Dependencies\n\n- Wikipedia API or library\n- Rate limiting awareness\n\n## Rough Scope\n\n- Determine whether to use direct API calls or existing library\n- Implement search and article retrieval\n- Normalize content for indexing or tool calls\n\n## Estimate\n\n- Story points: 2\n\n## Relevent resources\n\nYou might find [MediaWiki API docs](https://www.mediawiki.org/wiki/API:Main_page) useful while working on this task\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n\n#Ready\n\nNo blockers.\n\n#Ready\n#ready\n\n"}
{"id":"f4e15fe7-e616-4e34-94a6-a4ca655f4f4c","title":"Fix `@promethean/ui-components` lint failures","status":"todo","priority":"P2","owner":"","labels":["codex-task"],"created":"2025-02-14T00:00:00.000Z","uuid":"f4e15fe7-e616-4e34-94a6-a4ca655f4f4c","created_at":"2025-02-14T00:00:00.000Z","path":"docs/agile/tasks/fix_ui_components_lint.md","content":"# Fix `@promethean/ui-components` lint failures\n\n## Context\n\n- `nx run @promethean/ui-components:lint` currently fails and blocks the lint stage.\n- We need the lint target clean so Nx pipelines succeed.\n\n## Acceptance Criteria\n\n- [ ] `nx run @promethean/ui-components:lint` completes without errors.\n- [x] No new lint errors introduced in other packages.\n- [x] Document lint root cause and fix in this task note.\n\n## Plan\n\n1. Reproduce the lint failure and capture the errors.\n2. Update source files to satisfy the reported lint rules.\n3. Re-run the lint command to verify it passes.\n4. Prepare code review notes summarizing the fix.\n\n## Notes\n\n- Scope limited to `packages/ui-components` package.\n- Avoid modifying shared eslint config unless required by rule.\n\n## Outcome\n\n- Migrated the `UiChatPanel` and `UiFileExplorer` components to use shadow DOM templates instead of mutating host element style properties, satisfying the functional immutability rules.\n- Tightened tests to accumulate lint-friendly immutable tuples.\n- Added `.nxignore` to exclude the duplicate `packages/kanban-cli` stub so the Nx project graph resolves and allows lint runs.\n- `pnpm nx run @promethean/ui-components:lint` still fails with ESLint error on `ava.config.mjs` file.\n- Issue: ESLint config only covers `**/*.{ts,tsx}` files but lint script runs `eslint .` which includes all files.\n"}
{"id":"f5553298-1c51-41c2-acb5-df4d9f518c89","title":"Align task template frontmatter with canonical schema","status":"in_progress","priority":"P2","owner":"","labels":["docs","automation"],"created":"2025-09-30T20:05:49.925339+00:00","uuid":"f5553298-1c51-41c2-acb5-df4d9f518c89","created_at":"2025-09-30T20:05:49.925339+00:00","path":"docs/agile/tasks/update_task_template_frontmatter_alignment.md","content":"#InProgress\n\n## üõ†Ô∏è Description\n\nUpdate the shared task stub template so that new notes start with the canonical task metadata, generate the metadata automatically via Templater, and make sure the downstream kanban regeneration still works with the new structure.\n\n---\n\n## üéØ Goals\n\n- [x] Replace the legacy template frontmatter with the canonical task schema\n- [x] Populate required fields with Templater helpers and emit the matching status hashtag\n- [x] Demonstrate that board regeneration recognizes a note created from the updated template\n\n---\n\n## üì¶ Requirements\n\n- [x] Mirror the schema shown in `docs/agile/tasks/task-generator-system.md`\n- [x] Leverage `tp.user.uuidv4()` and `tp.date.now(...)` to prefill identifiers and timestamps\n- [x] Capture verification evidence after regenerating the board\n\n---\n\n## üìã Subtasks\n\n- [x] Draft and review the new template content\n- [x] Create a fresh task note using the new template for validation\n- [x] Run the board regeneration workflow and confirm the task appears\n\n---\n\n## üßÆ Story Points\n\n2\n\n---\n\n## üîó Related Epics\n\n- [[kanban]]\n\n---\n\n## ‚õìÔ∏è Blocked By\n\n- None\n\n## ‚õìÔ∏è Blocks\n\n- None\n\n---\n\n## üîç Relevant Links\n\n- `docs/agile/templates/task.stub.template.md`\n- `docs/agile/tasks/task-generator-system.md`\n- `node packages/kanban/dist/index.js regenerate`\n"}
{"id":"f6c39654-1e09-4741-9aeb-bdb200cc7216","title":"Expand functional loop refactors across repository","status":"todo","priority":"P3","owner":"","labels":["refactor","functional-style"],"created":"2025-09-18T19:29:29Z","uuid":"f6c39654-1e09-4741-9aeb-bdb200cc7216","created_at":"2025-09-18T19:29:29Z","path":"docs/agile/tasks/expand-functional-loop-refactors.md","content":"#Todo\n\n# Description\nDocument and schedule additional refactors that replace imperative loops with functional primitives (e.g., map, filter, reduce) in packages that still rely on mutable iteration.\n\n## Goals\n- Identify the highest-impact modules where loop refactors would improve readability and composability.\n- Capture any dependencies or blockers that require human coordination.\n- Provide clear acceptance criteria so future agents or maintainers can implement the changes confidently.\n\n## Requirements\n- Survey at least three packages that still rely heavily on imperative loops.\n- Record example locations with file paths and explain why a functional approach would help.\n- Note any tests or benchmarks that must be run after refactoring.\n\n## Subtasks\n- [ ] Audit loop-heavy modules and prioritize candidates for refactoring.\n- [ ] Outline expected functional transformations (map/filter/reduce/etc.) for each candidate.\n- [ ] Confirm testing strategy with the owning team or update documentation with required checks.\n\n## Comments\nUse this section for async coordination notes and links to relevant code reviews or discussions.\n"}
{"id":"f88c9d06-c8f3-4a60-96b7-9fddf412ba5e","title":"kanban processor","status":"todo","priority":"P3","owner":"","labels":["ecs","kanban","planning"],"created":"2025-09-15T02:02:58.515Z","uuid":"f88c9d06-c8f3-4a60-96b7-9fddf412ba5e","created_at":"2025-09-15T02:02:58.515Z","path":"docs/agile/tasks/kanban-processor.md","content":"\n## üßæ Summary\nUnify the historical kanban scripts into an ECS-powered experience that keeps Markdown + CLI flows authoritative while introducing modern interaction patterns.\n\n## üìé Attached Planning Docs\n- [[ecs_board_planning_note|ECS Board Planning Note]] ‚Üê circulate for sign-off before implementation.\n\n## ‚úÖ Coordination Checklist\n- [ ] Share the planning note with Platform DX, Board Ops/Agents, and Product stakeholders.\n- [ ] Capture Product/Engineering/Operations sign-off decisions in this task once approvals are granted.\n- [ ] Align subsequent slices to the acceptance criteria defined in the planning note.\n\n## ü§ù Stakeholders\n- Platform DX (CLI maintainers)\n- Board Ops / Agent team\n- Product owner for ECS initiative\n\n## üîç Relevant Links\n- [[kanban]]\n- `packages/kanban/src/index.ts`\n- `docs/agile/agents.md`\n\n#todo\n#accepted\n"}
{"id":"fa552127-22e2-45b7-85ca-0c9f5064ea48","title":"cephalon context window from collections","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.508Z","uuid":"fa552127-22e2-45b7-85ca-0c9f5064ea48","created_at":"2025-09-15T02:02:58.508Z","path":"docs/agile/tasks/cephalon_context_window_from_collections.md","content":"Cephalon: Build LLM context window from collections\n\nGoal: Ensure the LLM prompt context draws from `transcripts`, `agent_messages`, and any domain-specific collections using `ContextManager.compileContext` consistently in ECS orchestration.\n\nWhy: Without persisted agent/user messages in Mongo/Chroma, the agent loops. With persistence restored, wire context fetch for each turn.\n\nScope:\n- In `OrchestratorSystem` integration (wired in `start-dialog.scope.ts`), replace ad-hoc getContext callback with a call to `bot.context.compileContext([text])` including both user and agent histories.\n- Optionally add a `formatAssistantMessages` flag for role-annotated context.\n\nExit Criteria:\n- LLM requests include recent conversation and relevant memories from collections.\n- Unit test stubbing `ContextManager` to validate callback invocation.\n\n#incoming #cephalon #context #llm\n\n"}
{"id":"fb040f44-7250-4828-9f2b-c36f65ed35e3","title":"integrate sonarqube into devops","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.515Z","uuid":"fb040f44-7250-4828-9f2b-c36f65ed35e3","created_at":"2025-09-15T02:02:58.515Z","path":"docs/agile/tasks/integrate-sonarqube-into-devops.md","content":"### **Task: Integrate SonarQube into DevOps Pipeline**  \n**Description**:  \nAdd SonarQube to your DevOps pipeline to enforce code quality checks, detect bugs, and ensure compliance with coding standards. This involves configuring SonarQube analysis within your CI/CD workflow and aligning it with your team‚Äôs development practices.  \n\n**Steps**:  \n1. **Set Up SonarQube**: Install and configure SonarQube server, ensuring it is accessible via your DevOps environment.  \n2. **Configure Pipeline**: Integrate SonarQube into your CI/CD pipeline (e.g., using Jenkins, GitHub Actions, or GitLab CI) to run static code analysis during builds.  \n3. **Define Quality Gates**: Establish quality gates in SonarQube to block deployments if code quality thresholds (e.g., bug count, code duplication) are not met.  \n4. **Automate Reporting**: Generate and publish SonarQube reports (HTML, JSON) for team visibility and compliance tracking.  \n5. **Secure Access**: Ensure secure authentication between your pipeline and SonarQube (e.g., via tokens or service accounts).  \n\n**Relevant Resources**:  \n- [this] (refer to the linked resource for detailed configuration examples).  \n\n**Comments**:  \n- Append additional notes or adjustments to this task as needed for your specific DevOps tools or team workflows.  \n- Ensure alignment with existing processes outlined in [[integrate-sonarqube-into-devops]].  \n#incoming\n\n"}
{"id":"fb14db8b-0d66-4f4a-a30d-d7ef8631ddef","title":"task generator system","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.522Z","uuid":"fb14db8b-0d66-4f4a-a30d-d7ef8631ddef","created_at":"2025-09-15T02:02:58.522Z","path":"docs/agile/tasks/task-generator-system.md","content":"## üõ†Ô∏è Description\n\nCreate a utility that scaffolds new task files from a template to keep the board organized and consistent.\n\n---\n\n## üéØ Goals\n\n- Automate task file creation with required metadata\n- Reduce manual effort when adding items to the board\n\n---\n\n## üì¶ Requirements\n\n- [ ] Command-line script generates markdown from template\n- [ ] Ensures unique filenames and injects status hashtags\n- [ ] Includes minimal tests for generation logic\n\n---\n\n## üìã Subtasks\n\n- [ ] Define task template parameters\n- [ ] Implement generator script\n- [ ] Integrate with Makefile or npm script\n- [ ] Document usage in docs/agile/README\n\n---\n## üßÆ Story Points\n\n2\n\n---\n\n## üîó Related Epics\n\n#framework-core\n\n---\n\n## ‚õìÔ∏è Blocked By\n\nNothing\n\n## ‚õìÔ∏è Blocks\n\nNothing\n\n---\n\n## üîç Relevant Links\n\n- [[kanban]]\n\n#framework-core #Ready\n\n\n## Comments\n\n### Summary\n\n- The task spec still requires a command-line generator with unique filename/status handling and tests, but the change only adds Obsidian Templater definitions that invoke UI-specific commands, leaving the requested automation unmet.\n    \n\n### Issues\n\n1. **Task generator remains UI-dependent instead of a tested CLI utility** ‚Äî The specification calls for a command-line script, unique filename/hashtag injection, npm/Makefile integration, and accompanying tests.\n    \n\nThe implementation adds only an Obsidian Templater file that relies on `tp` macros and `chatgpt-md` UI commands, so nothing can run headless, no automation adds the required status hashtags, and no tests or npm hooks were introduced.\n\nSuggested taskBuild a tested CLI task generator that meets the spec\n"}
{"id":"fb4e2a3c-f44e-4418-87d5-bbe14ab1216f","title":"document etag semantics and cache headers for snap md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.512Z","uuid":"fb4e2a3c-f44e-4418-87d5-bbe14ab1216f","created_at":"2025-09-15T02:02:58.512Z","path":"docs/agile/tasks/document_etag_semantics_and_cache_headers_for_snap.md","content":"## üõ†Ô∏è Task: Document ETag semantics and cache headers for `/snap/:key`\n\nGenerated from [../unique/2025.08.08.20.08.83.md](../unique/2025.08.08.20.08.83.md)\n\n---\n\n## üéØ Goals\n- [ ] TODO\n\n---\n\n## üì¶ Requirements\n- [ ] TODO\n\n---\n\n## üìã Subtasks\n- [ ] TODO\n\n---\n\n\n#rejected\n\n"}
{"id":"fc716d8d-01f4-4fd7-bf67-db151d0dac03","title":"Description","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.514Z","uuid":"fc716d8d-01f4-4fd7-bf67-db151d0dac03","created_at":"2025-09-15T02:02:58.514Z","path":"docs/agile/tasks/flatten-sibilant-src-folders.md","content":"# Description\n\nDescribe your task\n\n## Requirements/Definition of done\n\n- If it doesn't have this, we can't accept it\n\n## Tasks \n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Relevent resources\n\nYou might find [this] useful while working on this task\n\n## Comments\n\nUseful for agents to engage in append only conversations about this task.\n"}
{"id":"fd144ce1-d6aa-41ff-a544-a03c583d071b","title":"set up new user roles and policies for the systems","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.520Z","uuid":"fd144ce1-d6aa-41ff-a544-a03c583d071b","created_at":"2025-09-15T02:02:58.520Z","path":"docs/agile/tasks/set-up-new-user-roles-and-policies-for-the-systems.md","content":"## üõ†Ô∏è Description\n\nRight now we only have an admin role and a simple policy matrix.\nWe want to be able to set up policies easily through an interface and config files.\n\nWe want permissions to be fine grain, such that I could lock an agent into ./docs, forbid specific files within docs to that same agent, prevent writes to certain files\n\nWe want it to be more than \"yes\" or \"no\" we want levels of approval like \"yes\", \"no\", \"with explicit permission from user in a session\", \"with explicit permission from user every time\", possibly defining the user that can give permission to be another agent who is solely responsible for managing certain kinds of permissions.\n\n## üì¶ Requirements\n- Support multiple user roles with distinct permission sets.\n- Policies configurable through both an interface and config files.\n- Allow path-level restrictions and multi-level approval workflows.\n\n## ‚úÖ Acceptance Criteria\n- System defines and enforces at least two new roles beyond admin.\n- An agent can be confined to a specific directory path.\n- Users can grant elevated access via explicit approval flow.\n\n## Tasks\n\n- [ ] Step 1\n- [ ] Step 2\n- [ ] Step 3\n- [ ] Step 4\n\n## Relevent resources\nYou might find [this] useful while working on this task\n\n## Comments\nUseful for agents to engage in append only conversations about this task.\n\n## Story Points\n\n- Estimate: 5\n- Assumptions: Existing auth framework can be extended for granular roles.\n- Dependencies: Permission schema design and configuration interface.\n#ready\n\n"}
{"id":"fe5eb9d4-18ff-46e5-8a30-81dffe1f711f","title":"refactor any python modules not currently for ml stuff discord etc 2 md","status":"todo","priority":"P3","owner":"","labels":[],"created":"2025-09-15T02:02:58.518Z","uuid":"fe5eb9d4-18ff-46e5-8a30-81dffe1f711f","created_at":"2025-09-15T02:02:58.518Z","path":"docs/agile/tasks/refactor_any_python_modules_not_currently_for_ml_stuff_discord_etc_2.md","content":"## üõ†Ô∏è Description\n**Status:** blocked\n\nMinimize Python usage in the Promethean framework by **isolating and containing Python code** to only where it is unavoidable‚Äîprimarily for machine learning model execution‚Äîwhile moving orchestration, glue code, and non-ML logic to faster, more maintainable languages (JavaScript/TypeScript, Sibilant, Hy, etc.).\n\nThe aim is to reduce Python‚Äôs footprint in the system, improve performance, and avoid Python-specific dependency and packaging complexity, without losing the ability to run ML models that are only available in Python ecosystems.\n\n---\n\n## üéØ Goals\n\n* Restrict Python usage to **model execution layers** only.\n* Replace Python-based orchestration and processing code with preferred languages.\n* Standardize **inter-language communication** (e.g., WebSockets, gRPC, shared memory).\n* Make Python components **self-contained microservices** to avoid polluting the rest of the stack.\n* Reduce deployment complexity by minimizing Python dependency chains.\n\n---\n\n## üì¶ Requirements\n\n* [ ] Audit current Python usage across services.\n* [ ] Identify Python code that can be rewritten or moved to other languages.\n* [ ] Define a standard **IPC or RPC mechanism** for connecting Python ML services to non-Python orchestration layers.\n* [ ] Implement containerized or isolated Python ML runners.\n* [ ] Document guidelines for when Python *is* acceptable.\n\n---\n\n## üìã Subtasks\n\n* [ ] Map all Python modules and their roles in the framework.\n* [ ] Flag Python code that is not strictly ML-bound for migration.\n* [ ] Build JS/TS wrappers for ML inference endpoints.\n* [ ] Implement service contracts for Python-based ML microservices.\n* [ ] Test latency and throughput impact of non-Python orchestration.\n* [ ] Remove unused Python dependencies and slim Python environments.\n\n---\n\n## üîó Related Epics\n\n#framework-core\n#language-strategy\n#performance-optimization\n\n## Blockers\n- No active owner or unclear scope\n\n#breakdown\n\n"}
