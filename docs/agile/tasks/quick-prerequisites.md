---
uuid: "29b1cb7a-1555-4d84-abe7-86cc25ca7e5d"
title: "Quick prerequisites"
slug: "quick-prerequisites"
status: "incoming"
priority: "P3"
labels: ["docops", "labeled"]
created_at: "2025-10-12T19:03:19.226Z"
estimates:
  complexity: ""
  scale: ""
  time_to_completion: ""
---




































































































































































## üóÇ Source

- Path: docs/labeled/2025.10.07.21.50.05.md

## üìù Context Summary

Love it. Let‚Äôs make this practical: build a few Qwen3 variants (quant + context tweaks), then run (1) a coding benchmark that‚Äôs respected and easy to run locally (HumanEval/MBPP via **EvalPlus**) and (2) a **tool-use** benchmark that‚Äôs become the default for function-calling (**BFCL**). I‚Äôm giving you a single Bash script that does the whole loop.

A couple of facts before the code (so you know I‚Äôm not hand-waving):

* **EvalPlus** has a native **Ollama backend**; you just point it at `http://localhost:11434/v1` and it works. ([GitHub][1])
* **BFCL** ships a `bfcl` CLI. It can talk to **pre-existing OpenAI-compatible endpoints** (skip server setup & set endpoint/port), which is exactly what Ollama exposes at `/v1`. We‚Äôll run the ‚Äúprompt‚Äù (non-native FC) track to avoid hard OpenAI‚Äêstyle tool schemas, which Ollama doesn‚Äôt implement natively yet. ([PyPI][2])
* To squeeze more into VRAM: (a) quantize the *weights* when creating the model (`ollama create --quantize q4_K_M|q5_K_M|q8_0`), and (b) quantize the **KV cache** with `OLLAMA_KV_CACHE_TYPE` (e.g., `q8_0`), ideally with Flash Attention on. You can also bump default context with `OLLAMA_CONTEXT_LENGTH`. ([ollama.readthedocs.io][3])

## üìã Tasks

- [ ] Draft actionable subtasks from the summary
- [ ] Define acceptance criteria
- [ ] Link back to related labeled docs



































































































































































