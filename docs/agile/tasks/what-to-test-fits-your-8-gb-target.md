---
uuid: "cad9400b-e3f6-4ae4-b818-6c8b23ae86b3"
title: "What to test (fits your 8 GB target)"
slug: "what-to-test-fits-your-8-gb-target"
status: "incoming"
priority: "P3"
labels: ["docops", "labeled"]
<<<<<<< HEAD
created_at: "2025-10-12T22:46:41.457Z"
=======
created_at: "2025-10-12T21:40:23.578Z"
>>>>>>> bug/kanban-duplication-issues
estimates:
  complexity: ""
  scale: ""
  time_to_completion: ""
---









































































































































































































































































<<<<<<< HEAD



















































































































=======
>>>>>>> bug/kanban-duplication-issues
## üóÇ Source

- Path: docs/labeled/2025.10.07.22.43.20.md

## üìù Context Summary

You‚Äôre on the right scent. With 8 GB VRAM you can realistically test a **tool-calling + reasoning** text model (e.g., Gemma 3 ‚Äútools‚Äù variants) and a **vision** model (MiniCPM-V 4.5) using CPU offload‚Äîthen compare them to remote judges (OpenRouter, Z.ai). Below is a tight plan + a TypeScript scaffold (ESM, FP-leaning, AVA tests, Web Components view) so you can start running real numbers today.

---

# What to test (fits your 8 GB target)

* **Tool-calling text (local)**
  Community Gemma 3 builds with *tools* are available on Ollama and explicitly aimed at low-VRAM rigs (several Q2/Q3 quant tags). These hook into Ollama‚Äôs native tool-calling API. ([Ollama][1])
  ‚Ä¢ Examples you can pull: `doomgrave/gemma3-tools` (Q2_K / Q3_K_S for ‚â§8 GB), `aliafshar/gemma3-it-qat-tools`, `Hituzip/gemma3-tools`, or *Gemma3n with tools* (`mashriram/gemma3nTools`). ([Ollama][1])

* **Vision (local)**
  **MiniCPM-V 4.5** has an Ollama card and a GGUF release; it‚Äôs designed for end-side deployment and supports ‚Äúfast‚Äù vs ‚Äúdeep‚Äù thinking modes. Expect to lean on CPU RAM and reduce image sizes on an 8 GB GPU. ([Ollama][2])

* **Reasoning (local) optional**
  **Unsloth** dynamic GGUFs for Qwen3/DeepSeek brin

## üìã Tasks

- [ ] Draft actionable subtasks from the summary
- [ ] Define acceptance criteria
- [ ] Link back to related labeled docs








































































































































































































































































<<<<<<< HEAD



















































































































=======
>>>>>>> bug/kanban-duplication-issues
