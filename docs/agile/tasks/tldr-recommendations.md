---
uuid: "5eabc953-f95c-47ae-940f-14ec84f1a745"
title: "tl;dr recommendations"
slug: "tldr-recommendations"
status: "incoming"
priority: "P3"
tags: ["docops", "labeled"]
created_at: "2025-10-10T03:23:55.970Z"
estimates:
  complexity: ""
  scale: ""
  time_to_completion: ""
---







## ğŸ—‚ Source

- Path: docs/labeled/2025.10.07.22.13.53.md

## ğŸ“ Context Summary

love this direction. unsloth gives you three wins that map to your goals:

1. dynamic GGUFs that actually hold up on serious coding evals (Polyglot/Aider), not just toy scores. vendor-claimed, but itâ€™s exactly the benchmark you wanted to run yourself. ([Unsloth Docs][1])
2. a clean Docker path for local fine-tuning/RL without dependency hell. ([Unsloth Docs][2])
3. one-click(ish) export to **GGUF** + auto **Modelfile** for **Ollama**, which fixes the #1 cause of â€œmy finetune is gibberish in Ollamaâ€: wrong chat template. ([Unsloth Docs][3])

below is a compact, practical pipeline thatâ€™s unsloth-native, runs **Aider Polyglot** for coding, and (optionally) **BFCL** for tool-use. it assumes youâ€™ll start from Unslothâ€™s prebuilt GGUFs (Qwen3-8B/14B, Qwen3-Coder, DeepSeek-V3.1) and/or export your own with Unsloth.

---

# tl;dr recommendations

* If your rig canâ€™t comfortably host 14B, start with **Unsloth Qwen3-8B GGUF** (UD Q5_K_M) and compare to **14B UD Q5_K_M**â€”same template, same ctx. ([Hugging Face][4])
* For serious coding, add **Qwen3-Coder-30B A3B (UD)** if RAM allows; itâ€™s what multiple third-party runs (AMD/Cline) found to be the first â€œreliably agenticâ€ local coder tier. ([Cl

## ğŸ“‹ Tasks

- [ ] Draft actionable subtasks from the summary
- [ ] Define acceptance criteria
- [ ] Link back to related labeled docs






