---
uuid: "31880e88-3d87-4d17-b739-8764165a93a4"
title: "Replace mock LLM integration with real @promethean/llm package"
slug: "Replace mock LLM integration with real @promethean llm package"
status: "incoming"
priority: "P1"
labels: ["llm", "integration", "package", "mock"]
created_at: "2025-10-13T05:00:38.078Z"
estimates:
  complexity: ""
  scale: ""
  time_to_completion: ""
lastCommitSha: "0595bd1132f4026c9c009e2cb83e47c23bf028cf"
commitHistory:
  -
    sha: "0595bd1132f4026c9c009e2cb83e47c23bf028cf"
    timestamp: "2025-10-19 17:05:08 -0500\n\ndiff --git a/docs/agile/tasks/Replace mock LLM integration with real @promethean llm package 2.md b/docs/agile/tasks/Replace mock LLM integration with real @promethean llm package 2.md\nindex 859566a21..600222131 100644\n--- a/docs/agile/tasks/Replace mock LLM integration with real @promethean llm package 2.md\t\n+++ b/docs/agile/tasks/Replace mock LLM integration with real @promethean llm package 2.md\t\n@@ -10,11 +10,14 @@ estimates:\n   complexity: \"\"\n   scale: \"\"\n   time_to_completion: \"\"\n-lastCommitSha: \"deec21fe4553bb49020b6aa2bdfee1b89110f15d\"\n-commitHistory: \n-  - sha: \"deec21fe4553bb49020b6aa2bdfee1b89110f15d\"\n-    timestamp: \"2025-10-19T16:27:40.282Z\"\n-    action: \"Bulk commit tracking initialization\"\n+lastCommitSha: \"0c140f121508744f78bb321911ead58d496fb391\"\n+commitHistory:\n+  -\n+    sha: \"0c140f121508744f78bb321911ead58d496fb391\"\n+    timestamp: \"2025-10-19 17:05:08 -0500\\n\\ndiff --git a/docs/agile/tasks/Refactor global state to dependency injection in indexer-core.md b/docs/agile/tasks/Refactor global state to dependency injection in indexer-core.md\\nindex 10569f5eb..c4319673f 100644\\n--- a/docs/agile/tasks/Refactor global state to dependency injection in indexer-core.md\\t\\n+++ b/docs/agile/tasks/Refactor global state to dependency injection in indexer-core.md\\t\\n@@ -10,11 +10,14 @@ estimates:\\n   complexity: \\\"\\\"\\n   scale: \\\"\\\"\\n   time_to_completion: \\\"\\\"\\n-lastCommitSha: \\\"deec21fe4553bb49020b6aa2bdfee1b89110f15d\\\"\\n-commitHistory: \\n-  - sha: \\\"deec21fe4553bb49020b6aa2bdfee1b89110f15d\\\"\\n-    timestamp: \\\"2025-10-19T16:27:40.282Z\\\"\\n-    action: \\\"Bulk commit tracking initialization\\\"\\n+lastCommitSha: \\\"8c13e3ad64cbd4e4fd1d1e97ef08276a228200cb\\\"\\n+commitHistory:\\n+  -\\n+    sha: \\\"8c13e3ad64cbd4e4fd1d1e97ef08276a228200cb\\\"\\n+    timestamp: \\\"2025-10-19 17:05:08 -0500\\\\n\\\\ndiff --git a/docs/agile/tasks/Refactor Large Files in agents-workflow Package.md b/docs/agile/tasks/Refactor Large Files in agents-workflow Package.md\\\\nindex 29704a184..500dcbad3 100644\\\\n--- a/docs/agile/tasks/Refactor Large Files in agents-workflow Package.md\\\\t\\\\n+++ b/docs/agile/tasks/Refactor Large Files in agents-workflow Package.md\\\\t\\\\n@@ -10,11 +10,14 @@ estimates:\\\\n   complexity: \\\\\\\"\\\\\\\"\\\\n   scale: \\\\\\\"\\\\\\\"\\\\n   time_to_completion: \\\\\\\"\\\\\\\"\\\\n-lastCommitSha: \\\\\\\"deec21fe4553bb49020b6aa2bdfee1b89110f15d\\\\\\\"\\\\n-commitHistory: \\\\n-  - sha: \\\\\\\"deec21fe4553bb49020b6aa2bdfee1b89110f15d\\\\\\\"\\\\n-    timestamp: \\\\\\\"2025-10-19T16:27:40.282Z\\\\\\\"\\\\n-    action: \\\\\\\"Bulk commit tracking initialization\\\\\\\"\\\\n+lastCommitSha: \\\\\\\"63cefd8085df82c82f4c0931a0113b524b3f6738\\\\\\\"\\\\n+commitHistory:\\\\n+  -\\\\n+    sha: \\\\\\\"63cefd8085df82c82f4c0931a0113b524b3f6738\\\\\\\"\\\\n+    timestamp: \\\\\\\"2025-10-19T22:05:08.079Z\\\\\\\"\\\\n+    message: \\\\\\\"Update task: 8ea3254c-3f78-4f09-9af6-b4ceba4c51f1 - Update task: Refactor Large Files in agents-workflow Package\\\\\\\"\\\\n+    author: \\\\\\\"Error <foamy125@gmail.com>\\\\\\\"\\\\n+    type: \\\\\\\"update\\\\\\\"\\\\n ---\\\\n \\\\n # Refactor Large Files in agents-workflow Package\\\\\\\\n\\\\\\\\n## üö® File Size Issues\\\\\\\\n\\\\\\\\n**Current Status**: Two files exceed the 300-line limit, making them difficult to maintain and understand\\\\\\\\n\\\\\\\\n### Files Requiring Refactoring:\\\\\\\\n\\\\\\\\n**src/providers/ollama.ts (332 lines) - 32 lines over limit:**\\\\\\\\n- Large OllamaModel class with multiple responsibilities\\\\\\\\n- Complex buildRequest method with high cognitive complexity\\\\\\\\n- Mixed concerns: model creation, request building, response handling\\\\\\\\n- Utility functions that could be extracted\\\\\\\\n\\\\\\\\n**src/workflow/loader.ts (324 lines) - 24 lines over limit:**\\\\\\\\n- Complex mergeDefinitions function (complexity: 24)\\\\\\\\n- Large resolveNodeDefinition function (52 lines)\\\\\\\\n- Mixed responsibilities: definition resolution, model resolution, tool resolution\\\\\\\\n- Multiple utility functions that could be modularized\\\\\\\\n\\\\\\\\n## üéØ Acceptance Criteria\\\\\\\\n\\\\\\\\n### File Size Reduction:\\\\\\\\n- [ ] **ollama.ts**: Reduce from 332 to ‚â§300 lines\\\\\\\\n- [ ] **loader.ts**: Reduce from 324 to ‚â§300 lines\\\\\\\\n- [ ] **Maintain functionality**: All existing features must work identically\\\\\\\\n- [ ] **Test coverage**: Maintain or improve existing test coverage\\\\\\\\n\\\\\\\\n### Code Organization:\\\\\\\\n- [ ] **Single responsibility**: Each module should have one clear purpose\\\\\\\\n- [ ] **Extracted utilities**: Move reusable functions to separate modules\\\\\\\\n- [ ] **Clear interfaces**: Well-defined module boundaries\\\\\\\\n- [ ] **Documentation**: Updated JSDoc for all public APIs\\\\\\\\n\\\\\\\\n## üîß Refactoring Strategy\\\\\\\\n\\\\\\\\n### Phase 1: Analysis & Planning (0.5 day)\\\\\\\\n1. **Dependency mapping**: Identify all imports and dependencies\\\\\\\\n2. **Function grouping**: Plan logical groupings for extraction\\\\\\\\n3. **Interface design**: Design clean module interfaces\\\\\\\\n4. **Test coverage review**: Ensure adequate test coverage for refactored code\\\\\\\\n\\\\\\\\n### Phase 2: ollama.ts Refactoring (1-1.5 days)\\\\\\\\n\\\\\\\\n**Current Structure Issues:**\\\\\\\\n\\\\\\\\n\\\\\\\\n**Proposed New Structure:**\\\\\\\\n\\\\\\\\n\\\\\\\\n**Extraction Plan:**\\\\\\\\n1. **types.ts**: Move OllamaClientLike, OllamaModelProviderOptions\\\\\\\\n2. **utils.ts**: Extract flattenEntries, toMessageContent, isMessageItem\\\\\\\\n3. **converters.ts**: Move convertTools, convertSettings, normalizeJsonSchema\\\\\\\\n4. **model.ts**: Focus only on OllamaModel class, reduce complexity\\\\\\\\n5. **provider.ts**: Clean OllamaModelProvider implementation\\\\\\\\n\\\\\\\\n### Phase 3: loader.ts Refactoring (1-1.5 days)\\\\\\\\n\\\\\\\\n**Current Structure Issues:**\\\\\\\\n\\\\\\\\n\\\\\\\\n**Proposed New Structure:**\\\\\\\\n\\\\\\\\n\\\\\\\\n**Extraction Plan:**\\\\\\\\n1. **definition-utils.ts**: Extract mergeDefinitions, isRecord\\\\\\\\n2. **file-loader.ts**: Move loadReferencedDefinition with security validation\\\\\\\\n3. **node-resolver.ts**: Focus on node definition resolution\\\\\\\\n4. **model-resolver.ts**: Extract model and tool resolution logic\\\\\\\\n5. **graph-builder.ts**: Clean workflow graph creation\\\\\\\\n\\\\\\\\n### Phase 4: Testing & Validation (0.5 day)\\\\\\\\n1. **Import updates**: Update all import statements\\\\\\\\n2. **Test validation**: Ensure all tests pass\\\\\\\\n3. **Integration testing**: Verify no breaking changes\\\\\\\\n4. **Performance validation**: Ensure no performance regression\\\\\\\\n\\\\\\\\n## üìä Success Metrics\\\\\\\\n\\\\\\\\n### Quantitative:\\\\\\\\n- [ ] **Line count**: ollama.ts ‚â§300 lines, loader.ts ‚â§300 lines\\\\\\\\n- [ ] **Function complexity**: All functions ‚â§15 complexity\\\\\\\\n- [ ] **Test coverage**: Maintain ‚â•90% coverage\\\\\\\\n- [ ] **Build time**: No increase in build time\\\\\\\\n\\\\\\\\n### Qualitative:\\\\\\\\n- [ ] **Readability**: Improved code organization and clarity\\\\\\\\n- [ ] **Maintainability**: Easier to modify and extend\\\\\\\\n- [ ] **Reusability**: Extracted utilities can be reused\\\\\\\\n- [ ] **Documentation**: Clear module boundaries and purposes\\\\\\\\n\\\\\\\\n## üîç Detailed Refactoring Examples\\\\\\\\n\\\\\\\\n### Example 1: ollama.ts Utility Extraction\\\\\\\\n\\\\\\\\n**Before (all in ollama.ts):**\\\\\\\\n\\\\\\\\n\\\\\\\\n**After (in ollama/utils.ts):**\\\\\\\\n\\\\\\\\n\\\\\\\\n### Example 2: loader.ts Complexity Reduction\\\\\\\\n\\\\\\\\n**Before (mergeDefinitions complexity: 24):**\\\\\\\\n\\\\\\\\n\\\\\\\\n**After (split into focused functions):**\\\\\\\\n\\\\\\\\n\\\\\\\\n## ‚õìÔ∏è Dependencies\\\\\\\\n\\\\\\\\n### Internal Dependencies:\\\\\\\\n- Current import structure within agents-workflow package\\\\\\\\n- Test files that import from these modules\\\\\\\\n- Other packages that depend on agents-workflow\\\\\\\\n\\\\\\\\n### External Dependencies:\\\\\\\\n- @openai/agents types and interfaces\\\\\\\\n- ollama package types\\\\\\\\n- Node.js filesystem APIs\\\\\\\\n\\\\\\\\n## ‚õìÔ∏è Blocks\\\\\\\\n- **Blocked by**: Fix Critical Linting Violations task (should do lint fixes first)\\\\\\\\n- **Blocks**: Enhanced error handling implementation\\\\\\\\n- **Blocks**: Performance optimization tasks\\\\\\\\n\\\\\\\\n## üöÄ Rollout Plan\\\\\\\\n\\\\\\\\n### Phase 1: Preparation\\\\\\\\n- [ ] Create new file structure\\\\\\\\n- [ ] Set up proper exports\\\\\\\\n- [ ] Update import statements gradually\\\\\\\\n\\\\\\\\n### Phase 2: Incremental Migration\\\\\\\\n- [ ] Extract utilities first (lowest risk)\\\\\\\\n- [ ] Refactor core classes\\\\\\\\n- [ ] Update tests incrementally\\\\\\\\n\\\\\\\\n### Phase 3: Validation\\\\\\\\n- [ ] Full test suite validation\\\\\\\\n- [ ] Integration testing with dependent packages\\\\\\\\n- [ ] Performance benchmarking\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n*This refactoring will significantly improve code maintainability and reduce cognitive load for developers working with the agents-workflow package.*\\\"\\n+    message: \\\"Update task: 8ea3254c-3f78-4f09-9af6-b4ceba4c51f1 - Update task: Refactor Large Files in agents-workflow Package\\\"\\n+    author: \\\"Error\\\"\\n+    type: \\\"update\\\"\\n ---\\n \\n The indexer-core uses global mutable state (CHROMA, EMBEDDING_FACTORY, etc.) which makes testing difficult, causes race conditions, and violates functional programming principles.\\\\n\\\\n**Global state issues:**\\\\n- Global variables make unit testing impossible\\\\n- Race conditions in concurrent scenarios\\\\n- Difficult to mock dependencies for testing\\\\n- Violates functional programming preferences\\\\n- Makes code harder to reason about\\\\n\\\\n**Refactoring approach:**\\\\n- Create IndexerContext class to manage state\\\\n- Implement dependency injection pattern\\\\n- Pass context to functions instead of using globals\\\\n- Add factory functions for creating configured instances\\\\n- Maintain backward compatibility during transition\\\\n\\\\n**Benefits:**\\\\n- Improved testability with mockable dependencies\\\\n- Better thread safety and concurrency handling\\\\n- Cleaner separation of concerns\\\\n- Easier configuration management\\\\n- Alignment with functional programming goals\\\\n\\\\n**Files affected:**\\\\n- packages/indexer-core/src/indexer.ts (major refactoring)\\\\n- Add context and factory modules\\\\n\\\\n**Priority:** MEDIUM - Architecture improvement\"\n+    message: \"Update task: 052d991b-3f5d-482d-8a44-8176f4091e5a - Update task: Refactor global state to dependency injection in indexer-core\"\n+    author: \"Error\"\n+    type: \"update\"\n ---\n \n ## ‚õìÔ∏è Blocked By"
    message: "Update task: 01798ced-1ace-41cc-9fc9-ac6225a69aa2 - Update task: Replace mock LLM integration with real @promethean llm package"
    author: "Error"
    type: "update"
---

## Overview\n\nReplace the mock LLM integration in packages/kanban/src/lib/task-content/ai.ts with the real @promethean/llm package to enable actual AI-powered task analysis, rewriting, and breakdown functionality.\n\n## Current State\n\n- The TaskAIManager class uses mockLLMGenerate function (lines 19-95) that returns hardcoded responses\n- TODO comment on line 18 indicates this is temporary until dependency issues are resolved\n- System is configured to use qwen3:8b model via ollama at localhost:11434\n- The @promethean/llm package exists and has proper ollama integration\n\n## Technical Requirements\n\n### 1. Dependency Integration\n\n- Add @promethean/llm as a dependency to packages/kanban/package.json\n- Import and use the generate function from @promethean/llm\n- Configure the LLM driver to use ollama with qwen3:8b model\n\n### 2. Code Replacement\n\n- Replace mockLLMGenerate function with real LLM integration\n- Maintain the same interface and return types for compatibility\n- Ensure proper error handling and timeout management\n- Preserve existing prompt engineering and validation logic\n\n### 3. Configuration\n\n- Use existing environment variables: LLM_DRIVER=ollama, LLM_MODEL=qwen3:8b\n- Ensure proper timeout handling (current config: 60 seconds)\n- Maintain retry logic for failed requests\n\n### 4. Testing Requirements\n\n- Add unit tests for real LLM integration\n- Mock LLM responses in test environment\n- Test error scenarios (network failures, timeouts)\n- Validate response format and structure\n\n## Implementation Details\n\n### Key Files to Modify\n\n1. packages/kanban/package.json - Add dependency\n2. packages/kanban/src/lib/task-content/ai.ts - Replace mock integration\n3. packages/kanban/src/lib/task-content/tests/ai.test.ts - Add tests (create if needed)\n\n### Integration Approach\n\n- Use @promethean/llm generate function with proper prompt formatting\n- Leverage existing buildAnalysisPrompt, buildRewritePrompt, buildBreakdownPrompt methods\n- Maintain JSON response parsing and validation\n- Preserve existing error handling patterns\n\n### Configuration Management\n\n- Respect existing TaskAIManagerConfig interface\n- Use environment variables for LLM configuration\n- Ensure backward compatibility with existing config options\n\n## Acceptance Criteria\n\n1. **Functional Integration**\n\n   - [ ] @promethean/llm dependency added and working\n   - [ ] All three AI methods (analyzeTask, rewriteTask, breakdownTask) use real LLM\n   - [ ] Responses are properly parsed and validated\n   - [ ] Error handling works for network failures and timeouts\n\n2. **Configuration**\n\n   - [ ] Uses ollama driver with qwen3:8b model by default\n   - [ ] Respects environment variables for configuration\n   - [ ] Maintains existing TaskAIManagerConfig interface\n\n3. **Testing**\n\n   - [ ] Unit tests added for LLM integration\n   - [ ] Mock responses work in test environment\n   - [ ] Error scenarios properly tested\n   - [ ] All existing tests continue to pass\n\n4. **Performance & Reliability**\n\n   - [ ] Requests timeout appropriately (60 seconds)\n   - [ ] Retry logic handles transient failures\n   - [ ] Memory usage remains reasonable\n   - [ ] No regression in existing functionality\n\n5. **Documentation**\n   - [ ] Code comments updated to reflect real integration\n   - [ ] TODO comment removed\n   - [ ] Any new configuration options documented\n\n## Dependencies & Prerequisites\n\n- @promethean/llm package must be built and available\n- Ollama service running with qwen3:8b model pulled\n- Network access to localhost:11434 for ollama\n- Proper environment variables set\n\n## Risks & Considerations\n\n1. **Network Dependencies**: Real LLM calls require ollama service availability\n2. **Response Variability**: Real LLM responses may differ from mock expectations\n3. **Performance**: Real LLM calls will be slower than mock responses\n4. **Error Handling**: Need robust handling of network failures and malformed responses\n5. **Testing**: Tests must mock LLM responses to avoid external dependencies\n\n## Success Metrics\n\n- All AI functionality works with real LLM responses\n- Response times are reasonable (< 30 seconds for most operations)\n- Error rate is low (< 5% for normal operations)\n- Test coverage remains above 80%\n- No regression in existing kanban functionality
