---
```
uuid: b543e648-991c-4365-8d76-280c60c2c31e
```
title: decouple from ollama md
status: todo
priority: P3
labels: []
```
created_at: '2025-09-15T02:02:58.510Z'
```
---
## ğŸ› ï¸ Task: Add Ollama formally to pipeline

It's difficult to get the system running for codex and CI/CD for them to properly run integration tests, or  just to sanity  test that it starts.



---

## ğŸ¯ Goals

- Allow the system to source models from a diverse set of LLM providers
- Open up the option to train custom LLM more easily for the system

---

## ğŸ“¦ Requirements

- System Can use any model from:
	- ollama
	- HuggingFace
	- ...npu/open vino?

---

## ğŸ“‹ Subtasks

- [ ] Write driver classes for llm service
- [ ] 


---

## ğŸ”— Related Epics
```
#framework-core
```
---

## â›“ï¸ Blocked By

Nothing

## â›“ï¸ Blocks

Nothing

---

## ğŸ” Relevant Links

- [[kanban]]
#ready

