---
$$
uuid: 1b526d91-244f-48d7-af9e-149f6c91bf40
$$
created_at: local-offline-model-deployment-strategy.md
filename: local-offline-model-deployment-strategy
title: local-offline-model-deployment-strategy
$$
description: >-
$$
  A fully-local, air-gapped deployment strategy that eliminates SaaS
  dependencies and telemetry. Models are served from a local volume, with TEI
  and Haystack configured to run offline using local model paths. Ollama is
  preloaded with models to avoid external pulls.
tags:
  - local
  - air-gapped
  - offline
  - model-deployment
  - tei
  - haystack
  - ollama
  - self-hosted
$$
related_to_uuid:
$$
  - 22b989d5-f4aa-4880-8632-709c21830f83
  - fc21f824-4244-4030-a48e-c4170160ea1d
  - 2792d448-c3b5-4050-93dd-93768529d99c
  - dd00677a-2280-45a7-91af-0728b21af3ad
  - 6620e2f2-de6d-45d8-a722-5d26e160b370
  - 291c7d91-da8c-486c-9bc0-bd2254536e2d
  - 1f32c94a-4da4-4266-8ac0-6c282cfb401f
  - e9b27b06-f608-4734-ae6c-f03a8b1fcf5f
  - a4a25141-6380-40b9-9cd7-b554b246b303
  - 5e408692-0e74-400e-a617-84247c7353ad
  - dd89372d-10de-42a9-8c96-6bc13ea36d02
  - 64a9f9f9-58ee-4996-bdaf-9373845c6b29
  - 1b1338fc-bb4d-41df-828f-e219cc9442eb
  - 13951643-1741-46bb-89dc-1beebb122633
  - 10d98225-12e0-4212-8e15-88b57cf7bee5
  - 78eeedf7-75bc-4692-a5a7-bb6857270621
  - ed6f3fc9-5eb1-482c-8b3c-f0abc5aff2a2
  - 30ec3ba6-fbca-4606-ac3e-89b747fbeb7c
  - 62bec6f0-4e13-4f38-aca4-72c84ba02367
  - 008f2ac0-bfaa-4d52-9826-2d5e86c0059f
  - e1056831-ae0c-460b-95fa-4cf09b3398c6
  - 49d1e1e5-5d13-4955-8f6f-7676434ec462
  - ba244286-4e84-425b-8bf6-b80c4eb783fc
  - ffb9b2a9-744d-4a53-9565-130fceae0832
  - b09141b7-544f-4c8e-8f49-bf76cecaacbb
$$
related_to_title:
$$
  - field-node-diagram-set
  - Fnord Tracer Protocol
  - Docops Feature Updates
  - heartbeat-fragment-demo
  - graph-ds
  - Ice Box Reorganization
  - field-node-diagram-outline
  - field-node-diagram-visualizations
  - Functional Embedding Pipeline Refactor
  - i3-bluetooth-setup
  - komorebi-group-window-hack
  - Layer1SurvivabilityEnvelope
  - Canonical Org-Babel Matplotlib Animation Template
  - Duck's Attractor States
  - Creative Moments
  - typed-struct-compiler
  - Unique Concepts
  - Unique Info Dump Index
  - zero-copy-snapshots-and-workers
  - eidolon-field-math-foundations
  - RAG UI Panel with Qdrant and PostgREST
  - EidolonField
  - System Scheduler with Resource-Aware DAG
  - obsidian-ignore-node-modules-regex
  - field-interaction-equations
references:
  - uuid: 78eeedf7-75bc-4692-a5a7-bb6857270621
    line: 1016
    col: 0
    score: 1
  - uuid: ed6f3fc9-5eb1-482c-8b3c-f0abc5aff2a2
    line: 175
    col: 0
    score: 1
  - uuid: 30ec3ba6-fbca-4606-ac3e-89b747fbeb7c
    line: 1221
    col: 0
    score: 1
  - uuid: 62bec6f0-4e13-4f38-aca4-72c84ba02367
    line: 1058
    col: 0
    score: 1
  - uuid: 1b1338fc-bb4d-41df-828f-e219cc9442eb
    line: 515
    col: 0
    score: 1
  - uuid: 10d98225-12e0-4212-8e15-88b57cf7bee5
    line: 251
    col: 0
    score: 1
  - uuid: 13951643-1741-46bb-89dc-1beebb122633
    line: 559
    col: 0
    score: 1
  - uuid: 008f2ac0-bfaa-4d52-9826-2d5e86c0059f
    line: 1033
    col: 0
    score: 1
  - uuid: 2792d448-c3b5-4050-93dd-93768529d99c
    line: 226
    col: 0
    score: 1
  - uuid: 1f32c94a-4da4-4266-8ac0-6c282cfb401f
    line: 705
    col: 0
    score: 1
  - uuid: 22b989d5-f4aa-4880-8632-709c21830f83
    line: 719
    col: 0
    score: 1
  - uuid: e9b27b06-f608-4734-ae6c-f03a8b1fcf5f
    line: 601
    col: 0
    score: 1
  - uuid: fc21f824-4244-4030-a48e-c4170160ea1d
    line: 1060
    col: 0
    score: 1
  - uuid: a4a25141-6380-40b9-9cd7-b554b246b303
    line: 726
    col: 0
    score: 1
  - uuid: 6620e2f2-de6d-45d8-a722-5d26e160b370
    line: 996
    col: 0
    score: 1
  - uuid: dd00677a-2280-45a7-91af-0728b21af3ad
    line: 667
    col: 0
    score: 1
  - uuid: 5e408692-0e74-400e-a617-84247c7353ad
    line: 736
    col: 0
    score: 1
  - uuid: 291c7d91-da8c-486c-9bc0-bd2254536e2d
    line: 645
    col: 0
    score: 1
  - uuid: dd89372d-10de-42a9-8c96-6bc13ea36d02
    line: 739
    col: 0
    score: 1
  - uuid: 64a9f9f9-58ee-4996-bdaf-9373845c6b29
    line: 816
    col: 0
    score: 1
  - uuid: e1056831-ae0c-460b-95fa-4cf09b3398c6
    line: 351
    col: 0
    score: 0.98
  - uuid: 49d1e1e5-5d13-4955-8f6f-7676434ec462
    line: 207
    col: 0
    score: 0.95
  - uuid: ba244286-4e84-425b-8bf6-b80c4eb783fc
    line: 358
    col: 0
    score: 0.93
  - uuid: fc21f824-4244-4030-a48e-c4170160ea1d
    line: 399
    col: 0
    score: 0.93
  - uuid: ffb9b2a9-744d-4a53-9565-130fceae0832
    line: 215
    col: 0
    score: 0.93
  - uuid: b09141b7-544f-4c8e-8f49-bf76cecaacbb
    line: 301
    col: 0
    score: 0.93
  - uuid: 9a8ab57e-507c-4c6b-aab4-01cea1bc0501
    line: 129
    col: 0
    score: 0.89
  - uuid: aee4718b-9f8b-4635-a0c1-ef61c9bea8f1
    line: 307
    col: 0
    score: 0.89
  - uuid: d2b3628c-6cad-4664-8551-94ef8280851d
    line: 504
    col: 0
    score: 0.89
  - uuid: b4e64f8c-4dc9-4941-a877-646c5ada068e
    line: 348
    col: 0
    score: 0.88
  - uuid: c14edce7-0656-45b2-aaf3-51f042451b7d
    line: 388
    col: 0
    score: 0.87
  - uuid: 63a1cc28-b85c-4ce2-b754-01c2bc0c0bc3
    line: 491
    col: 0
    score: 0.87
  - uuid: 41ce0216-f8cc-4eed-8d9a-fcc25be21425
    line: 10
    col: 0
    score: 0.86
  - uuid: f8877e5e-1e4f-4478-93cd-a0bf86d26a41
    line: 44
    col: 0
    score: 0.86
  - uuid: d28090ac-f746-4958-aab5-ed1315382c04
    line: 28
    col: 0
    score: 0.86
  - uuid: 51932e7b-4237-4756-bcae-8be6d535d0d1
    line: 217
    col: 0
    score: 0.86
  - uuid: c5c5ff1c-d1bc-45c7-8a84-55a4a847dfc5
    line: 185
    col: 0
    score: 0.86
  - uuid: cf6b9b17-bb91-4219-aa5c-172cba02b2da
    line: 111
    col: 0
    score: 0.85
  - uuid: 5f210ca2-54e9-445b-afe4-fb340d4992c5
    line: 150
    col: 0
    score: 0.85
  - uuid: 6498b9d7-bd35-4bd3-89fb-af1c415c3cd1
    line: 427
    col: 0
    score: 0.85
  - uuid: babdb9eb-3b15-48a7-8a22-ecc53af7d397
    line: 147
    col: 0
    score: 0.85
  - uuid: af5d2824-faad-476c-a389-e912d9bc672c
    line: 120
    col: 0
    score: 0.85
---
Absolutely. Here’s a **fully-local, air-gapped** revision: no SaaS hooks, no tokens, no phone-home where we can disable it. Models served from a **local `/models`** volume; TEI/Haystack run **offline**. If you haven’t cached models yet, drop them into `./infra/models/**` before starting. ^ref-ad7f1ed3-1-0

---

# 1) Compose deltas — purge SaaS & lock down telemetry

### Remove these services entirely

* `oauth2-proxy` $GitHub provider = SaaS$ ^ref-ad7f1ed3-9-0
* Any OpenAI/remote LLM refs (none were in compose, keep it that way) ^ref-ad7f1ed3-10-0

### Keep these $all self-hosted/local$: Traefik/NGINX, Grafana, Prometheus, Loki, Tempo, Promtail, cAdvisor, node\_exporter, Portainer, Postgres, Mongo, Redis, Meili, MinIO, OpenSearch, Ollama, TEI, Haystack, SonarQube, Gitea, Drone (with Gitea), Keycloak, CrowdSec, NATS/RabbitMQ/Mosquitto, Temporal, Airflow, Scrapy/Playwright/Selenium, code-server.

### Add a shared models volume

```yaml
volumes:
  models:
```
$$
^ref-ad7f1ed3-16-0
$$
### Mount models + force offline on AI services
$$
 ^ref-ad7f1ed3-23-0
$$
$$
**TEI (embeddings & CLIP)**
$$
$$
 ^ref-ad7f1ed3-25-0
$$
```yaml
  tei-embeddings:
    profiles: ["ai"]
    image: ghcr.io/huggingface/text-embeddings-inference:1.6
    environment:
      - MODEL_ID=/models/nomic-embed-text-v1.5        # local path, not HF id
      - NUM_SHARD=1
      - HF_HUB_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
    volumes:
      - models:/models:ro
    ports: ["8081:80"]
    networks: [prom-net]
    restart: unless-stopped

  tei-clip:
    profiles: ["ai","vision"]
    image: ghcr.io/huggingface/text-embeddings-inference:1.6
    environment:
      - MODEL_ID=/models/openai-clip-vit-large-patch14  # local path mirror
      - TASK=feature-extraction
      - HF_HUB_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
    volumes:
      - models:/models:ro
    ports: ["8082:80"]
    networks: [prom-net]
    restart: unless-stopped
^ref-ad7f1ed3-25-0
```

**Haystack (no external pulls)** ^ref-ad7f1ed3-57-0

```yaml
  haystack:
    profiles: ["ai","rag"]
    image: deepset/haystack:base
    environment:
      - PIPELINE_YAML=/app/pipelines/default.yaml
      - HF_HUB_OFFLINE=1
      - TRANSFORMERS_OFFLINE=1
      - TORCH_HOME=/models/torch
      - TRANSFORMERS_CACHE=/models/transformers
    volumes:
      - ./infra/haystack:/app/pipelines
      - models:/models:ro
    ports: ["8000:8000"]
    networks: [prom-net]
    restart: unless-stopped
^ref-ad7f1ed3-57-0
    depends_on: [opensearch, meilisearch, postgres] ^ref-ad7f1ed3-76-0
```
$$
 ^ref-ad7f1ed3-78-0
$$
$$
**Ollama (local only)**
$$
$$
 ^ref-ad7f1ed3-80-0
$$
> To keep it offline, preload models to `./infra/ollama` (modelfiles and blobs) and avoid `ollama pull`.

```yaml
  ollama:
    profiles: ["ai"]
    image: ollama/ollama:0.3.14
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_OFFLINE=1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
              driver: "nvidia"
    volumes:
      - ./infra/ollama:/root/.ollama   # local model store
      - /usr/share/fonts:/usr/share/fonts:ro
    ports: ["11434:11434"]
^ref-ad7f1ed3-80-0
    networks: [prom-net]
    restart: unless-stopped
```
$$
^ref-ad7f1ed3-105-0
$$
### Disable analytics/telemetry where possible

```yaml
  grafana:
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASS:-admin}
      - GF_ANALYTICS_REPORTING_DISABLED=true
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
      - GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s/grafana/

  meilisearch:
    environment:
      - MEILI_NO_ANALYTICS=true

  loki:
    environment:
      - LOKI_DISABLE_LOGS_STORAGE_METRICS=true
    # already set: analytics.reporting_enabled: false (in loki config)

  opensearch:
    environment:
      - discovery.type=single-node
      - plugins.security.disabled=true
      - bootstrap.memory_lock=true
      - OPENSEARCH_JAVA_OPTS=-Xms2g -Xmx2g
      - compatibility.override_main_response_version=true
      - logger.deprecation.level=error
      - plugins.query.stage.threshold=0
    # stays local; no telemetry configured

  code-server:
    environment:
      - PASSWORD=${CODE_SERVER_PASSWORD:-code}
      - DO_NOT_TRACK=1

^ref-ad7f1ed3-105-0
  keycloak:
    # Dev mode is fine inside your LAN; stays local.
    command: start-dev ^ref-ad7f1ed3-146-0
^ref-ad7f1ed3-146-0
```
$$
^ref-ad7f1ed3-146-0
$$
### Traefik labels: keep basic auth local (no IdP)
$$
 ^ref-ad7f1ed3-152-0
$$
Use htpasswd and your own secrets file (no OAuth providers). Nothing external here.
$$
 ^ref-ad7f1ed3-152-0
$$
---
$$
 ^ref-ad7f1ed3-156-0
$$
# 2) Haystack pipeline (BM25 only, no model downloads)

If you want **zero model downloads**, return the top-k passages directly $no LLM/reader$. You still get a working API.
$$
`infra/haystack/default.yaml`
$$
```yaml
version: 2
components:
  - name: DocumentStore
    type: OpenSearchDocumentStore
    params:
      host: opensearch
      port: 9200
      scheme: http
      index: documents
      verify_certs: false

  - name: Retriever
    type: BM25Retriever
    params:
      document_store: DocumentStore
      top_k: 10

  - name: Joiner
    type: JoinDocuments
    params:
      join_mode: concatenate
      weights: [1.0]

pipelines:
  - name: indexing
    nodes:
      - name: FileTypeRouter
        type: FileTypeRouter
        inputs: [File]
        params:
          mime_types:
            text/plain: TextConverter
      - name: TextConverter
        type: TextConverter
        inputs: [FileTypeRouter]
        params:
          clean_whitespace: true
          clean_empty_lines: true
      - name: Preprocessor
        type: PreProcessor
        inputs: [TextConverter]
        params:
          split_length: 200
          split_overlap: 20
          split_respect_sentence_boundary: true
      - name: DocumentStore
        inputs: [Preprocessor]

  - name: query
    nodes:
^ref-ad7f1ed3-156-0
      - name: Retriever
        inputs: [Query]
      - name: Joiner ^ref-ad7f1ed3-215-0
^ref-ad7f1ed3-217-0
^ref-ad7f1ed3-215-0
        inputs: [Retriever]
^ref-ad7f1ed3-217-0
^ref-ad7f1ed3-215-0
```
$$
^ref-ad7f1ed3-217-0
$$
$$
^ref-ad7f1ed3-215-0
$$
### Optional: **local reader** (still offline)

If you want answers, add a local reader that points to **pre-downloaded weights** in `/models`. Example:

```yaml
  - name: Reader
    type: FARMReader
    params:
      model_name_or_path: /models/deepset-roberta-base-squad2   # local dir
      use_gpu: false
      no_ans_boost: -10

pipelines:
  - name: query
^ref-ad7f1ed3-217-0
    nodes: ^ref-ad7f1ed3-234-0
      - name: Retriever
^ref-ad7f1ed3-234-0 ^ref-ad7f1ed3-240-0
        inputs: [Query]
^ref-ad7f1ed3-240-0
^ref-ad7f1ed3-234-0 ^ref-ad7f1ed3-246-0
      - name: Reader
^ref-ad7f1ed3-249-0
^ref-ad7f1ed3-248-0
^ref-ad7f1ed3-246-0
^ref-ad7f1ed3-240-0
^ref-ad7f1ed3-234-0
^ref-ad7f1ed3-230-0 ^ref-ad7f1ed3-255-0
        inputs: [Retriever] ^ref-ad7f1ed3-248-0
```$$
 ^ref-ad7f1ed3-249-0
$$
$$
^ref-ad7f1ed3-232-0
$$
$$
 ^ref-ad7f1ed3-240-0
$$
Keep `HF_HUB_OFFLINE=1` and ensure `/models/deepset-roberta-base-squad2/**` exists. ^ref-ad7f1ed3-246-0
$$
--- ^ref-ad7f1ed3-248-0
$$
$$
 ^ref-ad7f1ed3-249-0
$$
# 3) TEI with local model dirs (embeddings later) ^ref-ad7f1ed3-255-0
$$
 ^ref-ad7f1ed3-246-0
$$
When you’re ready for vector search, point OpenSearch to embedding fields and **use TEI offline** (above). Your retriever becomes `EmbeddingRetriever` with `embedding_model:  (no tokens needed). Again: models must live under `/models`.
$$
 ^ref-ad7f1ed3-248-0
$$
$$
--- ^ref-ad7f1ed3-249-0
$$
$$
 ^ref-ad7f1ed3-255-0
$$
# 4) Caching models $one-time, offline-friendly$

You have two options:

* **Manual**: copy model directories into `./infra/models/**` so containers see them at `/models/**`.
* **Air-gapped mirror**: on a machine with internet, `git lfs clone` or `huggingface-cli download ...` and then rsync the directories into `./infra/models/`. Inside containers we set `HF_HUB_OFFLINE=1`, so nothing reaches out.

---

# 5) Quick start $local-only$

```bash
# 1) put models in ./infra/models/**
#    e.g. ./infra/models/nomic-embed-text-v1.5, ./infra/models/openai-clip-vit-large-patch14

# 2) bring up core infra
docker compose --profile data --profile observability --profile edge up -d

# 3) bring up AI (offline)
docker compose --profile ai up -d

# 4) bring up RAG API
docker compose --profile rag up -d

# 5) index and query locally
^ref-ad7f1ed3-255-0
docker exec -it haystack bash -lc \
  "haystack pipeline run --pipeline indexing --file /app/pipelines/samples/README.txt"
^ref-ad7f1ed3-283-0
^ref-ad7f1ed3-282-0
^ref-ad7f1ed3-290-0 ^ref-ad7f1ed3-291-0
^ref-ad7f1ed3-288-0 ^ref-ad7f1ed3-292-0
^ref-ad7f1ed3-297-0
^ref-ad7f1ed3-295-0 ^ref-ad7f1ed3-302-0
^ref-ad7f1ed3-293-0
^ref-ad7f1ed3-292-0
^ref-ad7f1ed3-291-0 ^ref-ad7f1ed3-305-0
^ref-ad7f1ed3-290-0 ^ref-ad7f1ed3-306-0
^ref-ad7f1ed3-288-0 ^ref-ad7f1ed3-307-0
^ref-ad7f1ed3-286-0
^ref-ad7f1ed3-283-0
^ref-ad7f1ed3-282-0
^ref-ad7f1ed3-286-0 ^ref-ad7f1ed3-293-0
^ref-ad7f1ed3-283-0
^ref-ad7f1ed3-282-0 ^ref-ad7f1ed3-295-0
 ^ref-ad7f1ed3-286-0
curl -s  \ ^ref-ad7f1ed3-297-0
  -H 'Content-Type: application/json' \ ^ref-ad7f1ed3-288-0
  -d '{"query":"What is in the README?","params":{"Retriever":{"top_k":5}}}' ^ref-ad7f1ed3-282-0
```$$
 ^ref-ad7f1ed3-283-0 ^ref-ad7f1ed3-290-0
$$
$$
 ^ref-ad7f1ed3-291-0
$$
$$
--- ^ref-ad7f1ed3-292-0 ^ref-ad7f1ed3-302-0
$$
$$
 ^ref-ad7f1ed3-286-0 ^ref-ad7f1ed3-293-0
$$
## Sanity notes
$$
 ^ref-ad7f1ed3-288-0 ^ref-ad7f1ed3-295-0 ^ref-ad7f1ed3-305-0
$$
* **No tokens anywhere.** ^ref-ad7f1ed3-306-0
* **No SaaS IdPs.** Use **Keycloak** if you need SSO, all local. ^ref-ad7f1ed3-290-0 ^ref-ad7f1ed3-297-0 ^ref-ad7f1ed3-307-0
* **No model downloads at runtime.** Everything mounted from `/models`. ^ref-ad7f1ed3-291-0
* **Telemetry** disabled where supported. ^ref-ad7f1ed3-292-0
$$
 ^ref-ad7f1ed3-293-0
$$
Want me to spit out a **one-shot script** that validates all mounted model dirs exist (and fails fast) before boot? Or trim the stack further $e.g., drop Portainer/code-server if you want zero optional surfaces$?
$$
 ^ref-ad7f1ed3-295-0 ^ref-ad7f1ed3-302-0
$$
\#infrastructure #airgapped #selfhosted #docker #docker-compose #rag #search #observability #security
$$
-249-0
$$
$$
^ref-ad7f1ed3-232-0
$$
$$
 ^ref-ad7f1ed3-240-0
$$
Keep `HF_HUB_OFFLINE=1` and ensure `/models/deepset-roberta-base-squad2/**` exists. ^ref-ad7f1ed3-246-0
$$
--- ^ref-ad7f1ed3-248-0
$$
$$
 ^ref-ad7f1ed3-249-0
$$
# 3) TEI with local model dirs (embeddings later) ^ref-ad7f1ed3-255-0
$$
 ^ref-ad7f1ed3-246-0
$$
When you’re ready for vector search, point OpenSearch to embedding fields and **use TEI offline** (above). Your retriever becomes `EmbeddingRetriever` with `embedding_model:  (no tokens needed). Again: models must live under `/models`.
$$
 ^ref-ad7f1ed3-248-0
$$
$$
--- ^ref-ad7f1ed3-249-0
$$
$$
 ^ref-ad7f1ed3-255-0
$$
# 4) Caching models $one-time, offline-friendly$

You have two options:

* **Manual**: copy model directories into `./infra/models/**` so containers see them at `/models/**`.
* **Air-gapped mirror**: on a machine with internet, `git lfs clone` or `huggingface-cli download ...` and then rsync the directories into `./infra/models/`. Inside containers we set `HF_HUB_OFFLINE=1`, so nothing reaches out.

---

# 5) Quick start $local-only$

```bash
# 1) put models in ./infra/models/**
#    e.g. ./infra/models/nomic-embed-text-v1.5, ./infra/models/openai-clip-vit-large-patch14

# 2) bring up core infra
docker compose --profile data --profile observability --profile edge up -d

# 3) bring up AI (offline)
docker compose --profile ai up -d

# 4) bring up RAG API
docker compose --profile rag up -d

# 5) index and query locally
^ref-ad7f1ed3-255-0
docker exec -it haystack bash -lc \
  "haystack pipeline run --pipeline indexing --file /app/pipelines/samples/README.txt"
^ref-ad7f1ed3-283-0
^ref-ad7f1ed3-282-0
^ref-ad7f1ed3-290-0 ^ref-ad7f1ed3-291-0
^ref-ad7f1ed3-288-0 ^ref-ad7f1ed3-292-0
^ref-ad7f1ed3-297-0
^ref-ad7f1ed3-295-0 ^ref-ad7f1ed3-302-0
^ref-ad7f1ed3-293-0
^ref-ad7f1ed3-292-0
^ref-ad7f1ed3-291-0 ^ref-ad7f1ed3-305-0
^ref-ad7f1ed3-290-0 ^ref-ad7f1ed3-306-0
^ref-ad7f1ed3-288-0 ^ref-ad7f1ed3-307-0
^ref-ad7f1ed3-286-0
^ref-ad7f1ed3-283-0
^ref-ad7f1ed3-282-0
^ref-ad7f1ed3-286-0 ^ref-ad7f1ed3-293-0
^ref-ad7f1ed3-283-0
^ref-ad7f1ed3-282-0 ^ref-ad7f1ed3-295-0
 ^ref-ad7f1ed3-286-0
curl -s  \ ^ref-ad7f1ed3-297-0
  -H 'Content-Type: application/json' \ ^ref-ad7f1ed3-288-0
  -d '{"query":"What is in the README?","params":{"Retriever":{"top_k":5}}}' ^ref-ad7f1ed3-282-0
```$$
 ^ref-ad7f1ed3-283-0 ^ref-ad7f1ed3-290-0
$$
$$
 ^ref-ad7f1ed3-291-0
$$
$$
--- ^ref-ad7f1ed3-292-0 ^ref-ad7f1ed3-302-0
$$
$$
 ^ref-ad7f1ed3-286-0 ^ref-ad7f1ed3-293-0
$$
## Sanity notes
$$
 ^ref-ad7f1ed3-288-0 ^ref-ad7f1ed3-295-0 ^ref-ad7f1ed3-305-0
$$
* **No tokens anywhere.** ^ref-ad7f1ed3-306-0
* **No SaaS IdPs.** Use **Keycloak** if you need SSO, all local. ^ref-ad7f1ed3-290-0 ^ref-ad7f1ed3-297-0 ^ref-ad7f1ed3-307-0
* **No model downloads at runtime.** Everything mounted from `/models`. ^ref-ad7f1ed3-291-0
* **Telemetry** disabled where supported. ^ref-ad7f1ed3-292-0
$$
 ^ref-ad7f1ed3-293-0
$$
Want me to spit out a **one-shot script** that validates all mounted model dirs exist (and fails fast) before boot? Or trim the stack further $e.g., drop Portainer/code-server if you want zero optional surfaces$?
$$
 ^ref-ad7f1ed3-295-0 ^ref-ad7f1ed3-302-0
$$
\#infrastructure #airgapped #selfhosted #docker #docker-compose #rag #search #observability #security
<!-- GENERATED-SECTIONS:DO-NOT-EDIT-BELOW -->
## Related content
- $field-node-diagram-set$$field-node-diagram-set.md$
- [Fnord Tracer Protocol]$fnord-tracer-protocol.md$
- [Docops Feature Updates]$docops-feature-updates.md$
- $heartbeat-fragment-demo$$heartbeat-fragment-demo.md$
- $graph-ds$$graph-ds.md$
- [Ice Box Reorganization]$ice-box-reorganization.md$
- $field-node-diagram-outline$$field-node-diagram-outline.md$
- $field-node-diagram-visualizations$$field-node-diagram-visualizations.md$
- [Functional Embedding Pipeline Refactor]$functional-embedding-pipeline-refactor.md$
- $i3-bluetooth-setup$$i3-bluetooth-setup.md$
- $komorebi-group-window-hack$$komorebi-group-window-hack.md$
- [Layer1SurvivabilityEnvelope](layer1survivabilityenvelope.md)
- $Canonical Org-Babel Matplotlib Animation Template$$canonical-org-babel-matplotlib-animation-template.md$
- [Duck's Attractor States]$ducks-attractor-states.md$
- [Creative Moments]$creative-moments.md$
- $typed-struct-compiler$$typed-struct-compiler.md$
- [Unique Concepts]$unique-concepts.md$
- [Unique Info Dump Index]$unique-info-dump-index.md$
- $zero-copy-snapshots-and-workers$$zero-copy-snapshots-and-workers.md$
- $eidolon-field-math-foundations$$eidolon-field-math-foundations.md$
- [RAG UI Panel with Qdrant and PostgREST]$rag-ui-panel-with-qdrant-and-postgrest.md$
- [EidolonField](eidolonfield.md)
- $System Scheduler with Resource-Aware DAG$$system-scheduler-with-resource-aware-dag.md$
- $obsidian-ignore-node-modules-regex$$obsidian-ignore-node-modules-regex.md$
- $field-interaction-equations$$field-interaction-equations.md$
## Sources
- $typed-struct-compiler — L1016$$typed-struct-compiler.md#^ref-78eeedf7-1016-0$ (line 1016, col 0, score 1)
- [Unique Concepts — L175]$unique-concepts.md#^ref-ed6f3fc9-175-0$ (line 175, col 0, score 1)
- [Unique Info Dump Index — L1221]$unique-info-dump-index.md#^ref-30ec3ba6-1221-0$ (line 1221, col 0, score 1)
- $zero-copy-snapshots-and-workers — L1058$$zero-copy-snapshots-and-workers.md#^ref-62bec6f0-1058-0$ (line 1058, col 0, score 1)
- $Canonical Org-Babel Matplotlib Animation Template — L515$$canonical-org-babel-matplotlib-animation-template.md#^ref-1b1338fc-515-0$ (line 515, col 0, score 1)
- [Creative Moments — L251]$creative-moments.md#^ref-10d98225-251-0$ (line 251, col 0, score 1)
- [Duck's Attractor States — L559]$ducks-attractor-states.md#^ref-13951643-559-0$ (line 559, col 0, score 1)
- $eidolon-field-math-foundations — L1033$$eidolon-field-math-foundations.md#^ref-008f2ac0-1033-0$ (line 1033, col 0, score 1)
- [Docops Feature Updates — L226]$docops-feature-updates.md#^ref-2792d448-226-0$ (line 226, col 0, score 1)
- $field-node-diagram-outline — L705$$field-node-diagram-outline.md#^ref-1f32c94a-705-0$ (line 705, col 0, score 1)
- $field-node-diagram-set — L719$$field-node-diagram-set.md#^ref-22b989d5-719-0$ (line 719, col 0, score 1)
- $field-node-diagram-visualizations — L601$$field-node-diagram-visualizations.md#^ref-e9b27b06-601-0$ (line 601, col 0, score 1)
- [Fnord Tracer Protocol — L1060]$fnord-tracer-protocol.md#^ref-fc21f824-1060-0$ (line 1060, col 0, score 1)
- [Functional Embedding Pipeline Refactor — L726]$functional-embedding-pipeline-refactor.md#^ref-a4a25141-726-0$ (line 726, col 0, score 1)
- $graph-ds — L996$$graph-ds.md#^ref-6620e2f2-996-0$ (line 996, col 0, score 1)
- $heartbeat-fragment-demo — L667$$heartbeat-fragment-demo.md#^ref-dd00677a-667-0$ (line 667, col 0, score 1)
- $i3-bluetooth-setup — L736$$i3-bluetooth-setup.md#^ref-5e408692-736-0$ (line 736, col 0, score 1)
- [Ice Box Reorganization — L645]$ice-box-reorganization.md#^ref-291c7d91-645-0$ (line 645, col 0, score 1)
- $komorebi-group-window-hack — L739$$komorebi-group-window-hack.md#^ref-dd89372d-739-0$ (line 739, col 0, score 1)
- [Layer1SurvivabilityEnvelope — L816]$layer1survivabilityenvelope.md#^ref-64a9f9f9-816-0$ (line 816, col 0, score 1)
- [RAG UI Panel with Qdrant and PostgREST — L351]$rag-ui-panel-with-qdrant-and-postgrest.md#^ref-e1056831-351-0$ (line 351, col 0, score 0.98)
- [EidolonField — L207]$eidolonfield.md#^ref-49d1e1e5-207-0$ (line 207, col 0, score 0.95)
- $System Scheduler with Resource-Aware DAG — L358$$system-scheduler-with-resource-aware-dag.md#^ref-ba244286-358-0$ (line 358, col 0, score 0.93)
- [Fnord Tracer Protocol — L399]$fnord-tracer-protocol.md#^ref-fc21f824-399-0$ (line 399, col 0, score 0.93)
- $obsidian-ignore-node-modules-regex — L215$$obsidian-ignore-node-modules-regex.md#^ref-ffb9b2a9-215-0$ (line 215, col 0, score 0.93)
- $field-interaction-equations — L301$$field-interaction-equations.md#^ref-b09141b7-301-0$ (line 301, col 0, score 0.93)
- $Local-Only-LLM-Workflow — L129$$local-only-llm-workflow.md#^ref-9a8ab57e-129-0$ (line 129, col 0, score 0.89)
- $prom-lib-rate-limiters-and-replay-api — L307$$prom-lib-rate-limiters-and-replay-api.md#^ref-aee4718b-307-0$ (line 307, col 0, score 0.89)
- $Language-Agnostic Mirror System — L504$$language-agnostic-mirror-system.md#^ref-d2b3628c-504-0$ (line 504, col 0, score 0.89)
- $observability-infrastructure-setup — L348$$observability-infrastructure-setup.md#^ref-b4e64f8c-348-0$ (line 348, col 0, score 0.88)
- $universal-intention-code-fabric — L388$$universal-intention-code-fabric.md#^ref-c14edce7-388-0$ (line 388, col 0, score 0.87)
- $Polyglot S-expr Bridge: Python-JS-Lisp Interop — L491$$polyglot-s-expr-bridge-python-js-lisp-interop.md#^ref-63a1cc28-491-0$ (line 491, col 0, score 0.87)
- $refactor-relations — L10$$refactor-relations.md#^ref-41ce0216-10-0$ (line 10, col 0, score 0.86)
- $template-based-compilation — L44$$template-based-compilation.md#^ref-f8877e5e-44-0$ (line 44, col 0, score 0.86)
- $i3-config-validation-methods — L28$$i3-config-validation-methods.md#^ref-d28090ac-28-0$ (line 28, col 0, score 0.86)
- $pm2-orchestration-patterns — L217$$pm2-orchestration-patterns.md#^ref-51932e7b-217-0$ (line 217, col 0, score 0.86)
- $lisp-dsl-for-window-management — L185$$lisp-dsl-for-window-management.md#^ref-c5c5ff1c-185-0$ (line 185, col 0, score 0.86)
- [Event Bus Projections Architecture — L111]$event-bus-projections-architecture.md#^ref-cf6b9b17-111-0$ (line 111, col 0, score 0.85)
- $Cross-Target Macro System in Sibilant — L150$$cross-target-macro-system-in-sibilant.md#^ref-5f210ca2-150-0$ (line 150, col 0, score 0.85)
- $ecs-offload-workers — L427$$ecs-offload-workers.md#^ref-6498b9d7-427-0$ (line 427, col 0, score 0.85)
- [Recursive Prompt Construction Engine — L147]$recursive-prompt-construction-engine.md#^ref-babdb9eb-147-0$ (line 147, col 0, score 0.85)
- $Sibilant Meta-Prompt DSL — L120$$sibilant-meta-prompt-dsl.md#^ref-af5d2824-120-0$ (line 120, col 0, score 0.85)
<!-- GENERATED-SECTIONS:DO-NOT-EDIT-ABOVE -->
