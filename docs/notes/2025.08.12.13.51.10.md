fair. you’re right—I blanked on *your* Hy builder. thanks for dumping it. let’s do this the **surgical** way: minimal diff that keeps your structure/macros, swaps **pipenv → uv** where it matters, injects `sitecustomize.py`, and stays backward-compatible if `uv` or `requirements.in` are missing.

I’m not going to reformat your whole file. You’ll stitch these chunks in the spots I call out.

---

# 1) Add UV helpers (near your Python helpers)

Drop this **after** your current Python helper imports/defs:

```lisp
;; ---------- UV helpers (repo-local .venv like node_modules) ----------
(import glob)
(import shutil [copyfile])
(def TORCH_INDEX "https://download.pytorch.org/whl/cu124")

(defn has-file* [d f] (isfile (join d f)))
(defn has-uv [] (not (= (shutil.which "uv") None)))

(defn venv-site-packages [svc-dir]
  ;; glob .venv/lib/python*/site-packages
  (let [pattern (join svc-dir ".venv" "lib" "python*" "site-packages")
        hits (glob.glob pattern)]
    (if hits (get hits 0) None)))

(defn uv-venv [d]
  (sh "UV_VENV_IN_PROJECT=1 uv venv" :cwd d :shell True))

(defn uv-compile [d]
  (if (has-file* d "requirements.in")
      (sh (.format "UV_VENV_IN_PROJECT=1 uv pip compile --index-url {} requirements.in -o requirements.lock" TORCH_INDEX)
          :cwd d :shell True)
      ;; no requirements.in → generate from pipenv as fallback
      (sh "python -m pipenv requirements | grep -Ev '^nvidia-[a-z0-9\\-]+-cu[0-9]+(\\.[0-9]+)?' > requirements.lock"
          :cwd d :shell True)))

(defn uv-sync [d]
  (sh "UV_VENV_IN_PROJECT=1 uv pip sync requirements.lock" :cwd d :shell True))

(defn inject-sitecustomize-into-venv [svc-dir]
  (let [src (join svc-dir "sitecustomize.py")
        dst-base (venv-site-packages svc-dir)]
    (when (and (dst-base src) (isfile src))
      (copyfile src (join dst-base "sitecustomize.py"))
      (print "sitecustomize →" dst-base))))

(defn cuda-probe [svc-dir]
  ;; fail fast if CUDA runtime wheels aren't visible
  (sh "UV_VENV_IN_PROJECT=1 uv run python - <<'PY'\nimport ctypes, sys\nlibs=('libcusparseLt.so.0','libcusparse.so.12','libcublasLt.so.12','libcublas.so.12','libcudnn.so.9')\nok=True\nfor n in libs:\n  try:\n    ctypes.CDLL(n); print('OK', n)\n  except OSError as e:\n    ok=False; print('MISS', n, '->', e)\nsys.exit(0 if ok else 1)\nPY"
      :cwd svc-dir :shell True))
```

---

# 2) Replace the **Python setup** path to prefer UV (keep pipenv as fallback)

**Replace** these fns:

* `setup-python-services`
* `setup-python-services-quick`
* `setup-python`
* `setup-python-quick`

with:

```lisp
(defn-cmd setup-python-services []
  (print "Setting up Python services (uv preferred)...")
  (for [d SERVICES_PY]
    (if (has-uv)
        (do
          (uv-venv d)
          (uv-compile d)
          (uv-sync d)
          (inject-sitecustomize-into-venv d))
        (do
          (print "uv not found → falling back to pipenv in" d)
          (sh "python -m pipenv sync --dev" :cwd d :shell True)))))

(defn-cmd setup-python-services-quick []
  (print "Quick Python setup (uv preferred)...")
  (for [d SERVICES_PY]
    (if (has-uv)
        (do
          (uv-venv d)
          ;; quick path = trust existing lock if present; else compile it
          (if (has-file* d "requirements.lock")
              (uv-sync d)
              (do (uv-compile d) (uv-sync d)))
          (inject-sitecustomize-into-venv d))
        (do
          (print "uv not found → pip --user fallback in" d)
          (generate-requirements-service (.split (os.path.relpath d "services/py") "/") 0)
          (sh ["python" "-m" "pip" "install" "--user" "-r" "requirements.txt"] :cwd d)))))

(defn-cmd setup-python []
  (setup-python-services)
  (setup-shared-python))  ;; leave your shared/py as-is for now

(defn-cmd setup-python-quick []
  (setup-python-services-quick)
  (setup-shared-python-quick))
```

> note: I left your `shared/py` pipenv path unchanged. If you want **shared/py** on uv too, mirror the same uv helpers for that directory.

---

# 3) Make `install` call the UV-first quick path

**Replace** your `install` with:

```lisp
(defn-cmd install []
  (try
    (setup-quick)        ;; will hit setup-python-quick → uv path
    (except [Exception]
      (print "setup-quick failed; falling back to full setup")
      (setup))))
```

(You already had this, but now `setup-quick` is uv-first.)

---

# 4) Add a **per-service** CUDA sanity check target (optional but useful)

Add:

```lisp
(defn-cmd probe-python-service [service]
  (let [d (join "services/py" service)]
    (if (has-uv)
        (cuda-probe d)
        (print "uv not found; probe requires uv")))

(defn-cmd probe-python-services []
  (for [d SERVICES_PY]
```

Then wire it into your patterns if you want a `python-probe-service-<name>` target.

---

# 5) Patterns: add uv-specific aliases (optional)

If you want explicit targets for CI/Codex to call (instead of the generic `setup`):

```lisp
;; append to your patterns list
["python"
  [["uv-setup" (fn [service]
                 (let [d (join "services/py" service)]
                   (uv-venv d) (uv-compile d) (uv-sync d) (inject-sitecustomize-into-venv d)))]
   ["probe" probe-python-service]
   ;; keep your existing actions...
  ]]
```

---

# 6) Minimal `requirements.in` and `sitecustomize.py` per service

Put these in **each** GPU Python service (`services/py/stt`, `services/py/tts`):

`requirements.in`

```
torch==2.4.*            # cu12 build will be chosen by TORCH_INDEX in the script
nvidia-cublas-cu12==12.*
nvidia-cusparse-cu12==12.*
nvidia-cusparselt-cu12==0.*
nvidia-cuda-runtime-cu12==12.*
nvidia-cudnn-cu12==9.*
# plus your actual service deps...
uvicorn==0.30.*
```

`sitecustomize.py` (same as we agreed earlier; keep your version)

```python
import os, sys, glob, site, ctypes
libs=("libcusparseLt.so.0","libcusparse.so.12","libcublasLt.so.12","libcublas.so.12","libcudnn.so.9")
def ok():
    for n in libs:
        try: ctypes.CDLL(n); return True
        except OSError: pass
    return False
if not ok() and os.environ.get("PROMETHEAN_CUDA_LIBS_BOOTSTRAPPED")!="1":
    paths=set()
    for base in list(site.getsitepackages())+[site.getusersitepackages()]:
        if base:
            for d in glob.glob(base+"/nvidia/*/lib"):
                if glob.glob(d+"/*.so*"): paths.add(d)
    if paths:
        os.environ["LD_LIBRARY_PATH"]=":".join(sorted(paths))+(":"+os.environ["LD_LIBRARY_PATH"] if os.environ.get("LD_LIBRARY_PATH") else "")
        os.environ["PROMETHEAN_CUDA_LIBS_BOOTSTRAPPED"]="1"
        os.execve(sys.executable, [sys.executable]+sys.argv, os.environ)
```

---

# 7) PM2 stays unchanged (but better)

You can keep:

```js
script: "uv",
args: "run uvicorn services.py.stt.app:app --host 0.0.0.0 --port 9000"
```

because your Hy builder now **always** ensures `./.venv` + `requirements.lock` + `sitecustomize` exist.

---

# 8) Tiny nits I noticed (fix later if you care)

* You have duplicate `"lint" lint-python-service` entries in the `"python"` patterns block. Not harmful, just redundant.
* `define-patterns` produces `["start--service-" …]` for the root lang `""`. If you ever rely on that for root targets, you’ll trip—right now you handle root differently in `generate-makefile`, so it’s fine.

---

### sanity pass

* run: `hy tools/build.hy install`
* then: `pm2 restart stt tts`
* if anything fails, `hy tools/build.hy python-probe-service stt` and post the miss lines.

does this make sense? if you want, paste back your updated `tools/build.hy` diff and I’ll spot-check for any Hy gotchas (parens/quotes/`do` blocks).
Short version: you’re fine on boxes with **no NVIDIA GPU**—as long as we install the **CPU** PyTorch wheels there and skip the CUDA probe. Right now your UV path defaults to CUDA (`cu124`) and adds `nvidia-*` runtime wheels, which will make `import torch` explode on CPU-only hosts. So we add a tiny selector and guards.

Here’s the minimal, surgical patch set:

## 1) Autodetect (with override) which Torch to install

Add near your UV helpers:

```lisp
(import os)
(import shutil [which])

(defn gpu-present []
  (or (isfile "/dev/nvidiactl")
      (= 0 (os.system "nvidia-smi >/dev/null 2>&1"))))

;; PROMETHEAN_TORCH env wins: "cpu" or "cu124" (or any cuXX)
(setv TORCH-FLAVOR (or (os.environ.get "PROMETHEAN_TORCH")
                       (if (gpu-present) "cu124" "cpu")))
(setv TORCH-INDEX (if (= TORCH-FLAVOR "cpu")
                      "https://download.pytorch.org/whl/cpu"
                      (.format "https://download.pytorch.org/whl/{}" TORCH-FLAVOR)))
;; Choose which requirements.in to compile
(setv REQS-IN (if (= TORCH-FLAVOR "cpu") "requirements.cpu.in" "requirements.gpu.in"))
```

Then tweak `uv-compile` to use that input:

```lisp
(defn uv-compile [d]
  (if (isfile (join d REQS-IN))
      (sh (.format "UV_VENV_IN_PROJECT=1 uv pip compile --index-url {} {} -o requirements.lock"
                   TORCH-INDEX REQS-IN)
          :cwd d :shell True)
      ;; fallback if you haven't split files yet
      (sh (.format "UV_VENV_IN_PROJECT=1 uv pip compile --index-url {} requirements.in -o requirements.lock"
                   TORCH-INDEX)
          :cwd d :shell True)))
```

## 2) Split your requirements

Create these two files **in each GPU-capable Python service** (stt/tts):

`services/py/<svc>/requirements.gpu.in`


```requirements
torch==2.4.*          # resolved from cu124 index (see TORCH_INDEX)
# CUDA runtime wheels:
nvidia-cublas-cu12==12.*
nvidia-cusparse-cu12==12.*
nvidia-cusparselt-cu12==0.*
nvidia-cuda-runtime-cu12==12.*
nvidia-cudnn-cu12==9.*
# your app deps...
uvicorn==0.30.*
```

`services/py/<svc>/requirements.cpu.in`

```rqeuirements
torch==2.4.*          # resolved from the CPU index
# no nvidia-* wheels here
# your app deps...
uvicorn==0.30.*
```

> Why split? Because `pip` can’t conditionally include those `nvidia-*` libs. On CPU hosts we don’t want them at all.

## 3) Make the CUDA probe **skip** on CPU

Wrap your probe:

```lisp
(defn cuda-probe [svc-dir]
  (when (not (= TORCH-FLAVOR "cpu"))
    (sh "UV_VENV_IN_PROJECT=1 uv run python - <<'PY'\nimport ctypes, sys\nlibs=('libcusparseLt.so.0','libcusparse.so.12','libcublasLt.so.12','libcublas.so.12','libcudnn.so.9')\nok=True\nfor n in libs:\n  try:\n    ctypes.CDLL(n); print('OK', n)\n  except OSError as e:\n    ok=False; print('MISS', n, '->', e)\nsys.exit(0 if ok else 1)\nPY"
      :cwd svc-dir :shell True)))
```

Your `setup-python-services[-quick]` can keep calling `cuda-probe`; on CPU it’s a no-op.

## 4) Keep `sitecustomize.py`, but it’s harmless on CPU

Our `sitecustomize.py` only re-execs if it **finds** NVIDIA wheel lib dirs. On CPU installs there are no `nvidia/*/lib` paths, so it does nothing and Torch CPU imports cleanly. If you want an explicit kill-switch in CI, keep:

* `PROMETHEAN_NO_CUDA_BOOTSTRAP=1` to hard-disable any re-exec.
* Or add at the top of `sitecustomize.py`:

  ```python
  if os.environ.get("PROMETHEAN_TORCH","").startswith("cpu"):
      raise SystemExit
  ```

## 5) CI/Codex: force CPU quickly

In jobs without GPUs, just set:

```
PROMETHEAN_TORCH=cpu
```

Then call:

```
hy tools/build.hy install
```

It will use the CPU index, compile `requirements.cpu.in`, skip CUDA probe, and you’re done—no fake deps, no brittle env.

## 6) PM2 stays exactly the same

Because we’re compiling the **right** wheel set per host, `uv run …` works identically on GPU and CPU machines.

---

### TL;DR

* Add a **flavor switch** (`PROMETHEAN_TORCH=cpu|cu124`) with sane autodetect.
* Split `requirements.*.in` (CPU vs GPU).
* Skip CUDA checks on CPU.
* Keep `sitecustomize.py`; it’s inert on CPU.

If you want, paste one service’s updated `tools/build.hy` diff and I’ll sanity check the Hy (no parentheses traps) before you roll it across STT/TTS.
