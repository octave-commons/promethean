yeah, venvs feel janky next to `node_modules`. you can make Python behave a lot more like Node if you standardize your tooling and put the env **inside the repo** and **never “activate”** anything.

here’s the setup I recommend (it’s the least-pain path I’ve found for GPU Python):

## make Python behave like node

**Tooling: `uv` (fast pip replacement) + in-project `.venv` + “run without activating.”**

1. install uv once:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
# make sure ~/.local/bin is on PATH
```

2. in each service (`services/py/stt`, `services/py/tts`), create a **project-local venv**:

```bash
cd services/py/stt
UV_VENV_IN_PROJECT=1 uv venv  # creates ./.venv (like node_modules)
```

3. pin deps and lock:

* put your deps in `requirements.in` (not compiled).
* generate a lock (deterministic):

```bash
uv pip compile requirements.in -o requirements.lock
uv pip sync requirements.lock
```

(no `source`/activation; it installs into `./.venv`)

4. run anything with the venv **without activating**:

```bash
uv run python -c "import torch; print(torch.__version__)"
uv run uvicorn services.py.stt.app:app --host 0.0.0.0 --port 9000
```

> `uv run` is your `npx`—it picks the right .venv automatically. CI/CD and PM2 don’t need to know about venvs anymore.

## fix the CUDA-runtime drama once, inside the project

Python can’t “vendor” `.so`’s by default, but NVIDIA publishes the CUDA runtime as pip wheels. Install them and bootstrap the loader **inside the venv** so services don’t care about `LD_LIBRARY_PATH`.

1. add these to `requirements.in` (versions can be pinned to your torch series):

```
torch==2.4.*        # choose cu12 build via index below
# torchvision/torchaudio if needed

# CUDA runtime wheels (match your torch cu12)
nvidia-cublas-cu12==12.*
nvidia-cusparse-cu12==12.*
nvidia-cusparselt-cu12==0.*
nvidia-cuda-runtime-cu12==12.*
nvidia-cudnn-cu12==9.*
```

compile & sync again with uv (as above). If you need the CUDA index for torch:

```
uv pip compile --index-url https://download.pytorch.org/whl/cu124 requirements.in -o requirements.lock
```

2. drop a **sitecustomize** into the venv so the loader path is fixed automatically (no PM2 hacks):

`services/py/stt/sitecustomize.py` (uv will copy into `.venv` at install; or place into `.venv/.../site-packages` after bootstrap):

```python
# sitecustomize.py – auto-expose wheel CUDA libs
import os, sys, glob, site, ctypes

def ok():
    for n in ("libcusparseLt.so.0","libcusparse.so.12",
              "libcublasLt.so.12","libcublas.so.12","libcudnn.so.9"):
        try: ctypes.CDLL(n); return True
        except OSError: pass
    return False

if not ok():
    libdirs=set()
    for base in list(site.getsitepackages())+[site.getusersitepackages()]:
        if not base: continue
        for d in glob.glob(base + "/nvidia/*/lib"):
            if glob.glob(d+"/*.so*"): libdirs.add(d)
    if libdirs and os.environ.get("PROMETHEAN_CUDA_LIBS_BOOTSTRAPPED")!="1":
        os.environ["LD_LIBRARY_PATH"]=":".join(sorted(libdirs)) + \
            ((":"+os.environ["LD_LIBRARY_PATH"]) if os.environ.get("LD_LIBRARY_PATH") else "")
        os.environ["PROMETHEAN_CUDA_LIBS_BOOTSTRAPPED"]="1"
        os.execve(sys.executable, [sys.executable]+sys.argv, os.environ)
```

* this makes torch imports work under **PM2/systemd** with zero service-env tweaks.
* CI can opt out: `PROMETHEAN_NO_CUDA_BOOTSTRAP=1` (add an early return if you want).

## PM2 feels normal again

In `ecosystem.config.js`, ditch activation and wrappers—call `uv run`:

```js
module.exports = {
  apps: [
    {
      name: "stt",
      cwd: "services/py/stt",
      script: "uv",
      args: "run uvicorn services.py.stt.app:app --host 0.0.0.0 --port 9000",
      env: {
        // optional: disable CUDA bootstrap in CI
        // "PROMETHEAN_NO_CUDA_BOOTSTRAP": "1"
      }
    },
    {
      name: "tts",
      cwd: "services/py/tts",
      script: "uv",
      args: "run uvicorn services.py.tts.app:app --host 0.0.0.0 --port 9001"
    }
  ]
}
```

No `source`, no `VIRTUAL_ENV` gymnastics, no global PATH edits. It just works, like `npx`.

## one-button bootstrap for new machines

Put this at repo root as `scripts/bootstrap-python.sh` and call it from CI & docs:

```bash
#!/usr/bin/env bash
set -euo pipefail
cd "$(dirname "$0")/.."

ensure() {
  local dir="$1"
  pushd "$dir" >/dev/null
  export UV_VENV_IN_PROJECT=1
  uv venv
  # choose the right torch index for your CUDA series:
  uv pip compile --index-url https://download.pytorch.org/whl/cu124 requirements.in -o requirements.lock
  uv pip sync requirements.lock
  # drop sitecustomize if not present
  py_site="$(uv run python - <<'PY'
import site, json
print(site.getsitepackages()[-1])
PY
)"
  if [ ! -f "$py_site/sitecustomize.py" ] && [ -f "sitecustomize.py" ]; then
    cp sitecustomize.py "$py_site/sitecustomize.py"
  fi
  # sanity probe
  uv run python - <<'PY'
import ctypes
for n in ("libcusparseLt.so.0","libcusparse.so.12","libcublasLt.so.12","libcublas.so.12","libcudnn.so.9"):
    try: ctypes.CDLL(n); print("OK", n)
    except OSError as e: print("MISS", n, "->", e)
PY
  popd >/dev/null
}

ensure services/py/stt
ensure services/py/tts
```

Run: `bash scripts/bootstrap-python.sh`

## why this solves your pain

* **in-repo .venv** ≈ `node_modules` → no global state.
* **`uv run`** ≈ `npx` → no activation; PM2/CI just call commands.
* **CUDA wheels + sitecustomize** → torch finds GPU libs without fragile env vars.
* **locked requirements** → upgrades are explicit and diffable.

If you want, I’ll generate minimal `requirements.in` for `stt`/`tts` from your current `pip freeze` and add the bootstrap + PM2 changes. Then you get “npm-like” determinism without switching stacks mid-project.
