Perfect ‚úÖ ‚Äî so instead of a **single log entry mirrored into both**, we‚Äôll treat **MongoDB and ChromaDB as independent sinks**:

* **MongoDB** ‚Üí structured/audit logging, query by fields.
* **ChromaDB** ‚Üí semantic embedding logging, query by meaning.
* You can query **either system separately**, and they don‚Äôt depend on each other.

---

# üìù Agent Task: Add Dual Independent Logging to SmartGPT Bridge

## üìå Context

The SmartGPT Bridge has working APIs and a valid OpenAPI spec.
We need **persistent logging** across two independent backends:

* **MongoDB** ‚Üí structured filters (`path`, `method`, `statusCode`, `timestamps`).
* **ChromaDB** ‚Üí semantic logs (embed full JSON log into a `bridge_logs` collection).

---

## üéØ Goals

* Log **all requests/responses** into **MongoDB**.
* Log the **same entries (independently)** into **ChromaDB**.
* Provide endpoints to query each log store separately:

  * `/logs/query` ‚Üí MongoDB filter queries.
  * `/logs/search` ‚Üí ChromaDB semantic search.

---

## ‚úÖ Requirements

### 1. MongoDB Structured Logging

* Define `Log` schema:

```ts
{
  method: String,       // HTTP method
  path: String,         // Request path
  statusCode: Number,   // Response status
  request: Object,      // { query, params, body }
  response: Object,     // JSON response (if any)
  error: String,        // Error message (if any)
  createdAt: Date       // Timestamp
}
```

* Indexes:

  * `{ path: 1, createdAt: -1 }`
  * `{ statusCode: 1 }`
* Store in `bridge_logs` collection.

---

### 2. ChromaDB Semantic Logging

* Reuse `getChroma()` from `src/indexer.js`.
* Use **separate collection** `bridge_logs`.
* Insert logs as **documents** with metadata for filtering:

```ts
await collection.add({
  ids: [uuid or requestId],
  documents: [JSON.stringify(entry)],   // full log as JSON
  metadatas: {
    path: entry.path,
    method: entry.method,
    status: entry.statusCode,
    createdAt: entry.createdAt.toISOString()
  },
});
```

* This means Chroma logs can be queried **without touching Mongo**.

---

### 3. Independent Log Endpoints

#### `/logs/query` (Mongo)

* **operationId:** `queryLogs`
* **Body filters:**

  * `path` (string)
  * `method` (string)
  * `statusCode` (int)
  * `since` / `until` (ISO date)
* **Returns:** structured Mongo query results (limit 100).

#### `/logs/search` (Chroma)

* **operationId:** `semanticLogSearch`
* **Body filters:**

  * `q` (string, required) ‚Üí semantic search query
  * `n` (int, default 10) ‚Üí max results
* **Returns:** Chroma semantic matches with metadata.

---

## üì¶ Deliverables

* ‚úÖ MongoDB `Log` model with indexes
* ‚úÖ Middleware hook writing to both Mongo + Chroma **independently**
* ‚úÖ `/logs/query` ‚Üí filterable Mongo logs
* ‚úÖ `/logs/search` ‚Üí semantic Chroma logs

---

‚ö°Ô∏è With this setup:

* Mongo queries = deterministic & fast filters.
* Chroma queries = fuzzy/semantic retrieval.
* Neither backend depends on the other (you can scale or debug them independently).

---

üëâ Do you want me to also **add correlation IDs** (e.g. a `requestId`) so that the same log entry in Mongo and Chroma can still be cross-referenced if needed?

Got it ‚úÖ ‚Äî you want **metadata-powered queries in Chroma**, not just free-text semantic search. That means every log entry we push into the Chroma collection needs:

1. A **rich metadata object** (not just `path/method/status`).
2. The `/logs/search` endpoint should support **filters** (e.g. ‚Äúsemantic search for errors, but only where `statusCode=500`‚Äù).

Chroma already supports this:

```ts
collection.query({
  queryTexts: ["agent crash"],
  nResults: 10,
  where: { statusCode: 500, path: "/agent/start" }
});
```

---

# üìù Refined Task: Chroma Metadata-Aware Logging

## üìå Context

The SmartGPT Bridge logs all requests into Mongo (structured) and Chroma (semantic).
We want **metadata-aware semantic search** in Chroma.

---

## üéØ Goals

* Add **structured metadata** when inserting logs into Chroma.
* Enhance `/logs/search` endpoint to accept `filters` that map directly to Chroma‚Äôs `where` clause.

---

## ‚úÖ Requirements

### 1. Metadata Schema for Chroma

When writing a log entry to Chroma, store metadata like:

```ts
await collection.add({
  ids: [mongoDoc._id.toString()],
  documents: [JSON.stringify(entry)],   // full log
  metadatas: {
    path: entry.path,
    method: entry.method,
    statusCode: entry.statusCode,
    hasError: !!entry.error,
    createdAt: entry.createdAt.toISOString()
  },
});
```

üîë Notes:

* Keep values primitive (`string | number | boolean`).
* `createdAt` should be stored as string ISO timestamp for filtering ranges (you can also bucket to `YYYY-MM-DD`).

---

### 2. Extended `/logs/search` Endpoint

```ts
app.post('/logs/search', {
  schema: {
    summary: 'Semantic + metadata-aware search logs',
    operationId: 'semanticLogSearch',
    tags: ['Logs'],
    body: {
      type: 'object',
      required: ['q'],
      properties: {
        q: { type: 'string' },
        n: { type: 'integer', default: 10 },
        where: { type: 'object' }   // optional filter
      },
    },
  },
  handler: async (req) => {
    const { q, n, where } = req.body;
    const collection = await getLogCollection();
    return collection.query({
      queryTexts: [q],
      nResults: n || 10,
      where: where || {},   // e.g. { statusCode: 500, method: "POST" }
    });
  },
});
```

---

### 3. Example Queries

* Find **500 errors** semantically:

```json
{ "q": "error", "where": { "statusCode": 500 } }
```

* Find **agent starts that failed**:

```json
{ "q": "failed to start", "where": { "path": "/agent/start" } }
```

* Find logs **since today** (if you store `createdAt` bucketed):

```json
{ "q": "any", "where": { "createdAt": "2025-08-22" } }
```

---

## üì¶ Deliverables

* ‚úÖ Logs inserted into Chroma with metadata fields (`path`, `method`, `statusCode`, `hasError`, `createdAt`).
* ‚úÖ `/logs/search` endpoint accepts `where` filters ‚Üí maps to Chroma metadata filtering.
* ‚úÖ Can combine semantic + structured queries.

---

üëâ Do you want me to also **design a recommended metadata schema** (like `service`, `latencyMs`, `userId` hooks) so this logging layer can evolve into a full observability system later?
Excellent üöÄ ‚Äî if we‚Äôre treating Chroma as a **semantic log store**, then designing a **clean metadata schema** up front is critical. That way you can query logs both by meaning *and* by structured fields, like a hybrid between Mongo-style filtering and vector search.

Here‚Äôs what I recommend:

---

# üìù Recommended Metadata Schema for Chroma Log Collection

Each log entry stored in Chroma should include:

## üîë Core Fields (always present)

| Field        | Type    | Purpose                                                                |
| ------------ | ------- | ---------------------------------------------------------------------- |
| `path`       | string  | The HTTP route, e.g. `/agent/start`.                                   |
| `method`     | string  | The HTTP method, e.g. `POST`.                                          |
| `statusCode` | number  | Response status, e.g. `200`, `500`.                                    |
| `hasError`   | boolean | Quick filter for success vs failure.                                   |
| `createdAt`  | string  | ISO timestamp, for time filters.                                       |
| `latencyMs`  | number  | Request duration, for performance tracking.                            |
| `service`    | string  | `"smartgpt-bridge"` (or microservice name if multiple feed logs here). |

---

## üîß Optional Context Fields (when available)

| Field         | Type   | Purpose                                           |
| ------------- | ------ | ------------------------------------------------- |
| `userId`      | string | Caller identity if auth is present.               |
| `agentId`     | string | If request involves a specific agent.             |
| `requestId`   | string | Correlation ID across systems.                    |
| `operationId` | string | OpenAPI operation name (e.g. `semanticSearch`).   |
| `tags`        | array  | Arbitrary labels (e.g. `["indexer","critical"]`). |

---

## üì¶ Example Chroma Insert

```ts
await collection.add({
  ids: [requestId],
  documents: [JSON.stringify(entry)],   // full structured log
  metadatas: {
    path: entry.path,
    method: entry.method,
    statusCode: entry.statusCode,
    hasError: !!entry.error,
    createdAt: entry.createdAt.toISOString(),
    latencyMs: entry.latencyMs || 0,
    service: "smartgpt-bridge",
    requestId: entry.requestId,
    operationId: entry.operationId,
    userId: entry.userId || null,
    agentId: entry.agentId || null,
    tags: entry.tags || [],
  },
});
```

---

## üéØ Query Examples

* **Find all failed indexer calls**:

```json
{ "q": "failure", "where": { "path": "/indexer/status", "hasError": true } }
```

* **Find logs for a specific user**:

```json
{ "q": "any", "where": { "userId": "user-123" } }
```

* **Find agent crashes**:

```json
{ "q": "agent crashed", "where": { "agentId": "a-456" } }
```

* **Filter by latency** (e.g. slow calls):

```json
{ "q": "any", "where": { "latencyMs": { "$gte": 1000 } } }
```

---

## üöÄ Benefits

* Mongo = structured query, audit logs.
* Chroma = hybrid semantic + structured filters.
* Together = full observability stack, with **deterministic queries** + **meaning-based retrieval**.

---

üëâ Do you want me to **write the logging middleware skeleton** with this metadata schema baked in, so you can drop it directly into the SmartGPT Bridge?
Alright üöÄ ‚Äî here‚Äôs a **logging middleware skeleton** for the SmartGPT Bridge, with **dual sinks (Mongo + Chroma)** and the **rich metadata schema** baked in.

You can drop this into `src/plugins/logger.ts` (or similar) and wire it up in your app bootstrap.

---

# üì¶ `src/plugins/logger.ts`

```ts
import { FastifyPluginAsync } from "fastify";
import { Log } from "../models/Log.js";   // Mongo model
import { getChroma } from "../indexer.js"; // existing singleton client
import { v4 as uuidv4 } from "uuid";

// Get or create Chroma collection
async function getLogCollection() {
  return await getChroma().getOrCreateCollection({
    name: "bridge_logs",
    metadata: { type: "logs" },
  });
}

const mongoChromaLogger: FastifyPluginAsync = async (app) => {
  const collection = await getLogCollection();

  app.addHook("onRequest", async (req) => {
    // Attach correlation ID early
    (req as any).requestId = uuidv4();
    (req as any).startTime = Date.now();
  });

  app.addHook("onResponse", async (req, reply) => {
    const latencyMs = Date.now() - (req as any).startTime;

    const entry = {
      requestId: (req as any).requestId,
      method: req.method,
      path: req.url,
      statusCode: reply.statusCode,
      request: {
        query: req.query,
        params: req.params,
        body: req.body,
      },
      response: reply.payload ? safeJson(reply.payload) : undefined,
      createdAt: new Date(),
      latencyMs,
      service: "smartgpt-bridge",
      operationId: (reply.context?.schema as any)?.operationId,
    };

    try {
      // ‚úÖ Mongo insert
      const doc = await Log.create(entry);

      // ‚úÖ Chroma insert
      await collection.add({
        ids: [entry.requestId],
        documents: [JSON.stringify(entry)],
        metadatas: {
          requestId: entry.requestId,
          path: entry.path,
          method: entry.method,
          statusCode: entry.statusCode,
          hasError: false,
          createdAt: entry.createdAt.toISOString(),
          latencyMs: entry.latencyMs,
          service: entry.service,
          operationId: entry.operationId,
        },
      });
    } catch (err) {
      app.log.error({ err }, "Failed to write success log");
    }
  });

  app.addHook("onError", async (req, reply, error) => {
    const latencyMs = Date.now() - (req as any).startTime;

    const entry = {
      requestId: (req as any).requestId || uuidv4(),
      method: req.method,
      path: req.url,
      statusCode: reply.statusCode,
      request: {
        query: req.query,
        params: req.params,
        body: req.body,
      },
      error: error.message,
      createdAt: new Date(),
      latencyMs,
      service: "smartgpt-bridge",
      operationId: (reply.context?.schema as any)?.operationId,
    };

    try {
      // ‚úÖ Mongo insert
      const doc = await Log.create(entry);

      // ‚úÖ Chroma insert
      await collection.add({
        ids: [entry.requestId],
        documents: [JSON.stringify(entry)],
        metadatas: {
          requestId: entry.requestId,
          path: entry.path,
          method: entry.method,
          statusCode: entry.statusCode,
          hasError: true,
          createdAt: entry.createdAt.toISOString(),
          latencyMs: entry.latencyMs,
          service: entry.service,
          operationId: entry.operationId,
        },
      });
    } catch (err) {
      app.log.error({ err }, "Failed to write error log");
    }
  });
};

export default mongoChromaLogger;

// --- Helpers ---
function safeJson(payload: any) {
  try {
    return typeof payload === "string" ? JSON.parse(payload) : payload;
  } catch {
    return { raw: String(payload) };
  }
}
```

---

# üì¶ `src/models/Log.ts` (Mongo Model)

```ts
import { Schema, model } from "mongoose";

const LogSchema = new Schema({
  requestId: { type: String, required: true },
  method: String,
  path: String,
  statusCode: Number,
  request: Object,
  response: Object,
  error: String,
  createdAt: { type: Date, default: Date.now },
  latencyMs: Number,
  service: String,
  operationId: String,
});

LogSchema.index({ path: 1, createdAt: -1 });
LogSchema.index({ statusCode: 1 });
LogSchema.index({ operationId: 1 });

export const Log = model("Log", LogSchema, "bridge_logs");
```

---

# üì¶ Endpoints (Mongo vs Chroma)

```ts
// Mongo structured query
app.post('/logs/query', {
  schema: { operationId: 'queryLogs', tags: ['Logs'] },
  handler: async (req) => {
    const { path, method, statusCode, since, until } = req.body as any;
    const query: any = {};
    if (path) query.path = path;
    if (method) query.method = method.toUpperCase();
    if (statusCode) query.statusCode = statusCode;
    if (since || until) query.createdAt = {};
    if (since) query.createdAt.$gte = new Date(since);
    if (until) query.createdAt.$lte = new Date(until);
    return Log.find(query).sort({ createdAt: -1 }).limit(100);
  },
});

// Chroma semantic + metadata search
app.post('/logs/search', {
  schema: { operationId: 'semanticLogSearch', tags: ['Logs'] },
  handler: async (req) => {
    const { q, n, where } = req.body as any;
    const collection = await getLogCollection();
    return collection.query({
      queryTexts: [q],
      nResults: n || 10,
      where: where || {},
    });
  },
});
```

---

‚ú® With this setup:

* Mongo logs ‚Üí fast filters, structured auditing.
* Chroma logs ‚Üí semantic + metadata hybrid search.
* Each log entry has a **correlation ID** (`requestId`) so you can cross-reference Mongo + Chroma if needed.

---

üëâ Do you want me to also add a **retention/cleanup strategy** (e.g. TTL index in Mongo, capped Chroma collection) so logs don‚Äôt grow unbounded?
Perfect ‚úÖ ‚Äî retention is critical or else you‚Äôll end up with a **runaway log store** that eats disk.
Here‚Äôs how I‚Äôd design **retention & cleanup** for both **MongoDB** and **ChromaDB**.

---

# üìù Retention & Cleanup Strategy for Bridge Logs

## 1. MongoDB Retention (TTL Index)

MongoDB supports **TTL indexes** out of the box.
We can expire logs automatically after a given period (e.g. 30 days).

```ts
// src/models/Log.ts
const LogSchema = new Schema({
  requestId: { type: String, required: true },
  method: String,
  path: String,
  statusCode: Number,
  request: Object,
  response: Object,
  error: String,
  createdAt: { type: Date, default: Date.now, index: true },
  latencyMs: Number,
  service: String,
  operationId: String,
});

// Expire documents 30 days after `createdAt`
LogSchema.index({ createdAt: 1 }, { expireAfterSeconds: 60 * 60 * 24 * 30 });
```

üîë Benefits:

* Mongo automatically deletes old logs.
* No manual cleanup needed.
* Expiry window is tunable by changing `expireAfterSeconds`.

---

## 2. ChromaDB Retention

Chroma doesn‚Äôt have native TTL, but we can implement retention in **two ways**:

### A. Time-based Cleanup Job

Run a scheduled job (e.g. once/day) to prune old entries:

```ts
import { getLogCollection } from "../logging/chromaLog.js";

async function cleanupChromaLogs(days = 30) {
  const cutoff = new Date(Date.now() - days * 24 * 60 * 60 * 1000);
  const collection = await getLogCollection();

  // Query all log IDs older than cutoff
  const results = await collection.get({ 
    where: { createdAt: { "$lt": cutoff.toISOString() } }, 
    include: ["ids"] 
  });

  if (results.ids?.length) {
    await collection.delete({ ids: results.ids });
    console.log(`Deleted ${results.ids.length} Chroma logs older than ${days}d`);
  }
}
```

You can wire this into a cron, Kubernetes CronJob, or just run daily inside the service.

---

### B. Capped Collection Strategy

If storage is tight, you could implement a **rolling cap** (e.g. max 100k logs).
Fetch all IDs sorted by `createdAt`, and delete the oldest when above threshold.

---

## 3. Configuration

Make retention configurable via **env vars**:

```env
LOG_TTL_DAYS=30
LOG_MAX_CHROMA=100000
```

---

## 4. Future Enhancements

* **Archival**: Before deletion, dump expired logs to S3 for cold storage.
* **Per-namespace TTL**: Different retention for `/agent/` vs `/exec/` routes.
* **Sampling**: For very high-traffic routes, sample logs (e.g. only keep 1 in 10).

---

# üì¶ Deliverables

* ‚úÖ TTL index in Mongo (default 30 days).
* ‚úÖ Chroma cleanup job (time-based, default 30 days).
* ‚úÖ Env config for TTL duration + max Chroma docs.
* ‚úÖ Optional archival hook (future-proof).

---

üëâ Do you want me to also **extend the log schema with a `namespace` field** (like `"indexer"`, `"agent"`, `"exec"`) so retention rules can be tuned per subsystem?
Got it ‚úÖ ‚Äî here‚Äôs the **task spec** for the logging **retention & cleanup feature**.

---

# üìù Agent Task: Add Retention & Cleanup for SmartGPT Bridge Logs

## üìå Context

The SmartGPT Bridge now logs all requests into:

* **MongoDB** (structured/audit logs).
* **ChromaDB** (semantic search logs).

Both backends will grow indefinitely unless retention rules are applied.
We need automated cleanup to keep log storage bounded.

---

## üéØ Goals

* Expire old logs in **MongoDB** automatically via TTL index.
* Implement a **retention policy for ChromaDB** (time-based cleanup + optional capped size).
* Make retention **configurable via env vars**.

---

## ‚úÖ Requirements

### 1. MongoDB Retention (TTL Index)

* Add TTL index on `createdAt` field in `LogSchema`.
* Default expiry: **30 days**.
* Value configurable via `LOG_TTL_DAYS` env var.
* Example:

  ```ts
  LogSchema.index(
    { createdAt: 1 },
    { expireAfterSeconds: process.env.LOG_TTL_DAYS ? Number(process.env.LOG_TTL_DAYS) * 86400 : 60 * 60 * 24 * 30 }
  );
  ```

---

### 2. ChromaDB Retention

* Add cleanup utility in `src/logging/chromaCleanup.ts`.
* Two strategies:

#### (A) Time-based Cleanup

* Default: expire logs older than `LOG_TTL_DAYS`.
* Implementation: query Chroma `where: { createdAt: { "$lt": cutoffISO } }` and delete matching IDs.

#### (B) Capped Collection Cleanup

* Default: max 100k logs (`LOG_MAX_CHROMA`).
* If collection exceeds threshold, prune oldest entries.

---

### 3. Scheduled Execution

* Wire cleanup job to run periodically (e.g. once/day).
* Options:

  * Built-in setInterval inside service.
  * External cronjob (if deployed in k8s).

---

### 4. Configuration

Add env vars:

```env
LOG_TTL_DAYS=30         # days until expiry
LOG_MAX_CHROMA=100000   # max docs in chroma before pruning
```

---

### 5. Future-Proofing (Optional)

* Add `namespace` field (`indexer`, `agent`, `exec`) to logs.
* Allow per-namespace TTL and retention rules.
* Consider archival to S3 before deletion.

---

## üì¶ Deliverables

* ‚úÖ Mongo TTL index on `createdAt`.
* ‚úÖ Chroma cleanup utility (`cleanupChromaLogs`).
* ‚úÖ Configurable via `LOG_TTL_DAYS` and `LOG_MAX_CHROMA`.
* ‚úÖ Job scheduled daily (or pluggable via cron).
* ‚úÖ Optional per-namespace retention (future).

---

üëâ Do you want me to also **write the actual `cleanupChromaLogs` function + cron wiring** so it‚Äôs ready to drop in, or just keep it at task-spec level?
