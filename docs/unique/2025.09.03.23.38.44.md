---
uuid: e9aece2c-221c-44fc-9bc3-83c591c8e74d
created_at: '2025-09-03T23:38:44Z'
filename: promethean-qdrant-demo
title: promethean-qdrant-demo
description: >-
  Minimal TypeScript setup using native ESM with .js imports, pure functions,
  and immutability for Qdrant integration via Ollama embeddings. Includes AVA
  tests and GPL-3.0 license.
tags:
  - typescript
  - qdrant
  - ollama
  - esm
  - immutable
  - pure-functions
  - ava
  - gpl-3.0
---
Got it. Here’s a minimal, production-leaning TypeScript setup that:

* uses **native ESM** + **.js imports in TS source**
* sticks to **pure functions** and **immutability**
* talks to **Qdrant** via `@qdrant/js-client-rest`
* generates embeddings via **Ollama** (change the model if you want)
* includes an **AVA test** to prove it works
* is **GPL-3.0-only**

It’s flat—drop these files in a single package and run.

---

# 1) `package.json`

```json
{
  "name": "@promethean/qdrant-demo",
  "version": "0.0.1",
  "private": true,
  "license": "GPL-3.0-only",
  "type": "module",
  "engines": { "node": ">=20.0.0" },
  "scripts": {
    "dev": "tsx src/demo.ts",
    "test": "ava",
    "build": "tsc -p tsconfig.json"
  },
  "dependencies": {
    "@qdrant/js-client-rest": "^1.11.0"
  },
  "devDependencies": {
    "ava": "^6.0.0",
    "tsx": "^4.19.0",
    "typescript": "^5.6.2"
  }
}
```

# 2) `tsconfig.json`

```json
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "NodeNext",
    "moduleResolution": "NodeNext",
    "verbatimModuleSyntax": true,
    "strict": true,
    "noUncheckedIndexedAccess": true,
    "resolveJsonModule": true,
    "isolatedModules": true,
    "outDir": "dist",
    "forceConsistentCasingInFileNames": true
  },
  "include": ["src", "test"]
}
```

> Why `NodeNext` + `.js` imports in TS? This matches Node ESM behavior and lets you write `import … from "./file.js"` **in TS**, producing correct paths after build.

---

# 3) `src/env.ts`

Small helper—pure, no mutation. Reads env with sane defaults.

```ts
export type Env = Readonly<{
  OLLAMA_URL: string;
  QDRANT_URL: string;
  QDRANT_API_KEY?: string;
  MODEL: string;
  COLLECTION: string;
}>;

export const getEnv = (): Env => ({
  OLLAMA_URL: process.env.OLLAMA_URL ?? "http://localhost:11434",
  QDRANT_URL: process.env.QDRANT_URL ?? "http://localhost:6333",
  QDRANT_API_KEY: process.env.QDRANT_API_KEY,
  MODEL: process.env.EMBED_MODEL ?? "nomic-embed-text",
  COLLECTION: process.env.QDRANT_COLLECTION ?? "docs"
});
```

---

# 4) `src/embed-ollama.ts`

Ollama embeddings endpoint. Returns immutable vectors.

```ts
type EmbeddingResponse = Readonly<{ embedding: number[] }>;
type BatchEmbeddingResponse = Readonly<{ embeddings: number[][] }>;

export const embedOne = async (
  baseUrl: string,
  model: string,
  text: string
): Promise<ReadonlyArray<number>> => {
  const r = await fetch(`${baseUrl}/api/embeddings`, {
    method: "POST",
    headers: { "content-type": "application/json" },
    body: JSON.stringify({ model, prompt: text })
  });
  if (!r.ok) throw new Error(`Ollama embed failed: ${r.status} ${r.statusText}`);
  const j = (await r.json()) as EmbeddingResponse;
  return Object.freeze([...j.embedding]);
};

export const embedBatch = async (
  baseUrl: string,
  model: string,
  texts: ReadonlyArray<string>
): Promise<ReadonlyArray<ReadonlyArray<number>>> => {
  // Ollama’s embeddings API is single-text. Map in parallel with Promise.all.
  const results = await Promise.all(texts.map(t => embedOne(baseUrl, model, t)));
  return Object.freeze(results.map(v => Object.freeze([...v])));
};
```

---

# 5) `src/qdrant.ts`

Qdrant client + pure helpers for ensure-collection, upsert, and search.

```ts
import { QdrantClient } from "@qdrant/js-client-rest";

export const qdrant = (url: string, apiKey?: string) =>
  new QdrantClient({ url, apiKey });

export const ensureCollection = async (
  client: QdrantClient,
  name: string,
  vectorSize: number
) => {
  const exists = await client.getCollections().then(r =>
    (r.collections ?? []).some(c => c.name === name)
  );
  if (exists) return;

  await client.createCollection(name, {
    vectors: { size: vectorSize, distance: "Cosine" },
    hnsw_config: {
      m: 16,
      ef_construct: 128
    },
    optimizers_config: { default_segment_number: 2 }
  });
};

export type DocPoint = Readonly<{
  id: string;
  text: string;
  vector: ReadonlyArray<number>;
  meta?: Record<string, unknown>;
}>;

export const upsert = async (
  client: QdrantClient,
  name: string,
  points: ReadonlyArray<DocPoint>
) => {
  await client.upsert(name, {
    wait: true,
    points: points.map(p => ({
      id: p.id,
      vector: [...p.vector],
      payload: { text: p.text, ...(p.meta ?? {}) }
    }))
  });
};

export type SearchResult = Readonly<{
  id: string | number;
  score: number;
  payload?: Record<string, unknown>;
}>;

export const search = async (
  client: QdrantClient,
  name: string,
  vector: ReadonlyArray<number>,
  topK = 5,
  filter?: Record<string, unknown>
): Promise<ReadonlyArray<SearchResult>> => {
  const r = await client.search(name, {
    vector: [...vector],
    limit: topK,
    with_payload: true,
    filter
  });
  return Object.freeze(
    r.map(x =>
      Object.freeze({
        id: x.id!,
        score: x.score ?? 0,
        payload: (x.payload ?? undefined) as Record<string, unknown> | undefined
      })
    )
  );
};
```

---

# 6) `src/id.ts`

Stable, deterministic IDs from content (no mutations). Node has `crypto.subtle` in >=20.

```ts
export const stableId = async (s: string): Promise<string> => {
  const data = new TextEncoder().encode(s);
  const digest = await crypto.subtle.digest("SHA-256", data);
  const hex = Array.from(new Uint8Array(digest))
    .map(b => b.toString(16).padStart(2, "0"))
    .join("");
  // qdrant IDs can be strings; keep short but stable
  return hex.slice(0, 24);
};
```

---

# 7) `src/demo.ts`

End-to-end: ensure collection → embed → upsert → query. Pure transforms, no shared mutable state.

```ts
import { getEnv } from "./env.js";
import { embedBatch, embedOne } from "./embed-ollama.js";
import { qdrant, ensureCollection, upsert, search } from "./qdrant.js";
import { stableId } from "./id.js";

const main = async () => {
  const env = getEnv();

  // Your corpus
  const docs = Object.freeze([
    "Qdrant is a high-performance vector database written in Rust.",
    "Chroma is a simple, convenient local vector store good for prototyping.",
    "We use embeddings to do semantic search over text.",
    "Functional programming favors pure functions and immutability."
  ]);

  // 1) Embed docs (parallel)
  const vectors = await embedBatch(env.OLLAMA_URL, env.MODEL, docs);

  // 2) Ensure collection with proper dim
  const dim = vectors[0]?.length ?? 0;
  if (dim <= 0) throw new Error("Empty embedding dimension");
  const client = qdrant(env.QDRANT_URL, env.QDRANT_API_KEY);
  await ensureCollection(client, env.COLLECTION, dim);

  // 3) Upsert points
  const points = await Promise.all(
    docs.map(async (text, i) => ({
      id: await stableId(text),
      text,
      vector: vectors[i]!,
      meta: { source: "demo", idx: i }
    }))
  );
  await upsert(client, env.COLLECTION, points);

  // 4) Query
  const query = "Which store is better for large-scale search?";
  const qv = await embedOne(env.OLLAMA_URL, env.MODEL, query);
  const results = await search(client, env.COLLECTION, qv, 3);

  // 5) Print (no mutation)
  const view = results.map(r => ({
    id: r.id,
    score: Number(r.score.toFixed(4)),
    text: typeof r.payload?.text === "string" ? r.payload?.text : null
  }));
  console.log(JSON.stringify(view, null, 2));
};

main().catch(err => {
  console.error(err);
  process.exitCode = 1;
});
```

---

# 8) `test/qdrant.spec.ts`

Tiny AVA smoke test (skips if Qdrant/Ollama aren’t up).

```ts
import test from "ava";
import { getEnv } from "../src/env.js";
import { qdrant, ensureCollection, upsert, search } from "../src/qdrant.js";
import { embedBatch, embedOne } from "../src/embed-ollama.js";
import { stableId } from "../src/id.js";

const up = async () => {
  const env = getEnv();
  // quick ping
  const ok = await fetch(`${env.QDRANT_URL}/healthz`).then(r => r.ok).catch(() => false);
  const ok2 = await fetch(`${env.OLLAMA_URL}/api/tags`).then(r => r.ok).catch(() => false);
  return ok && ok2;
};

test("qdrant end-to-end", async t => {
  if (!(await up())) {
    t.log("Skipping: Qdrant or Ollama not reachable");
    t.pass();
    return;
  }

  const env = getEnv();
  const client = qdrant(env.QDRANT_URL, env.QDRANT_API_KEY);

  const docs = ["alpha beta", "semantic search with vectors", "chroma vs qdrant"];
  const vecs = await embedBatch(env.OLLAMA_URL, env.MODEL, docs);

  await ensureCollection(client, env.COLLECTION, vecs[0]!.length);

  const pts = await Promise.all(
    docs.map(async (text, i) => ({
      id: await stableId(text),
      text,
      vector: vecs[i]!,
      meta: { tag: "test" }
    }))
  );
  await upsert(client, env.COLLECTION, pts);

  const qv = await embedOne(env.OLLAMA_URL, env.MODEL, "vector database speed");
  const res = await search(client, env.COLLECTION, qv, 2);

  t.true(res.length > 0);
  t.truthy(res[0]!.payload?.text);
});
```

---

## Run it

1. Qdrant (Docker):

```bash
docker run -p 6333:6333 -v $(pwd)/qdrant_storage:/qdrant/storage qdrant/qdrant
```

2. Ollama (and pull a text embedding model):

```bash
# if not installed: https://ollama.com
ollama pull nomic-embed-text
```

3. Env + dev:

```bash
export OLLAMA_URL=http://localhost:11434
export QDRANT_URL=http://localhost:6333
export EMBED_MODEL=nomic-embed-text
export QDRANT_COLLECTION=docs

pnpm i
pnpm dev
pnpm test
```

---

## Notes / trade-offs (no fluff)

* **Speed**: This will already beat Chroma for non-trivial sizes. For millions of vectors, tune `hnsw_config` (`m`, `ef_construct`) and runtime `ef` (via search params) per your latency/recall target.
* **Filtering**: Add Qdrant `filter` in `search()` for payload constraints (e.g., `{ must: [{ key: "source", match: { value: "demo" } }] }`).
* **Dimensions**: Must match the embedding model. If you switch to `bge-m3` or similar, re-create the collection with the right `size`.
* **IDs**: I’m hashing text. In real apps, use your own stable IDs (file UUID + chunk index).
* **Immutability**: All transforms return new values; no shared mutable state. Easy to parallelize and test.

If you want, I can bolt on a tiny **@fastify/static** server that serves a webcomponent UI for querying the index next—still flat, still ESM.
