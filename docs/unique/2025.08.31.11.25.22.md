Roger. Here’s a clean **per-domain policy system** wired into your JS crawler — no Python, no SaaS. You get a single YAML file to tune throttles, allow/deny rules, login steps, headers/cookies, extraction rules, proxy overrides (e.g., Tor via Privoxy), and per-domain limits. Drop these files in, set one env var, and go.

---

## 0) Compose tweak (point crawler at policies file)

Add one env var + mount (keeps your code immutable):

```yaml
  crawler-js:
    # ...
    environment:
      - POLICY_FILE=/workspace/policies.yaml
      # rest unchanged (PROXY_URL still works as global default)
    volumes:
      - ./infra/crawler-js:/workspace:rw
      - crawl_data:/data
      - crawl_storage:/workspace/storage
```

---

## 1) Policy file (YAML)

`infra/crawler-js/policies.yaml`

```yaml
# Global defaults (used when no domain match)
defaults:
  sameDomainOnly: true
  respectRobots: true
  concurrency: 4          # per crawler (we still cap globally)
  rpm: 120                # requests per minute cap
  maxPages: 200           # global cap (crawler’s maxRequestsPerCrawl)
  maxDepth: 3             # link hop depth (0 = only seed)
  delayMs: 250            # base delay between requests
  jitterMs: 200           # random +/- added to delay
  retries: 2              # per-request retries
  dedupNormalize: true
  sitemapDiscover: true
  rssDiscover: true
  blockResources: ["image", "media", "font", "stylesheet"]  # save bandwidth
  allow: []              # regex list (case-insensitive); empty = allow all
  deny: []               # regex list; any match = block
  headers:               # sent on every request for matching domains
    Accept-Language: "en-US,en;q=0.9"
  cookies: []            # [{ name, value, domain, path, httpOnly, secure }]
  proxy: ""              # override (e.g., http://privoxy:8118). Empty = use PROXY_URL env.
  extractor:
    mode: "article"      # 'article' | 'simple' | 'raw'
    keepHtml: false

# Domain-specific overrides (first match wins). domains: list of host globs or regex.
domains:

  - name: "Hacker News"
    domains: ["news.ycombinator.com"]
    sameDomainOnly: true
    rpm: 60
    concurrency: 2
    maxPages: 500
    allow: ["^https://news\\.ycombinator\\.com/"]
    deny: ["\\.gif$", "\\.png$", "\\.jpg$"]
    extractor:
      mode: "simple"
      keepHtml: false

  - name: "Example Blog with login"
    domains: ["blog.example.com", "/^sub\\d+\\.example\\.com$/"]
    rpm: 30
    concurrency: 2
    maxDepth: 2
    proxy: "http://privoxy:8118"         # force Tor just for this domain
    headers:
      User-Agent: "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome Safari"
    cookies:
      - name: "cookie_consent"
        value: "yes"
        domain: ".example.com"
        path: "/"
        httpOnly: false
        secure: false
    login:                                # run once per origin before crawling links
      steps:
        - goto: "https://blog.example.com/login"
        - type: { selector: "input[name='username']", text: "local_user" }
        - type: { selector: "input[name='password']", text: "local_pass" }
        - click: { selector: "button[type='submit']" }
        - waitForSelector: { selector: "nav .user-avatar", timeoutMs: 15000 }
    extractor:
      mode: "article"
      keepHtml: false

  - name: "Docs site (HTML only, no JS)"
    domains: ["docs.example.org"]
    blockResources: ["*"]  # block all non-doc requests
    sitemapDiscover: true
    rssDiscover: false
    extractor:
      mode: "raw"
      keepHtml: true

# Optional: explicit seeds (in addition to CRAWL_SEED env)
seeds:
  - "https://news.ycombinator.com/"
  - "https://docs.example.org/"
```

**How matching works:** `domains[].domains` accepts either host globs (e.g., `*.example.com`) or regex strings delimited with `/.../`. First match wins.

---

## 2) Add a tiny YAML dep

`infra/crawler-js/package.json` — add `yaml`

```json
{
  "name": "crawler-js",
  "private": true,
  "type": "module",
  "scripts": { "start": "node src/crawl.js" },
  "dependencies": {
    "crawlee": "^3.9.2",
    "playwright": "^1.47.2",
    "robots-parser": "^3.0.1",
    "node-fetch": "^3.3.2",
    "fast-xml-parser": "^4.5.0",
    "p-limit": "^6.2.0",
    "yaml": "^2.5.1"
  }
}
```

---

## 3) Wire policies into the crawler

### `infra/crawler-js/src/utils.js` (additions)

```js
import fs from 'node:fs';
import path from 'node:path';
import YAML from 'yaml';

export function loadPolicies(filePath) {
  const p = filePath || process.env.POLICY_FILE || '/workspace/policies.yaml';
  const raw = fs.readFileSync(p, 'utf8');
  const cfg = YAML.parse(raw);
  cfg.defaults ||= {};
  cfg.domains ||= [];
  cfg.seeds ||= [];
  // preprocess domain matchers
  for (const d of cfg.domains) {
    d._matchers = (d.domains || []).map(s => {
      if (s.startsWith('/') && s.endsWith('/')) return { type: 're', re: new RegExp(s.slice(1, -1), 'i') };
      // glob-ish → convert dots and * to regex
      const rx = '^' + s.replace(/[.+?^${}()|[\]\\]/g, '\\$&').replace(/\*/g, '.*') + '$';
      return { type: 'glob', re: new RegExp(rx, 'i') };
    });
  }
  return cfg;
}

export function matchPolicyFor(url, policies) {
  const host = new URL(url).hostname;
  for (const d of policies.domains) {
    if (d._matchers?.some(m => m.re.test(host))) return d;
  }
  return policies.defaults || {};
}

export function compileRegexList(csvOrList) {
  if (!csvOrList) return [];
  const items = Array.isArray(csvOrList) ? csvOrList : String(csvOrList).split(',');
  return items.map(s => s.trim()).filter(Boolean).map(s => new RegExp(s, 'i'));
}
```

(Keep your previous helpers like `sleep`, `normalizeUrlForDedup`, `buildRobotsForOrigin`, `decideUrl`—they still apply, just feed them policy-specific values.)

### `infra/crawler-js/src/crawl.js` (replaced core with policy-aware flow)

```js
import { PlaywrightCrawler, KeyValueStore, Dataset, log, Configuration } from 'crawlee';
import fs from 'node:fs';
import path from 'node:path';
import { Agent as HttpProxyAgent } from 'node:http';
import { Agent as HttpsProxyAgent } from 'node:https';
import { XMLParser } from 'fast-xml-parser';
import fetch from 'node-fetch';
import uaPool from './ua.json' assert { type: 'json' };
import { sleep, compileRegexList, normalizeUrlForDedup, buildRobotsForOrigin, decideUrl, loadPolicies, matchPolicyFor } from './utils.js';
import { sinkToOpenSearch, sinkToMeili } from './sinks.js';

const policies = loadPolicies();
const globalDefaults = policies.defaults || {};
const envSeed = process.env.CRAWL_SEED;
const initialSeeds = new Set([].concat(policies.seeds || [], envSeed ? [envSeed] : []));

const outputDir = process.env.OUTPUT_DIR || '/data';
fs.mkdirSync(outputDir, { recursive: true });
const outPath = path.join(outputDir, 'out.jsonl');
const appendJSONL = (o) => fs.appendFileSync(outPath, JSON.stringify(o) + '\n');

// sinks (still optional/local)
const OS_URL = process.env.SINK_OPENSEARCH_URL || '';
const OS_INDEX = process.env.SINK_OPENSEARCH_INDEX || 'documents';
const MEILI_URL = process.env.SINK_MEILI_URL || '';
const MEILI_KEY = process.env.SINK_MEILI_KEY || '';
const MEILI_INDEX = process.env.SINK_MEILI_INDEX || 'documents';

function rotateUA(i) {
  return uaPool[i % uaPool.length] || uaPool[0];
}

function buildProxyAgents(url, policy) {
  const override = policy.proxy && policy.proxy.trim() ? policy.proxy.trim() : (process.env.PROXY_URL || '').trim();
  if (!override) return { httpAgent: undefined, httpsAgent: undefined, proxyUrl: '' };
  return {
    httpAgent: new HttpProxyAgent(override),
    httpsAgent: new HttpsProxyAgent(override),
    proxyUrl: override
  };
}

async function discoverSitemaps(origin, agents, enable) {
  if (!enable) return [];
  try {
    const res = await fetch(`${origin}/sitemap.xml`, { agent: origin.startsWith('https') ? agents.httpsAgent : agents.httpAgent, timeout: 10000 });
    if (!res.ok) return [];
    const xml = await res.text();
    const parser = new XMLParser({ ignoreAttributes: false });
    const j = parser.parse(xml);
    const urls = [];
    if (j.urlset?.url) {
      const arr = Array.isArray(j.urlset.url) ? j.urlset.url : [j.urlset.url];
      for (const u of arr) if (u.loc) urls.push(u.loc);
    }
    if (j.sitemapindex?.sitemap) {
      const arr = Array.isArray(j.sitemapindex.sitemap) ? j.sitemapindex.sitemap : [j.sitemapindex.sitemap];
      for (const sm of arr) if (sm.loc) urls.push(sm.loc);
    }
    return urls;
  } catch { return []; }
}

async function discoverRSS(origin, agents, enable) {
  if (!enable) return [];
  try {
    const res = await fetch(origin, { agent: origin.startsWith('https') ? agents.httpsAgent : agents.httpAgent, timeout: 10000 });
    if (!res.ok) return [];
    const html = await res.text();
    const matches = [...html.matchAll(/<link[^>]+type=['"]application\/(rss\+xml|atom\+xml)['"][^>]*>/gi)];
    const urls = [];
    for (const m of matches) {
      const href = (m[0].match(/href=['"]([^'"]+)['"]/i) || [])[1];
      if (href) urls.push(new URL(href, origin).toString());
    }
    return urls;
  } catch { return []; }
}

function extractByMode(mode, keepHtml, page) {
  return page.content().then(async html => {
    if (mode === 'raw') return keepHtml ? { html } : { text: html.replace(/\s+/g, ' ').slice(0, 500000) };
    if (mode === 'simple') {
      const title = await page.title().catch(()=> '');
      const text = await page.$eval('body', el => el.innerText).catch(()=> '');
      return keepHtml ? { title, html } : { title, text };
    }
    // 'article' heuristic (cheap)
    const title = await page.title().catch(()=> '');
    const meta = {};
    for (const n of ['og:title','og:description','description','article:author','author','og:site_name','article:published_time']) {
      try {
        meta[n] = await page.$eval(`meta[property="${n}"],meta[name="${n}"]`, el => el.content);
      } catch {}
    }
    const mainText = await page.$eval('main', el => el.innerText).catch(async () =>
      page.$eval('article', el => el.innerText).catch(async () =>
        page.$eval('body', el => el.innerText).catch(()=> '')
      )
    );
    return keepHtml ? { title, meta, html } : { title, meta, text: mainText };
  });
}

// cache: per-origin login done
const loginDone = new Set();

Configuration.set({ persistStorage: true, storageDir: './storage' });

const crawler = new PlaywrightCrawler({
  maxRequestsPerCrawl: +(process.env.CRAWL_MAX_PAGES || globalDefaults.maxPages || 200),
  maxConcurrency: +(process.env.CRAWL_CONCURRENCY || globalDefaults.concurrency || 4),
  maxRequestsPerMinute: +(process.env.CRAWL_REQS_PER_MIN || globalDefaults.rpm || 120),
  headless: true,
  requestHandlerTimeoutSecs: 90,
  launchContext: { launchOptions: { args: ['--no-sandbox', '--disable-dev-shm-usage'] } },

  async preNavigationHooks([{ request, page, session }, gotoOptions]) {
    const pol = matchPolicyFor(request.url, policies);
    const seedOrigin = new URL(request.url).origin;
    const allow = compileRegexList(pol.allow || globalDefaults.allow || []);
    const deny = compileRegexList(pol.deny || globalDefaults.deny || []);
    const sameDomainOnly = pol.sameDomainOnly ?? globalDefaults.sameDomainOnly ?? true;
    const respectRobots = pol.respectRobots ?? globalDefaults.respectRobots ?? true;
    const dedupNormalize = pol.dedupNormalize ?? globalDefaults.dedupNormalize ?? true;

    const agents = buildProxyAgents(request.url, pol);
    const urlToFetch = dedupNormalize ? normalizeUrlForDedup(request.url) : request.url;

    if (!decideUrl(urlToFetch, { sameDomainOnly, seedOrigin, allow, deny })) {
      request.noRetry = true; throw new Error('Filtered: allow/deny or cross-domain');
    }

    if (respectRobots) {
      const rb = await buildRobotsForOrigin(new URL(urlToFetch).origin, urlToFetch.startsWith('https') ? agents.httpsAgent : agents.httpAgent);
      if (!rb.isAllowed(urlToFetch, 'PrometheanCrawler')) {
        request.noRetry = true; throw new Error('Robots disallow');
      }
    }

    // headers and cookies
    const headers = { ...(globalDefaults.headers||{}), ...(pol.headers||{}) };
    request.headers = { ...(request.headers||{}), ...headers, 'User-Agent': request.headers?.['User-Agent'] || (session?.id ? rotateUA(parseInt(session.id,10)) : rotateUA(Math.floor(Math.random()*1000))) };

    if ((pol.cookies && pol.cookies.length) || (globalDefaults.cookies && globalDefaults.cookies.length)) {
      const cookies = [...(globalDefaults.cookies || []), ...(pol.cookies || [])];
      if (cookies.length) await page.context().addCookies(cookies);
    }

    // resource blocking
    const blockRules = (pol.blockResources?.length ? pol.blockResources : globalDefaults.blockResources) || [];
    if (blockRules.length) {
      await page.route('**/*', route => {
        const req = route.request();
        const type = req.resourceType();
        if (blockRules.includes('*') || blockRules.includes(type)) return route.abort();
        return route.continue();
      });
    }

    // per-domain login (once per origin)
    if (pol.login && !loginDone.has(seedOrigin)) {
      await page.context().clearCookies().catch(()=>{});
      for (const step of pol.login.steps || []) {
        if (step.goto) await page.goto(step.goto, { waitUntil: 'domcontentloaded', timeout: 30000 });
        if (step.type) await page.fill(step.type.selector, step.type.text, { timeout: 15000 });
        if (step.click) await page.click(step.click.selector, { timeout: 15000 });
        if (step.waitForSelector) await page.waitForSelector(step.waitForSelector.selector, { timeout: step.waitForSelector.timeoutMs || 15000 });
      }
      loginDone.add(seedOrigin);
    }

    // delay/jitter
    const base = +(pol.delayMs ?? globalDefaults.delayMs ?? 0);
    const jit = +(pol.jitterMs ?? globalDefaults.jitterMs ?? 0);
    if (base || jit) await new Promise(r => setTimeout(r, base + Math.floor(Math.random() * (jit + 1))));

    // proxy
    if (agents.proxyUrl) gotoOptions.proxy = { server: agents.proxyUrl };

    gotoOptions.waitUntil = 'domcontentloaded';
  },

  async requestHandler({ request, page, enqueueLinks }) {
    const pol = matchPolicyFor(request.url, policies);
    const dedupNormalize = pol.dedupNormalize ?? globalDefaults.dedupNormalize ?? true;

    const url = dedupNormalize ? normalizeUrlForDedup(request.url) : request.url;
    const title = await page.title().catch(()=>'');
    const now = new Date().toISOString();

    // extraction
    const mode = pol.extractor?.mode || globalDefaults.extractor?.mode || 'article';
    const keepHtml = pol.extractor?.keepHtml ?? globalDefaults.extractor?.keepHtml ?? false;
    const body = await extractByMode(mode, keepHtml, page);

    const doc = { url, title, fetched_at: now, ...body };

    appendJSONL(doc);
    await Dataset.pushData(doc);
    await sinkToOpenSearch([doc], { url: OS_URL, index: OS_INDEX });
    await sinkToMeili([doc], { url: MEILI_URL, index: MEILI_INDEX, apiKey: MEILI_KEY });

    // depth-aware enqueuing
    const sameDomainOnly = pol.sameDomainOnly ?? globalDefaults.sameDomainOnly ?? true;
    const allow = compileRegexList(pol.allow || globalDefaults.allow || []);
    const deny = compileRegexList(pol.deny || globalDefaults.deny || []);
    const maxDepth = +(pol.maxDepth ?? globalDefaults.maxDepth ?? 3);

    const { depth = 0 } = request.userData || {};
    if (depth < maxDepth) {
      await enqueueLinks({
        strategy: sameDomainOnly ? 'same-domain' : 'all',
        transformRequestFunction: (req) => {
          // filter by allow/deny as we queue
          if (!decideUrl(req.url, { sameDomainOnly, seedOrigin: new URL(request.url).origin, allow, deny })) return null;
          const next = dedupNormalize ? normalizeUrlForDedup(req.url) : req.url;
          return { url: next, userData: { depth: depth + 1 } };
        }
      });
    }
  },

  async failedRequestHandler({ request, error }) {
    appendJSONL({ url: request.url, error: String(error), failed_at: new Date().toISOString() });
  }
});

// bootstrap queue with policy seeds + env seed + optional sitemap/rss per domain
const rq = await KeyValueStore.open();
await rq.setValue('__meta__', { seeds: [...initialSeeds], started_at: new Date().toISOString() });

const seeds = new Set(initialSeeds);
for (const s of seeds) {
  const pol = matchPolicyFor(s, policies);
  const agents = buildProxyAgents(s, pol);
  const origin = new URL(s).origin;
  const sm = await discoverSitemaps(origin, agents, pol.sitemapDiscover ?? globalDefaults.sitemapDiscover ?? true);
  const rss = await discoverRSS(origin, agents, pol.rssDiscover ?? globalDefaults.rssDiscover ?? true);
  for (const u of sm) seeds.add(u);
  for (const u of rss) seeds.add(u);
}

log.setLevel(log.LEVELS.INFO);
await crawler.run([...seeds]);
console.log(`JSONL: ${outPath}`);
```

---

## 4) Run examples

**Local, no Tor:**

```bash
POLICY_FILE=./infra/crawler-js/policies.yaml \
docker compose --profile crawl-js up --build crawler-js
```

**Through Tor/Privoxy (for matching domains only):**

```bash
docker compose --profile crawl-js --profile tor up -d tor privoxy
POLICY_FILE=./infra/crawler-js/policies.yaml \
docker compose --profile crawl-js up --build crawler-js
```

(Any domain stanza with `proxy: "http://privoxy:8118"` will route via Tor. Everything else follows `PROXY_URL` if set, otherwise direct.)

---

## 5) Why this setup won’t fight you

* **Single source of truth**: `policies.yaml` controls throttles, RPM, depth, allow/deny, robots, headers, cookies, **login**, extract mode, and per-domain proxy.
* **Login once** per origin using deterministic steps (goto/type/click/wait).
* **Bandwidth sane**: resource blocking per domain (e.g., kill images/fonts on docs sites).
* **Extraction modes**: `article` (cheap heuristic), `simple` (title + body text), `raw` (HTML).
* **No surprise egress**: explicit proxy override per domain; keep the rest local.
* **No Python.** All Node, all the time.

If you want, I can add a **domain-local JavaScript extractor hook** in YAML (e.g., `extractor.script: "return document.querySelector('article')?.innerText || ''"`) that we `page.evaluate()` — gives you surgical control without editing code.

\#webcrawling #javascript #playwright #crawlee #docker #tor #privoxy #selfhosted #opensearch #meilisearch #config-as-code #policies
