Alright, let’s nail this down in math so we can *engineer* it, not vibe it.

# User-Specific Path Extraction (USPX): a mathematical blueprint

## 1) Setup and notation

Let a teacher MoE transformer be

$$
f_\theta(x) = \mathrm{LMHead}\big(h_L\big),\qquad
h_{\ell+1} = \mathrm{Block}_\ell(h_\ell),\quad \ell=1,\dots,L.
$$

Each block has attention and FFN; for MoE layers:

$$
\mathrm{FFN}^{\text{MoE}}(h) = \sum_{e=1}^{E} \pi_e(h)\, \mathrm{FFN}_e(h),\quad
\pi(h)=\mathrm{TopKSoftmax}\big(W_g h\big),\ \sum_e \pi_e(h)=1,
$$

with **Top-K** gating (typically $K\!\ll\!E$).
**Total parameters:** $P_{\text{tot}}$. **Active per token:** roughly those used by the selected $K$ experts, hence the confusing “3.6B active params” line.

Your **user distribution** $\mathcal{D}_U$ is a dataset of prompts/tasks representative of your workload (ErrBench).

We’ll identify a **path set** $\mathcal{G}$ of units to score and possibly prune:

* attention heads $g=(\ell,i) \in \{1..L\}\times\{1..H_\ell\}$,
* MLP channels/groups $g=(\ell,S)\subseteq \{1..d_{\text{mlp},\ell}\}$,
* MoE experts $g=(\ell,e)\in \{1..L_{\text{moe}}\}\times\{1..E_\ell\}$,
* (optional) **SAE features** $g=(\ell,k)$ discovered from sparse autoencoders.

Each group $g$ has parameter count $w_g$ (“cost”) and an **importance** $v_g$ (“value”) we’ll define below.

---

## 2) Memory & latency budgets (why this matters)

**Weights memory (quantized):**

$$
M_{\text{w}} = \frac{q}{8}\, P_{\text{tot}} \ \text{bytes},
$$

with $q$ bits/weight (e.g., 4 for GPTQ/AWQ; MXFP4 is model-specific compression).

**KV-cache memory** for dense attention ($b_{\text{kv}}$ bytes/element, $d_{\text{model}} = \sum_i d_k$):

$$
M_{\text{KV}} \approx N \cdot L \cdot 2\, d_{\text{model}} \cdot b_{\text{kv}} \quad \text{bytes},
$$

where $N$ is context length.
Example (feel the scale):

* $L\!=\!32, d_{\text{model}}\!=\!4096, b_{\text{kv}}\!=\!1$ (FP8), $N\!=\!4096$
  $\Rightarrow M_{\text{KV}}\approx 1.0\ \text{GiB}$.
* $L\!=\!48, d_{\text{model}}\!=\!6144, b_{\text{kv}}\!=\!1$, $N\!=\!4096$
  $\Rightarrow M_{\text{KV}}\approx 2.25\ \text{GiB}$.

**MoE compute vs memory:** per token, FLOPs scale with top-$K$ experts (compute-sparse), but **memory must hold all experts** unless you implement expert paging. Hence your 8 GB wall.

Takeaway: **Cut context first** (controls $M_{\text{KV}}$); **then** cut structure/experts (controls $M_{\text{w}}$).

---

## 3) Importance metrics (how we score paths)

We want each group’s **causal contribution** to your loss on $\mathcal{D}_U$.

### 3.1 Loss-difference (ablation) score

Mask group $g$ (zero outputs or route away experts), evaluate delta:

$$
v_g^{\text{abl}} \;=\; \mathbb{E}_{(x,y)\sim \mathcal{D}_U}\big[\, \mathcal{L}(f_{\theta\setminus g}(x),y) - \mathcal{L}(f_\theta(x),y)\,\big].
$$

Pros: faithful. Cons: expensive (one forward per group, ideally with attribution patching for stability).

### 3.2 First/second-order Taylor (cheap approximations)

With loss $\mathcal{L}$ and weights $\theta$, prune parameters in group $g$ ($\Delta\theta_g=-\theta_g$):

* **OBS/OBD diagonal**:

$$
v_g^{(2)} \approx \frac{1}{2}\,\theta_g^\top \mathrm{diag}(H)\, \theta_g \;\;\approx\;\; \frac{1}{2}\sum_{j\in g} \theta_j^2 H_{jj},
$$

estimate $H_{jj}$ via gradient variance or Gauss-Newton. Aggregate by group for **structured** saliency.

### 3.3 Routing frequency × marginal contribution (for MoE)

Let $r_{\ell,e}(x,t)\in\{0,1\}$ indicate expert $e$ chosen at layer $\ell$, token $t$; define:

$$
\text{usage}_{\ell,e} \;=\; \mathbb{E}_{x\sim\mathcal{D}_U}\, \mathbb{E}_t\,[\,r_{\ell,e}(x,t)\,],
$$

and marginal ablation on routed tokens:

$$
\Delta_{\ell,e} \;=\; \mathbb{E}_{x,t:\,r_{\ell,e}=1}\big[\mathcal{L}(f_{\theta\setminus(\ell,e)}(x),y) - \mathcal{L}(f_\theta(x),y)\big].
$$

Then set $v_{(\ell,e)} = \text{usage}_{\ell,e}\cdot \Delta_{\ell,e}$. This promotes **frequent & useful** experts.

### 3.4 SAE-feature mutual information (optional, more surgical)

Train a sparse autoencoder on layer activations $a_\ell\in\mathbb{R}^{d_\ell}$ to get codes $z_\ell = \mathrm{SAE}_\ell(a_\ell)\in\mathbb{R}^{k_\ell}$, and a task variable $S$ (e.g., success token-level, or low loss). Score:

$$
v_{(\ell,k)}^{\text{MI}} \;=\; I\!\left(z_{\ell,k};\,S\right).
$$

High-MI features are **task-predictive**; keep heads/channels that produce/consume those features.

---

## 4) Selection under a budget (make it a knapsack)

We want a kept set $K\subseteq \mathcal{G}$ that maximizes total value under a memory budget $B$:

$$
\max_{K\subseteq \mathcal{G}} \sum_{g\in K} v_g \quad \text{s.t.}\quad \sum_{g\in K} w_g \le B.
$$

* **Greedy ratio** $v_g/w_g$ is the fast baseline.
* Use **grouped constraints** (e.g., “keep at least $K_\ell$ experts per MoE layer”).
* Or solve via **Lagrangian** with $\lambda$: keep groups with $v_g - \lambda w_g>0$.

If you’re brave, learn masks directly.

---

## 5) Learnable masks (continuous L₀ with hard-concrete)

Associate each group $g$ with a **gate** $\alpha_g\in[0,1]$ multiplying its output. Optimize:

$$
\min_{\theta,\,\alpha}\ \mathbb{E}_{\mathcal{D}_U}\big[\mathcal{L}(f_{\theta,\alpha}(x),y)\big]
\;+\;
\lambda \sum_{g\in \mathcal{G}} \mathrm{L0}(\alpha_g),
$$

where $\mathrm{L0}$ is the expected L₀ “on/off” using a **hard-concrete** reparam (Louizos et al.). After training, **prune** groups with $\alpha_g$ below a threshold.
This gives you a *data-driven* sparse subnetwork tuned to your distribution.

---

## 6) Path-preserving distillation to a small dense student

Let student $f_\phi$ (7–8B dense). Minimize a **multi-term** objective:

**(a) Token-level KD (response-only mask is fine)**

$$
\mathcal{L}_{\text{KD}} = \mathbb{E}_{x\sim\mathcal{D}_U}\!\left[
\tau^2\, \mathrm{KL}\big( p_T(\cdot|x)/\tau\ \|\ p_S(\cdot|x)/\tau \big)\right].
$$

**(b) Intermediate matching on “important paths”**
Select layers $\mathcal{L}^\*$ and projection matrices $P_\ell$ that pick the **kept** features/heads. Fit per-layer linear maps $M_\ell$ (or 1×1 convs) to align spaces:

$$
\mathcal{L}_{\text{int}} = \sum_{\ell\in \mathcal{L}^\*}
\mathbb{E}_{x}\,\big\| M_\ell\, h^{S}_\ell(x) - P_\ell\, h^{T}_\ell(x)\big\|_2^2 .
$$

**(c) Group-lasso for *student* sparsity** (makes the student prefer the same “diamoic” routes)

$$
\mathcal{L}_{\text{grp}} = \sum_{g\in\mathcal{G}_S} \big\|W_g\big\|_2,
$$

where $W_g$ are student parameters of the corresponding head/channel group.

**Total:**

$$
\min_\phi\ \mathcal{L}_{\text{KD}} + \lambda_{\text{int}}\mathcal{L}_{\text{int}} + \lambda_{\text{grp}}\mathcal{L}_{\text{grp}}.
$$

Train with **QLoRA (NF4)** on a single decent GPU; then export the merged student to **GPTQ/AWQ 4-bit** for your 8 GB card.

---

## 7) MoE expert restriction (analysis vs serving)

During **analysis** on the teacher, you can *restrict* to a **kept expert set** $\mathcal{E}_\ell$ by modifying the gate:

$$
\tilde{\pi}_{\ell,e}(h) =
\begin{cases}
\frac{\exp(z_{\ell,e})}{\sum_{e'\in \mathcal{E}_\ell}\exp(z_{\ell,e'})}, & e\in \mathcal{E}_\ell,\\
0, & \text{otherwise,}
\end{cases}
\quad z_{\ell} = W_{g,\ell} h.
$$

Useful to validate your selection ($\Delta $loss vs baseline). For **serving**, prefer the **dense student** to avoid MoE memory overhead on your 8 GB box.

---

## 8) Bandit-style fast scoring (fewer full passes)

If evaluating every $g$ is costly, treat ablations as **arms** with reward $-\Delta\mathcal{L}$ and cost $w_g$. Use **UCB/Thompson sampling** to estimate $v_g$ with far fewer runs:

* Sample a batch of groups to mask,
* Observe loss delta,
* Update posterior/upper bounds,
* Prioritize uncertain but promising groups.

This gets you a near-optimal $K$ with 10–20× fewer evals than brute force.

---

## 9) What to actually prune (and when)

* **Phase 1 (safe):** **mask-only** (no shape change) + **context cut** → verify little loss on ErrBench.
* **Phase 2 (speed):** **structured pruning** of heads/channels/experts **then** short SFT (recover accuracy).
  Use Taylor $v_g^{(2)}$ to rank; prune lowest $v/w$; fine-tune 0.5–2 epochs on $\mathcal{D}_U$.
* **Phase 3 (product):** distill to **7–8B** student with the loss in §6; quantize; ship.

---

## 10) Sanity checks & acceptance criteria

* **Per-task loss delta:** for each task family $t$, require

$$
\Delta\mathcal{L}_t \;=\; \mathbb{E}_{(x,y)\in t}\big[\mathcal{L}(f_{\text{student}}(x),y)-\mathcal{L}(f_T(x),y)\big]
\le \epsilon_t.
$$

* **Latency gain:** tokens/sec improvement $\ge$ factor $G$ on the 8 GB laptop at $N\!=\!2048$.
* **Memory fit:** $M_{\text{w}} + M_{\text{KV}} \le 7.5\ \text{GiB}$ (safety margin).

---

## 11) Minimal experiment you can run this week

1. **Build ErrBench** (\~2–5k prompts).
2. **Compute $v_g$ for experts** via usage×marginal delta on 1–2 MoE layers; keep top-K per layer.
3. **Ablate heads** in 3–5 layers using attribution-patching; keep top-X heads.
4. **Mask-only run**: confirm small $\Delta$loss on ErrBench.
5. **Train student** (Llama-3.1-8B or Mistral-7B) with $\mathcal{L}_{\text{KD}}+\lambda_{\text{int}}\mathcal{L}_{\text{int}}$.
6. **Quantize 4-bit** and serve with ExLlamaV2; measure t/s and acceptance criteria.

If it fails, your masks are too aggressive or ErrBench is under-representative. Iterate.

---

## 12) Hard truths

* You won’t make a 20B MoE feel “fast” on 8 GB by clever routing alone. The **dense student** (7–8B, 4-bit) is the correct endgame for interactive use.
* **Context** is a first-class budget. Every time you forget that, KV-cache reminds you.
* Fancy interpretability helps you *choose the right bits* to keep; it doesn’t erase physics.

---

## 13) Useful constants & rules of thumb

* **KV cache (bytes)**: $M_{\text{KV}} \approx N L 2 d_{\text{model}} b_{\text{kv}}$.
* **FP8 cache** halves FP16; **INT4 cache** halves FP8 (with quality cost).
* **Group-lasso** promotes structured zeros → real speed on kernels that skip zeros.
* **Greedy $v/w$** is shockingly strong; start there, then refine with mask learning.

---

If you want, next step I’ll turn this into a **checklist with concrete hyperparams** (e.g., $ \lambda_{\text{int}}, \lambda_{\text{grp}}, \tau, K$ per layer) and a one-page **metric dashboard** you can drop into your eval harness.

\#hashtags
\#mechanistic-interpretability #moe #pruning #hardconcrete #distillation #qlora #knapsack #kv-cache #ants #daimoi #systems-design
