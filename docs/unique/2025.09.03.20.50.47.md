---
uuid: 45179b8b-0d83-4bdc-8e9c-a87ed586f34e
created_at: 2025.09.03.20.50.47.md
filename: docops-pipeline
description: >-
  A TypeScript-first, idempotent document operations pipeline that processes
  markdown files through a commit-driven workflow with content hashing for
  caching. It includes front-matter normalization, stage-based routing, and
  analytics tracking.
tags:
  - typescript
  - idempotent
  - commit-driven
  - content-caching
  - docops
  - pipeline
  - markdown
  - workflow
---
love this. let’s turn it into a concrete, idempotent, TypeScript-first doc-ops pipeline you can drop into `packages/` and wire into your existing `pipelines.yml`. it’s file-centric, commit-driven, cached, and easy to extend with agents.

# High-level flow (files only, everything traceable)

```mermaid
flowchart TD
  A[Inbox: docs/unique/incoming/] --> B[Triage + Front-matter seed]
  B --> C[Classify (LLM+rules)]
  C -->|task| D[Route → docs/agile/tasks/incoming/]
  C -->|doc|  E[Route → docs/unique/drafts/]
  D --> F[Task refine → breakdown, sizing, priority]
  E --> G[Doc refine → headings, summary, tags]
  F --> H[Review (peer / policy gates)]
  G --> H
  H --> I[Finalize → mark status, link to board]
  I --> J[Analytics: progress, lead time, throughput]
```

Every arrow creates a commit with machine-readable footers; all transforms are idempotent and cached by **content hash**, so re-running the pipeline is cheap and safe.

---

## Folder conventions

* **Inbox**: `docs/unique/incoming/` (dump *everything* here)
* **Task intake**: `docs/agile/tasks/incoming/YYYY/MM/`
* **Doc drafts**: `docs/unique/drafts/YYYY/MM/`
* **Final docs**: `docs/unique/published/`
* **Caches**: `.cache/docops/**`
* **Pipelines config**: `pipelines.yml` (adds a `docops` pipeline)

---

## Front-matter (single source of truth)

Every file gets normalized front-matter; agents only touch these fields and the markdown body.

```yaml
---
title: ""
uuid: ""
description: ""
tags: [#docops, #incoming]
created_at: 2025-09-03T00:00:00Z
updated_at: 2025-09-03T00:00:00Z
docops:
  stage: incoming            # incoming|triaged|routed|refined|review|finalized
  type: unknown              # unknown|task|doc
  priority: null             # P0..P3 or null
  estimate: null             # story points or hours
  source_path: ""            # original location
  routed_path: ""            # current canonical location (if routed)
  content_sha256: ""         # content-addressed cache key
  history: []                # append-only breadcrumbs for agents
---
```

No code blocks for tags (so Obsidian graph works).

---

## New packages (flat, TS-only)

```
packages/
  docops-schema/
  docops-git/
  docops-llm/
  docops-classify-route/
  docops-refine/            # (stub – for later passes)
  docops-review/            # (stub – for review gates)
  docops-analytics/         # (stub – metrics)
```

Small, composable CLIs; everything caches to `.cache/docops/…`.

---

## Minimal working skeleton

### `packages/docops-schema/src/index.ts`

```ts
import matter from "gray-matter";
import { z } from "zod";

export const Stage = z.enum(["incoming","triaged","routed","refined","review","finalized"]);
export const Type = z.enum(["unknown","task","doc"]);

export const DocOpsMeta = z.object({
  stage: Stage,
  type: Type,
  priority: z.string().nullable().default(null),
  estimate: z.number().nullable().default(null),
  source_path: z.string().default(""),
  routed_path: z.string().default(""),
  content_sha256: z.string().default(""),
  history: z.array(z.record(z.string(), z.any())).default([]),
});

export const FrontMatter = z.object({
  title: z.string().default(""),
  uuid: z.string().default(""),
  description: z.string().default(""),
  tags: z.array(z.string()).default([]),
  created_at: z.string().default(new Date().toISOString()),
  updated_at: z.string().default(new Date().toISOString()),
  docops: DocOpsMeta.default({
    stage: "incoming",
    type: "unknown",
    priority: null,
    estimate: null,
    source_path: "",
    routed_path: "",
    content_sha256: "",
    history: [],
  }),
});

export type FrontMatterT = z.infer<typeof FrontMatter>;

export function parseMD(src: string) {
  const parsed = matter(src);
  const data = FrontMatter.parse(parsed.data ?? {});
  return { fm: data, body: parsed.content };
}

export function stringifyMD(fm: FrontMatterT, body: string) {
  return matter.stringify(body, fm as any);
}
```

### `packages/docops-git/src/index.ts`

```ts
import simpleGit from "simple-git";

export type CommitFooter = Record<string, string>;

export async function commitWithFooters(
  paths: string[],
  message: string,
  footers: CommitFooter
) {
  const git = simpleGit();
  await git.add(paths);
  const footerText = Object.entries(footers)
    .map(([k, v]) => `${k}: ${v}`)
    .join("\n");
  await git.commit(`${message}\n\n${footerText}\n`);
}
```

### `packages/docops-llm/src/index.ts`

```ts
import { createHash } from "node:crypto";
import { promises as fs } from "node:fs";

const OLLAMA_URL = process.env.OLLAMA_URL ?? "http://localhost:11434";
const MODEL = process.env.DOCOPS_MODEL ?? "qwen2.5:3b-instruct";

export function sha256(buf: Buffer | string) {
  const h = createHash("sha256"); h.update(buf); return h.digest("hex");
}

type ClassifyOut = {
  type: "task" | "doc";
  title: string;
  description: string;
  tags: string[];
  priority?: "P0" | "P1" | "P2" | "P3";
};

export async function classify(content: string): Promise<ClassifyOut> {
  // Cache by content hash
  const key = sha256(content);
  const cachePath = `.cache/docops/classify/${key}.json`;
  try {
    await fs.mkdir(".cache/docops/classify", { recursive: true });
    const hit = await fs.readFile(cachePath, "utf8").catch(() => null);
    if (hit) return JSON.parse(hit);

    // Very small, transparent prompt. Swap to SmartGPT Bridge if you prefer.
    const prompt = `You are a strict classifier. Decide if the markdown is a TASK or DOC.
Return JSON: { "type": "task|doc", "title": "...", "description": "...", "tags": ["#foo"], "priority": "P0|P1|P2|P3" (omit for docs) }
### CONTENT
${content.slice(0, 10000)}`;

    const res = await fetch(`${OLLAMA_URL}/api/generate`, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ model: MODEL, prompt, stream: false }),
    });
    const data = await res.json();
    const txt: string = data.response ?? "{}";
    // naive JSON extraction (defensive)
    const jsonStart = txt.indexOf("{");
    const jsonEnd = txt.lastIndexOf("}");
    const parsed = JSON.parse(txt.slice(jsonStart, jsonEnd + 1));
    const out: ClassifyOut = {
      type: parsed.type === "task" ? "task" : "doc",
      title: String(parsed.title ?? "").slice(0, 140),
      description: String(parsed.description ?? "").slice(0, 500),
      tags: Array.isArray(parsed.tags) ? parsed.tags.map(String) : [],
      priority: parsed.priority,
    };
    await fs.writeFile(cachePath, JSON.stringify(out, null, 2));
    return out;
  } catch (e) {
    // Safe fallback if LLM fails: assume doc with generic title
    return {
      type: "doc",
      title: "Untitled",
      description: "Auto-triaged content",
      tags: ["#docops", "#triage"],
    };
  }
}
```

### `packages/docops-classify-route/src/cli.ts`

```ts
#!/usr/bin/env -S node --enable-source-maps
import { globby } from "globby";
import * as path from "node:path";
import { promises as fs } from "node:fs";
import { fileURLToPath } from "node:url";
import { classify, sha256 } from "@promethean/docops-llm";
import { parseMD, stringifyMD, Stage } from "@promethean/docops-schema";
import { commitWithFooters } from "@promethean/docops-git";
import { randomUUID, createHash } from "node:crypto";

// --- helpers
const ROOT = process.cwd();
const INBOX = "docs/unique/incoming";
const TASKS_BASE = "docs/agile/tasks/incoming";
const DOCS_BASE = "docs/unique/drafts";

function slugify(s: string) {
  return s.toLowerCase()
    .replace(/[^a-z0-9]+/g, "-")
    .replace(/(^-|-$)/g, "").slice(0, 80);
}

async function ensureDir(p: string) {
  await fs.mkdir(p, { recursive: true });
}

function yyyymm(d = new Date()) {
  const y = d.getFullYear();
  const m = String(d.getMonth() + 1).padStart(2, "0");
  return `${y}/${m}`;
}

// --- main
async function main() {
  const inputs = await globby([`${INBOX}/**/*.md`, `${INBOX}/**/*.txt`], { dot: false });
  if (inputs.length === 0) return;

  const touched: string[] = [];
  for (const src of inputs) {
    const raw = await fs.readFile(src, "utf8");
    const { fm, body } = parseMD(raw);

    const contentHash = sha256(body);
    const already = fm.docops?.content_sha256 === contentHash && fm.docops?.stage !== "incoming";
    if (already) continue; // idempotent: skip files already processed for same content

    // seed FM
    fm.uuid ||= randomUUID();
    fm.docops.source_path ||= src;
    fm.docops.content_sha256 = contentHash;

    // classify (cached)
    const c = await classify(body);
    const nowIso = new Date().toISOString();

    // update FM from classifier
    fm.title = fm.title || c.title || path.basename(src).replace(/\.\w+$/, "");
    fm.description = fm.description || c.description || "";
    fm.tags = Array.from(new Set([...(fm.tags || []), ...(c.tags || [])]));
    fm.docops.type = c.type;
    fm.updated_at = nowIso;
    fm.docops.stage = "routed";

    // compute destination
    const slug = slugify(fm.title || "untitled");
    const destDir = c.type === "task"
      ? path.join(TASKS_BASE, yyyymm())
      : path.join(DOCS_BASE, yyyymm());
    await ensureDir(destDir);
    const dest = path.join(destDir, `${slug}--${fm.uuid}.md`);
    fm.docops.routed_path = dest;

    // write updated content to temp, then move atomically
    const next = stringifyMD(fm, body);
    // If dest already exists with same content, skip move (idempotent)
    let doMove = true;
    try {
      const existing = await fs.readFile(dest, "utf8");
      if (existing === next) doMove = false;
    } catch {}
    if (doMove) {
      await fs.writeFile(dest, next, "utf8");
    }

    // record breadcrumb
    fm.docops.history.push({
      at: nowIso,
      action: "classify-route",
      from: src,
      to: dest,
      type: c.type,
    });

    // remove original if different path
    if (src !== dest) {
      // Keep a tiny tombstone to preserve path history if you like:
      await fs.rm(src, { force: true });
    }

    // commit
    const footers = {
      "Doc-Ops-Stage": fm.docops.stage,
      "Doc-Ops-Type": fm.docops.type,
      "Doc-Ops-Hash": fm.docops.content_sha256,
      "Source-Path": fm.docops.source_path,
      "Dest-Path": fm.docops.routed_path,
      "Doc-Ops-Action": "classify-route",
    };
    await commitWithFooters(
      [dest],
      `docops: route ${path.basename(src)} → ${c.type}/${path.basename(dest)}`,
      footers
    );
    touched.push(dest);
  }

  if (touched.length) {
    console.log(JSON.stringify({ routed: touched }, null, 2));
  }
}

main().catch((e) => {
  console.error(e);
  process.exit(1);
});
```

#### Minimal `package.json` examples

`packages/docops-classify-route/package.json`

```json
{
  "name": "@promethean/docops-classify-route",
  "version": "0.1.0",
  "type": "module",
  "bin": { "docops-classify-route": "dist/cli.js" },
  "scripts": {
    "build": "tsc -p tsconfig.json",
    "dev": "tsx src/cli.ts"
  },
  "dependencies": {
    "@promethean/docops-schema": "workspace:*",
    "@promethean/docops-llm": "workspace:*",
    "@promethean/docops-git": "workspace:*",
    "globby": "^14.0.2",
    "gray-matter": "^4.0.3",
    "simple-git": "^3.27.0"
  },
  "devDependencies": {
    "tsx": "^4.16.2",
    "typescript": "^5.5.4"
  }
}
```

(Do the same pattern for `docops-schema`, `docops-llm`, `docops-git` with `main` pointing at `dist/index.js`.)

---

## Pipeline wiring (add to your `pipelines.yml`)

```yaml
pipelines:
  - name: docops
    steps:
      - id: docops-classify-route
        cwd: .
        shell: "pnpm --filter @promethean/docops-classify-route build && pnpm --filter @promethean/docops-classify-route docops-classify-route"
        inputs: ["docs/unique/incoming/**/*.{md,txt}"]
        outputs: [
          "docs/agile/tasks/incoming/**/**/*.md",
          "docs/unique/drafts/**/**/*.md",
          ".cache/docops/classify/**/*.json"
        ]
```

This step is **idempotent**: reruns won’t duplicate or churn files; cache hits short-circuit the LLM. Every move produces a commit with footers.

---

## Commit conventions (machine-readable footers)

* `Doc-Ops-Stage: routed|refined|review|finalized`
* `Doc-Ops-Type: task|doc`
* `Doc-Ops-Hash: <sha256>`
* `Source-Path: ...`
* `Dest-Path: ...`
* `Doc-Ops-Action: classify-route|refine|review|finalize`

Analytics can later scan `git log` for these to compute throughput & lead time.

---

## Next passes (stubs you can fill in later)

* `@promethean/docops-refine`

  * **task**: expand into subtasks with acceptance criteria; populate `estimate`, `priority`, add `#todo/#ready` tags.
  * **doc**: generate summary, outline, add backlinks and consistent headings.
* `@promethean/docops-review`

  * Create a review branch, open a PR, or write a local “review checklist” file; gate on policy (license headers present, front-matter valid, etc.). Commit footers carry reviewer sign-offs.
* `@promethean/docops-analytics`

  * CLI that scans commits & front-matter to emit daily/weekly reports (CSV/JSON). Hook into a dashboard or have it write Markdown summaries into `docs/reports/`.

All of those should follow the same shape: read file → check cache by `content_sha256` → produce deterministic transform → commit with footers.

---

## UX notes (push-style & hands-free)

* **Push to you**: a notifier (TTS or Discord/voice) subscribes to the `docops` pipeline output dir changes and reads diffs aloud with “approve/redo” voice commands → runs refine/review steps.
* **Emacs**: provide a tiny layer/command that lists “Doc-Ops queue” by scanning `docops.stage != finalized`. But because it’s just files, anyone can use any editor.

---

## Why this fits your constraints

* **TypeScript everywhere**; new packages under `packages/…`.
* **Idempotent**: content-hash caches + no side effects if nothing changed.
* **Immutable & traceable**: every step is a commit; footers tell the story.
* **Easy to expand**: adding a new agent = new package + one CLI step.
* **Markdown-only**: front-matter is the API; tools just honor it.
* **Cache everything**: `.cache/docops/**` is the single cache root.

If you want, I can follow up with the `docops-refine` (task/doc) and `docops-analytics` CLIs next, plus a tiny review-gate that enforces peer review before `finalized`.
