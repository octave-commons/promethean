# Codex Custom Instructions (Codex Cloud)

## What to read at startup

1) **Find the latest run report**
   - Read `docs/reports/codex_cloud/INDEX.md` to see past runs.
   - Treat `docs/reports/codex_cloud/latest/` as the current canonical run directory.
   - In that folder, read:
     - `INDEX.md` (links + human summary)
     - `summary.tsv` (tabular per-step status)
     - `summary.jsonl` (per-step JSON lines, if present)
     - `eslint.json` (machine-readable lint result)
     - `affected-graph.html` (Nx graph, optional)

2) **Record a baseline**
   - Capture the absolute path to `BASELINE_DIR = docs/reports/codex_cloud/latest`.
   - Extract:
     - `BASELINE_BUILD_RC` from `summary.tsv` (row matching `nx-affected-build` or `pnpm-build`)
     - `BASELINE_LINT_RC` from `summary.tsv` (row matching `nx-affected-lint`)
     - `BASELINE_TEST_RC` from `summary.tsv` (row matching `nx-affected-test`)
     - `BASELINE_ESLINT_SET` from `eslint.json` as the set of issues keyed by `file:line:column:ruleId`.
   - Keep this baseline for the entire session. Do **not** overwrite it when you generate new runs.

> If no artifacts exist yet, create a baseline by running the maintenance script once (see “Run reports”) and then continue.

---

## Run reports during the session

Use the scripts that wrap the `describe` runner. They **never bail early** and always write timestamped artifacts with a per-run `INDEX.md`.

* **Maintenance (fast prime + project checks):**

  ```bash
  bash run/codex_maintenance.sh
  ```

**Important:** When you want to evaluate your changes, run the **maintenance** script to create a **new run** after your edits. Let’s call its directory `CURRENT_DIR` (the newest entry in `docs/reports/codex_cloud/INDEX.md`, or `docs/reports/codex_cloud/latest` if you didn’t run anything else in between).

---

## Compare your results to the baseline

1. **Load current results** from `CURRENT_DIR`:

   * `CURRENT_BUILD_RC`, `CURRENT_LINT_RC`, `CURRENT_TEST_RC` from `summary.tsv`
   * `CURRENT_ESLINT_SET` from `eslint.json` keyed by `file:line:column:ruleId`

2. **Compute deltas**:

   * **New lint errors/warnings introduced by this session**
     `NEW_ESLINT = CURRENT_ESLINT_SET − BASELINE_ESLINT_SET`
   * **Resolved lint items**
     `FIXED_ESLINT = BASELINE_ESLINT_SET − CURRENT_ESLINT_SET`
   * **Build/Test status deltas**
     Compare `(BASELINE_BUILD_RC, BASELINE_TEST_RC)` vs `(CURRENT_BUILD_RC, CURRENT_TEST_RC)`.

3. **Gate conditions** (must be true before you call the task complete):

   * No **new** ESLint errors in `NEW_ESLINT`. (Warnings are allowed unless your task/issue requires otherwise; if warnings must fail, enforce ESLint with `--max-warnings=0`.)
   * All packages you touched pass:

     ```bash
     pnpm --filter @promethean/<packagename> build
     ```
   * No **new** test failures compared to baseline.
   * `pnpm install` runs without error at the end (this keeps future cold-starts healthy).

4. **Write a session diff note** (optional but encouraged):

   * Create `CURRENT_DIR/DIFF_FROM_<baseline-ts>.md` summarizing:

     * Counts: new vs fixed ESLint items
     * Build/Test status before → after
     * Direct links to relevant logs in `CURRENT_DIR/logs/*.log`

---

## Working practices

* Prefer **TypeScript**. New modules go in `packages/<package-name>`. Keep scripts **idempotent** and **cache-aware** (ESLint `--cache`, TSC `--tsBuildInfoFile`, Nx cache).
* For multi-package repos, prefer **Nx**:

  ```bash
  pnpm exec nx affected -t build,lint,test --parallel --output-style=stream \
    --base=origin/main --head=HEAD
  ```
* Over the course of your session, periodically run ESLint on the files you change and track **new** issues you’ve introduced versus the baseline.
* Use `gh` CLI to find or create an issue for the work. Reference that issue in your PR.

---

## End-of-session checklist

1. Re-run the **maintenance** script to generate a `CURRENT_DIR` with your final state.
2. Compare against `BASELINE_DIR` and ensure:

   * No **new** lint errors in `CURRENT_DIR/eslint.json`.
   * Touched packages pass their `build` target.
   * No **new** test failures.
3. Verify `pnpm install` completes without error.
4. Push your branch, open a PR, and link the run artifacts:

   * Link to `CURRENT_DIR/INDEX.md`
   * If present, link to `CURRENT_DIR/DIFF_FROM_<baseline-ts>.md`

---

## Quick commands (you may use these to compute diffs)

```bash
# Paths
BASELINE_DIR="$(readlink -f docs/reports/codex_cloud/latest)"
CURRENT_DIR="$(readlink -f docs/reports/codex_cloud/latest)"  # after your new maintenance run

# ESLint sets (file:line:column:ruleId)
to_keys() {
  jq -r '.[] | .messages[]? |
    select(.ruleId != null) |
    "\(.filePath):\(.line//0):\(.column//0):\(.ruleId)"' "$1" 2>/dev/null
}

# New issues introduced
comm -13 <(to_keys "$BASELINE_DIR/eslint.json" | sort -u) \
         <(to_keys "$CURRENT_DIR/eslint.json"  | sort -u) \
  > "$CURRENT_DIR/eslint_new.txt"

# Fixed issues
comm -23 <(to_keys "$BASELINE_DIR/eslint.json" | sort -u) \
         <(to_keys "$CURRENT_DIR/eslint.json"  | sort -u) \
  > "$CURRENT_DIR/eslint_fixed.txt"

# Quick statuses from summary.tsv
BUILD_BEFORE=$(awk -F'\t' '$2~/nx-affected-build|pnpm-build/ {rc=$3} END{print rc+0}' "$BASELINE_DIR/summary.tsv")
BUILD_AFTER=$( awk -F'\t' '$2~/nx-affected-build|pnpm-build/ {rc=$3} END{print rc+0}' "$CURRENT_DIR/summary.tsv")
TEST_BEFORE=$( awk -F'\t' '$2~/nx-affected-test/ {rc=$3} END{print rc+0}' "$BASELINE_DIR/summary.tsv")
TEST_AFTER=$(  awk -F'\t' '$2~/nx-affected-test/ {rc=$3} END{print rc+0}' "$CURRENT_DIR/summary.tsv")
```

---

## Visual (session → reports → compare)

```mermaid
flowchart LR
  A[Start Session] --> B[Read latest baseline\n(ESLint, summary)]
  B --> C[Make changes]
  C --> D[Run maintenance script\n→ new CURRENT_DIR]
  D --> E[Compute deltas\nESLint new/fixed,\nBuild/Test rc changes]
  E --> F{New problems?}
  F -- yes --> G[Fix or revert\nrepeat D/E]
  F -- no  --> H[Open PR\nlink CURRENT_DIR/INDEX.md]
```

