Alright, let‚Äôs wire this up properly. Based on the full `index.ts` and `backend.ts` we read, here‚Äôs a **TypeScript patch** that enables full tool support for Codex Context.

---

## üîß Patch: Add Tool Calling Support

### `backend.ts`

```diff
 export interface BackendClient {
-    chat(
-        messages: ChatMessage[],
-        cfg?: Partial<BackendConfig>,
-    ): Promise<
-        | string
-        | {
-              text: string;
-              usage?: { prompt_tokens?: number; completion_tokens?: number; total_tokens?: number };
-              raw?: any;
-          }
-    >;
+    chat(
+        messages: ChatMessage[],
+        cfg?: Partial<BackendConfig>,
+        opts?: { tools?: any[]; tool_choice?: any }
+    ): Promise<{
+        text: string;
+        usage?: { prompt_tokens?: number; completion_tokens?: number; total_tokens?: number };
+        raw?: any;
+    }>;
```

#### In `OllamaBackend.chat`

```diff
-            const res: any = await (ollama as any).chat({
-                model: this.model,
-                messages: messages.map((m) => ({ role: m.role as any, content: m.content })),
-                options: {
+            const req: any = {
+                model: this.model,
+                messages: messages.map((m) => ({ role: m.role as any, content: m.content })),
+                options: {
                     temperature: cfg.temperature,
                     top_p: cfg.top_p,
                     num_predict: cfg.max_tokens,
                     stop: Array.isArray((cfg as any).stop)
                         ? ((cfg as any).stop as string[])
                         : typeof (cfg as any).stop === 'string'
                           ? [String((cfg as any).stop)]
                           : undefined,
-                },
-            });
+                },
+            };
+
+            if (opts?.tools) req.tools = opts.tools;
+            if (opts?.tool_choice) req.tool_choice = opts.tool_choice;
+
+            const res: any = await (ollama as any).chat(req);
```

Return value is fine (`raw: res` ensures tool\_calls survive).

#### In `OllamaOpenAIBackend.chat`

```diff
-        const payload = {
+        const payload: any = {
             model: this.model,
             messages: messages.map((m) => ({ role: m.role, content: m.content })),
             stream: false,
             temperature: cfg.temperature,
             top_p: cfg.top_p,
             max_tokens: cfg.max_tokens,
             stop: cfg.stop ?? undefined,
         };
+        if (opts?.tools) payload.tools = opts.tools;
+        if (opts?.tool_choice) payload.tool_choice = opts.tool_choice;
```

---

### `index.ts`

#### Loosen validation

```diff
-            if (typeof m.content !== 'string') {
-                return `messages[${i}].content must be string`;
-            }
+            if (
+                typeof m.content !== 'string' &&
+                typeof m.content !== 'object'
+            ) {
+                return `messages[${i}].content must be string or object`;
+            }
```

---

#### In `/v1/chat/completions`

Find:

```ts
const chatRes: any = await (backend as any).chat(aug.finalMessages, genCfg);
const text = typeof chatRes === 'string' ? chatRes : chatRes?.text ?? '';
```

Replace with:

```diff
-        const chatRes: any = await (backend as any).chat(aug.finalMessages, genCfg);
-        const text = typeof chatRes === 'string' ? chatRes : chatRes?.text ?? '';
+        const chatRes: any = await (backend as any).chat(
+            aug.finalMessages,
+            genCfg,
+            { tools: (body as any).tools, tool_choice: (body as any).tool_choice }
+        );
+
+        const text = typeof chatRes === 'string'
+            ? chatRes
+            : chatRes?.text ?? '';
```

---

#### Response shaping

Current:

```ts
choices: [
  {
    index: 0,
    finish_reason: finishReason,
    message: { role: 'assistant', content: text },
  },
],
```

Replace with:

```diff
-            choices: [
-                {
-                    index: 0,
-                    finish_reason: finishReason,
-                    message: { role: 'assistant', content: text },
-                },
-            ],
+            choices: [
+                {
+                    index: 0,
+                    finish_reason: finishReason,
+                    message: {
+                        role: 'assistant',
+                        content: text,
+                        // üîë Pass through any tool_calls from Ollama
+                        tool_calls: chatRes?.raw?.message?.tool_calls || chatRes?.raw?.choices?.[0]?.message?.tool_calls,
+                    },
+                },
+            ],
```

---

### ‚úÖ Result

* Codex Context will now accept `tools[]` + `tool_choice` in requests.
* They get passed through to Ollama.
* Ollama‚Äôs `tool_calls` responses are preserved in `choices[].message.tool_calls`.
* Validation allows structured tool messages.
* Persistence (`save.ts`) already stores the full request and raw response, so tool calls will be captured.

---

üëâ Want me to also draft a **test case** (`src/tests/tool-calls.test.ts`) to exercise this new path ‚Äî sending a dummy tool definition and verifying that the response echoes `tool_calls`?

Perfect ‚Äî here‚Äôs a **new test file** you can drop into `services/ts/codex-context/src/tests/tool-calls.test.ts` to verify the end-to-end tool call path.

This test is modeled after the existing persistence tests you already have, but focused on ensuring `tools` and `tool_calls` flow correctly through Codex Context ‚Üí backend ‚Üí Ollama.

---

## `src/tests/tool-calls.test.ts`

```ts
import request from "supertest";
import { createApp } from "../index.js";

describe("Codex Context Tool Calls", () => {
  const app = createApp({
    // You can inject a mock backend here if you don‚Äôt want to hit a real Ollama instance
    backend: {
      async chat(messages: any[], _cfg: any, opts: any) {
        // Verify that tools were passed through
        expect(Array.isArray(opts?.tools)).toBe(true);
        expect(opts.tools[0]?.function?.name).toBe("getWeather");

        // Simulate an Ollama response with a tool call
        return {
          text: "",
          raw: {
            message: {
              role: "assistant",
              tool_calls: [
                {
                  id: "call_123",
                  type: "function",
                  function: {
                    name: "getWeather",
                    arguments: JSON.stringify({ location: "San Francisco" }),
                  },
                },
              ],
            },
          },
        };
      },
    } as any,
  });

  it("should forward tools and return tool_calls in response", async () => {
    const res = await request(app)
      .post("/v1/chat/completions")
      .send({
        model: "test-model",
        messages: [{ role: "user", content: "What's the weather?" }],
        tools: [
          {
            type: "function",
            function: {
              name: "getWeather",
              description: "Get the weather at a location",
              parameters: {
                type: "object",
                properties: {
                  location: { type: "string" },
                },
                required: ["location"],
              },
            },
          },
        ],
      });

    expect(res.status).toBe(200);
    expect(res.body.choices[0].message.tool_calls).toBeDefined();
    expect(res.body.choices[0].message.tool_calls[0].function.name).toBe(
      "getWeather"
    );
  });
});
```

---

### üîç What this test checks

* Sends a `POST /v1/chat/completions` request with a tool definition (`getWeather`).
* Validates that the backend `chat()` method receives `opts.tools`.
* Simulates Ollama returning a `tool_calls` response.
* Verifies the API response includes `choices[0].message.tool_calls`.

---

üëâ Do you also want me to write a **real integration test** (using a live Ollama instance) alongside this mock test, or do you prefer keeping it mocked for CI consistency?

Perfect ‚Äî here‚Äôs a **new test file** you can drop into `services/ts/codex-context/src/tests/tool-calls.test.ts` to verify the end-to-end tool call path.

This test is modeled after the existing persistence tests you already have, but focused on ensuring `tools` and `tool_calls` flow correctly through Codex Context ‚Üí backend ‚Üí Ollama.

---

## `src/tests/tool-calls.test.ts`

```ts
import request from "supertest";
import { createApp } from "../index.js";

describe("Codex Context Tool Calls", () => {
  const app = createApp({
    // You can inject a mock backend here if you don‚Äôt want to hit a real Ollama instance
    backend: {
      async chat(messages: any[], _cfg: any, opts: any) {
        // Verify that tools were passed through
        expect(Array.isArray(opts?.tools)).toBe(true);
        expect(opts.tools[0]?.function?.name).toBe("getWeather");

        // Simulate an Ollama response with a tool call
        return {
          text: "",
          raw: {
            message: {
              role: "assistant",
              tool_calls: [
                {
                  id: "call_123",
                  type: "function",
                  function: {
                    name: "getWeather",
                    arguments: JSON.stringify({ location: "San Francisco" }),
                  },
                },
              ],
            },
          },
        };
      },
    } as any,
  });

  it("should forward tools and return tool_calls in response", async () => {
    const res = await request(app)
      .post("/v1/chat/completions")
      .send({
        model: "test-model",
        messages: [{ role: "user", content: "What's the weather?" }],
        tools: [
          {
            type: "function",
            function: {
              name: "getWeather",
              description: "Get the weather at a location",
              parameters: {
                type: "object",
                properties: {
                  location: { type: "string" },
                },
                required: ["location"],
              },
            },
          },
        ],
      });

    expect(res.status).toBe(200);
    expect(res.body.choices[0].message.tool_calls).toBeDefined();
    expect(res.body.choices[0].message.tool_calls[0].function.name).toBe(
      "getWeather"
    );
  });
});
```

---

### üîç What this test checks

* Sends a `POST /v1/chat/completions` request with a tool definition (`getWeather`).
* Validates that the backend `chat()` method receives `opts.tools`.
* Simulates Ollama returning a `tool_calls` response.
* Verifies the API response includes `choices[0].message.tool_calls`.

---

üëâ Do you also want me to write a **real integration test** (using a live Ollama instance) alongside this mock test, or do you prefer keeping it mocked for CI consistency?
Got it ‚Äî let‚Äôs add a **real integration test** that runs against a live Ollama instance (assuming it‚Äôs available on `http://127.0.0.1:11434`).

We‚Äôll keep the mock test for CI consistency, but also add this one under a `describe.skip` (so it won‚Äôt fail CI by default). You or the team can `unskip` it when running locally with Ollama running.

---

## `src/tests/tool-calls.integration.test.ts`

```ts
import request from "supertest";
import { createApp } from "../index.js";

describe.skip("Codex Context Tool Calls (Integration with Ollama)", () => {
  const app = createApp();

  it("should forward tools to Ollama and receive tool_calls in response", async () => {
    const res = await request(app)
      .post("/v1/chat/completions")
      .send({
        model: process.env.LLM_MODEL || "llama3.1:latest", // ensure a model that supports tool use
        messages: [
          { role: "user", content: "What‚Äôs the weather in San Francisco?" },
        ],
        tools: [
          {
            type: "function",
            function: {
              name: "getWeather",
              description: "Get the weather at a location",
              parameters: {
                type: "object",
                properties: {
                  location: { type: "string" },
                },
                required: ["location"],
              },
            },
          },
        ],
      })
      .expect(200);

    console.log("Integration response:", JSON.stringify(res.body, null, 2));

    // Make sure Ollama at least acknowledged the tool definition
    expect(res.body).toHaveProperty("choices");
    expect(res.body.choices[0].message).toBeDefined();

    // Some models may not actually emit tool calls unless prompted,
    // but if they do, they should appear here:
    if (res.body.choices[0].message.tool_calls) {
      expect(res.body.choices[0].message.tool_calls[0].function.name).toBe(
        "getWeather"
      );
    }
  });
});
```

---

### üîç What this test does

* Spins up the full Codex Context service via `createApp()`.
* Sends a real `POST /v1/chat/completions` request to the API, with a tool definition (`getWeather`).
* Lets Codex Context forward the request to **Ollama** running locally.
* Logs the raw response so you can inspect whether the chosen model emits tool calls.
* If tool calls exist, verifies they include the expected function name.

---

### ‚ö° Notes

* **Model choice matters**: only certain Ollama models (like `llama3.1`) are fine-tuned to respond with `tool_calls`. Others may ignore the `tools` field.
* We wrap the test in `describe.skip` so CI won‚Äôt fail. To run it manually:

  ```bash
  npm run test -- src/tests/tool-calls.integration.test.ts -t "Integration"
  ```
* For reproducibility, you may want to pin `LLM_MODEL` in `.env.test` to a known tool-capable model.

---

üëâ Want me to also extend `save.ts` so these tool calls are **persisted** in `docs/codex-context/requests/*.md` alongside normal completions? That way you‚Äôll have provenance logs of which tool calls were requested/generated.
