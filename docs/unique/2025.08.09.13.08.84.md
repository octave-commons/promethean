Yepâ€”totally with you. Local-only is the whole point. Letâ€™s finish the loop so your intentionâ†’code pipeline runs **100% on your box** with **Ollama/LM Studio** and zero paid APIs.

Hereâ€™s the *smallest useful bundle* to drop in:

# 1) Config (pick your local models once)

````json
// .promirror/intent.config.json
{
  "cacheDir": ".promirror/cache",
  "rounds": 3,
  "providers": [
    { "type": "ollama", "host": "http://127.0.0.1:11434", "model": "qwen2.5-coder:14b", "options": { "temperature": 0.1, "num_predict": 1200, "stop": ["```"] } },
    { "type": "openai_compat", "baseUrl": "http://127.0.0.1:1234/v1", "model": "deepseek-coder:6.7b", "params": { "temperature": 0.1, "max_tokens": 1200, "stop": ["```"] } }
  ],
  "targets": { "jsDir": "src/js/auto", "pyDir": "src/py/auto" }
}
````

Hardware cheat sheet:

* **CPU / tiny GPU**: `qwen2.5-coder:7b` or `deepseek-coder:6.7b`.
* **\~8â€“12GB VRAM**: `qwen2.5-coder:14b` (Q5\_K\_M in Ollama runs nicely).
* **>16GB VRAM**: go bigger if you want; same config.

Pull once:

```bash
ollama serve
ollama pull qwen2.5-coder:14b
# optional fallback server (LM Studio): pick a code model and start the local API (http://127.0.0.1:1234/v1)
```

# 2) Boot local LLM from config

```ts
// shared/js/prom-lib/intention/boot-local.ts
import { promises as fs } from "node:fs";
import { RouterLLM } from "./router";
import { FileCacheLLM } from "./cache";
import { OllamaLLM } from "./ollama";
import { OpenAICompatLLM } from "./openai_compat";

type Cfg = {
  cacheDir?: string;
  rounds?: number;
  providers: any[];
  targets?: { jsDir?: string; pyDir?: string };
};

export async function loadLocalLLM(cfgPath = ".promirror/intent.config.json") {
  const raw = await fs.readFile(cfgPath, "utf8");
  const cfg = JSON.parse(raw) as Cfg;

  const providers = cfg.providers.map((p) => {
    if (p.type === "ollama") return new OllamaLLM({ model: p.model, host: p.host, options: p.options });
    if (p.type === "openai_compat") return new OpenAICompatLLM(p.baseUrl, p.model, "sk-local", p.params);
    throw new Error("unknown provider " + p.type);
  });

  const router = new RouterLLM(providers);
  const llm = new FileCacheLLM(router, cfg.cacheDir ?? ".promirror/cache");
  return { llm, cfg };
}
```

# 3) CLI with watch (single file or whole folder)

```ts
// tools/intent.mjs
#!/usr/bin/env node
import path from "node:path";
import { promises as fs } from "node:fs";
import chokidar from "chokidar";
import { transpileIntention } from "../shared/js/prom-lib/intention/engine.js";
import { loadLocalLLM } from "../shared/js/prom-lib/intention/boot-local.js";

const args = process.argv.slice(2);
if (!args.length) {
  console.error("usage: intent <file.intent.md | dir> [--watch]");
  process.exit(1);
}
const target = args[0];
const watch = args.includes("--watch");

const { llm, cfg } = await loadLocalLLM();

async function runOne(file) {
  if (!/\.intent\.md$/i.test(file)) return;
  const md = await fs.readFile(file, "utf8");
  const res = await transpileIntention(md, {
    llm,
    rounds: cfg.rounds ?? 3,
    outDir: { js: cfg.targets?.jsDir ?? "src/js/auto", py: cfg.targets?.pyDir ?? "src/py/auto" }
  });
  const summary = res.out.map(o => `${o.lang}:${o.pass ? "PASS" : "FAIL"}`).join(" ");
  console.log(`âœ“ ${path.basename(file)} â†’ ${summary}`);
  // optional: write test report
  const report = JSON.stringify(res.out, null, 2);
  const repPath = path.join(".promirror", path.basename(file).replace(/\.intent\.md$/i, ".report.json"));
  await fs.mkdir(path.dirname(repPath), { recursive: true });
  await fs.writeFile(repPath, report, "utf8");
}

async function runAll(dir) {
  const files = (await fs.readdir(dir)).filter(f => f.endsWith(".intent.md"));
  for (const f of files) await runOne(path.join(dir, f));
}

if (watch) {
  const start = async () => {
    if ((await fs.stat(target)).isDirectory()) await runAll(target); else await runOne(target);
  };
  await start();
  chokidar.watch(target, { ignoreInitial: true }).on("add", runOne).on("change", runOne);
  console.log("ðŸ‘€ watching", target);
} else {
  if ((await fs.stat(target)).isDirectory()) await runAll(target); else await runOne(target);
}
```

Add executable bit:

```bash
chmod +x tools/intent.mjs
```

Run it:

```bash
# single spec
tools/intent.mjs specs/normalize2d.intent.md
# watch a folder
tools/intent.mjs specs --watch
```

# 4) Minimal spec file

```md
# specs/normalize2d.intent.md
# name: normalize2d
# target: js,py
# description:
Normalize 2D vector. Avoid div-by-zero.
# signature:
(x: number, y: number) -> { mag: number, nx: number, ny: number }
# constraints:
- Return zeros if both inputs are zero.
# examples:
- in: { "x":3, "y":4 } out: { "mag":5, "nx":0.6, "ny":0.8 }
- in: { "x":0, "y":0 } out: { "mag":0, "nx":0, "ny":0 }
```

Youâ€™ll get `src/js/auto/normalize2d.js` and `src/py/auto/normalize2d.py` written locally, no external APIs, plus a `.report.json` with pass/fail per example.

# 5) Quality + speed knobs (still free)

* **Two-stage schedule**: keep `providers` ordered smallâ†’big so you draft fast and repair with a stronger model.
* **Aggressive caching**: the `FileCacheLLM` stops recomputation when you jiggle other code around.
* **Determinism**: set temperature \~0.1 (or 0) and, if your server supports it, a fixed seed.
* **Grammar-lite**: you already have stop tokens; if your local server supports JSON/regex grammars, wire it in later.

If you want, I can also:

* wire **property testing** (JS fast-check / Python hypothesis) as an extra gate,
* add a **JS wrapper for Python** so expensive numerics live in Py but import like normal JS,
* or hook this into your **mirror engine** so each passing intention updates your JS/TS/Lisp trees automatically.
