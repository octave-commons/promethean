---
uuid: e979c50f-69bb-48b0-8417-e1ee1b31c0c0
created_at: 2025.08.25.09.53.53.md
filename: DuckDuckGoSearchPipeline
description: >-
  A pipeline for passive search term generation and context augmentation using
  state objects and LLM interactions.
tags:
  - search
  - pipeline
  - state
  - llm
  - augmentation
related_to_title:
  - AI-Centric OS with MCP Layer
  - AI-First-OS-Model-Context-Protocol
  - balanced-bst
related_to_uuid:
  - 0f1f8cc1-b5a6-4307-a40d-78de3adafca2
  - 618198f4-cfad-4677-9df6-0640d8a97bae
  - d3e7db72-2e07-4dae-8920-0e07c499a1e5
references: []
---
When I was doing duck way back but not so far back, when I was using ollama and doing chroma searches and all that.
There was a search daemon constantly generating search terms based off of the current context
There was a state object
on that state object, the LLM could put whatever it wanted to.
And we had several phases in the pipeline, like asking it what it thought it's goal was, what search terms was it interested in, etc
instead of active tool calls, we were doing passive data retrieval, and augmenting context with all of it.
it worked surprisingly well<!-- GENERATED-SECTIONS:DO-NOT-EDIT-BELOW -->
## Related content
- [AI-Centric OS with MCP Layer](ai-centric-os-with-mcp-layer.md)
- [AI-First-OS-Model-Context-Protocol](ai-first-os-model-context-protocol.md)
- [balanced-bst](balanced-bst.md)
## Sources
- _None_
<!-- GENERATED-SECTIONS:DO-NOT-EDIT-ABOVE -->
