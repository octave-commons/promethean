Yep—that’s the right instinct. Treat **Git as optional plumbing**, not a hard dependency. Build an OS-level versioning layer that’s:

* **Local-first** (works without remotes),
* **Content-addressed** (dedup & integrity),
* **Encrypted-at-rest + policy-gated exports** (so “what goes upstream” is explicit),
* **Git-bridgeable** (when you *do* want PRs, it can synthesize a branch/patch).

Here’s a compact architecture you can ship in TS without dragging Git everywhere.

# Core idea: VCS-lite + policies

* **CAS (content-addressed store):** chunks and trees by hash (blobs, trees, snapshots).
* **Snapshots:** point-in-time refs with metadata (who/why/tools).
* **Journal:** append-only events (create, modify, pipeline-apply, tag).
* **Policies:** decide what is allowed to **export** (paths/tags/redactions).
* **Remotes:** pluggable backends (local dir, S3-ish, SSH/rsync, Git bridge).
* **Secrets:** abstracted provider (env, keyring, age/sops) so remotes don’t leak.

Git becomes *one* remote, not the source of truth.

---

# Minimal layout

```
~/.prom/
  store/
    objects/ab/cdef...         # CAS blobs
    trees/12/3456...           # CAS trees (Merkle dir listings)
    snaps/2025-09-13T...json   # snapshot metadata
  journals/<space>.jsonl       # append-only logs per “space” (repo, dir, app)
  policies/<space>.json        # export/backup policies
  remotes/<name>.json          # remote definitions (type, url, caps)
```

A *space* is any versioned root: a project, a user dir, an app’s state folder.

---

# Modes

* **No-Git mode (default):** local snapshots + optional encrypted backups.
* **Git-bridge mode:** synthesize a working tree for selected files and emit a **patch/branch** (no secrets needed for local branch creation).
* **Hybrid:** keep CAS as source of truth; also commit to Git when available.

---

# Secrets & safety

* **Key mgmt:** generate an **age** keypair per machine or space; store public in policy, keep private in OS keyring or `pass/gnome-keyring` abstraction.
* **Export policy:** whitelist paths/tags; allow **redaction transforms** on export.
* **Zero implicit push:** every remote action requires a named **Export Profile**.

---

# Flow (OS-tool perspective)

```mermaid
flowchart LR
  W[Working Dir / App Data] -->|snapshot()| S[Snapshot Builder]
  S --> C[CAS]
  S --> J[Journal Append]
  C --> R[Remote Sync (optional)]
  P[Policy Engine] --> R
  subgraph Optional Git Bridge
    C --> G[Materializer]
    G --> D[.patch / temp branch]
  end
```

---

# TypeScript skeletons (idempotent, cache-friendly)

**packages/versioning/src/types.ts**

```ts
export type Hash = `sha256-${string}`;

export interface BlobRef { type: "blob"; hash: Hash; size: number; }
export interface TreeEntry { path: string; mode: "file"|"dir"|"symlink"; ref: BlobRef|TreeRef; }
export interface TreeRef { type: "tree"; hash: Hash; entries: number; }
export interface Snapshot {
  id: string;           // ISO ts or ULID
  root: TreeRef;
  parents: string[];
  meta: { space: string; author?: string; message?: string; tags?: string[]; tool?: string; };
}

export interface Policy {
  allow: string[];            // globs allowed for export
  deny?: string[];            // globs denied
  redact?: { pattern: string; replace: string; }[]; // inline transforms
  encrypt?: boolean;          // encrypt blob payloads for remotes
}

export interface Remote {
  name: string;
  type: "local"|"s3"|"ssh"|"git";
  url: string;                // path, s3://bucket/prefix, ssh://host:/path, git url
  options?: Record<string, unknown>; // region, branch, etc.
  policyRef: string;          // policies/<space>.json key
}
```

**packages/versioning/src/cas.ts**

```ts
import fs from "node:fs/promises";
import { createHash } from "node:crypto";
import { join, dirname } from "node:path";
import type { Hash } from "./types.js";

export class CAS {
  constructor(private root: string) {}
  private shard(hash: string) { return join(this.root, "objects", hash.slice(7,9), hash.slice(9)); }
  async put(data: Buffer): Promise<Hash> {
    const h = `sha256-${createHash("sha256").update(data).digest("hex")}` as Hash;
    const p = this.shard(h);
    await fs.mkdir(dirname(p), { recursive: true });
    try { await fs.stat(p); } catch { await fs.writeFile(p, data); }
    return h;
  }
  async get(hash: Hash): Promise<Buffer> {
    return fs.readFile(this.shard(hash));
  }
}
```

**packages/versioning/src/snapshot.ts**

```ts
import { promises as fs } from "node:fs";
import { join, resolve } from "node:path";
import { CAS } from "./cas.js";
import type { BlobRef, TreeRef, Snapshot } from "./types.js";

export class Snapshotter {
  constructor(private storeRoot: string, private cas: CAS) {}
  async snapshotDir(space: string, dir: string): Promise<Snapshot> {
    const rootTree = await this.buildTree(resolve(dir));
    const snap: Snapshot = {
      id: new Date().toISOString(),
      root: rootTree,
      parents: await this.lastSnapshotIds(space, 1),
      meta: { space, message: "autosnap" }
    };
    const p = join(this.storeRoot, "snaps", `${snap.id}.json`);
    await fs.mkdir(join(this.storeRoot, "snaps"), { recursive: true });
    await fs.writeFile(p, JSON.stringify(snap, null, 2));
    await fs.appendFile(join(this.storeRoot, "journals", `${space}.jsonl`),
      JSON.stringify({ ts: snap.id, event: "snapshot", space, root: rootTree.hash }) + "\n");
    return snap;
  }

  private async buildTree(dir: string): Promise<TreeRef> {
    const entries = await fs.readdir(dir, { withFileTypes: true });
    const treeEntries = [];
    for (const e of entries) {
      if (e.name === ".prom") continue; // ignore internal
      const full = join(dir, e.name);
      if (e.isDirectory()) {
        const sub = await this.buildTree(full);
        treeEntries.push({ path: e.name, mode: "dir", ref: sub });
      } else if (e.isFile()) {
        const data = await fs.readFile(full);
        const blobHash = await this.cas.put(data);
        treeEntries.push({ path: e.name, mode: "file", ref: { type: "blob", hash: blobHash, size: data.byteLength } as BlobRef });
      }
    }
    const treeData = Buffer.from(JSON.stringify(treeEntries));
    const treeHash = await this.cas.put(treeData);
    return { type: "tree", hash: treeHash, entries: treeEntries.length };
  }

  private async lastSnapshotIds(space: string, n: number): Promise<string[]> {
    try {
      const j = await fs.readFile(join(this.storeRoot, "journals", `${space}.jsonl`), "utf8");
      const lines = j.trim().split("\n").reverse().filter(Boolean)
        .map(l => JSON.parse(l)).filter((e: any) => e.event === "snapshot").slice(0, n);
      return lines.map(e => e.ts);
    } catch { return []; }
  }
}
```

**packages/versioning/src/export.ts**

```ts
import { promises as fs } from "node:fs";
import { join } from "node:path";
import type { Policy, Snapshot, TreeRef, BlobRef } from "./types.js";
import { CAS } from "./cas.js";
import picomatch from "picomatch";

export class Exporter {
  constructor(private cas: CAS) {}
  async materialize(root: string, snap: Snapshot, policy: Policy, outDir: string) {
    const allow = picomatch(policy.allow, { dot: true });
    const deny = policy.deny ? picomatch(policy.deny, { dot: true }) : () => false;
    await this.walkTree(root, "", snap.root, allow, deny, policy, outDir);
  }
  private async walkTree(base: string, rel: string, tree: TreeRef, allow: any, deny: any, policy: Policy, outDir: string) {
    const entries = JSON.parse((await this.cas.get(tree.hash)).toString());
    for (const ent of entries) {
      const p = rel ? `${rel}/${ent.path}` : ent.path;
      if (!allow(p) || deny(p)) continue;
      if (ent.mode === "dir") {
        await this.walkTree(base, p, ent.ref, allow, deny, policy, outDir);
      } else {
        let data = await this.cas.get((ent.ref as BlobRef).hash);
        if (policy.redact) {
          let txt = data.toString("utf8");
          for (const r of policy.redact) { txt = txt.replace(new RegExp(r.pattern, "g"), r.replace); }
          data = Buffer.from(txt);
        }
        const dest = join(outDir, p);
        await fs.mkdir(join(dest, ".."), { recursive: true } as any);
        await fs.writeFile(dest, data);
      }
    }
  }
}
```

This gives you:

* **Snapshotter.snapshotDir(space, path)** → local snapshot & journal.
* **Exporter.materialize(snap, policy)** → safe export tree (to share or to Git).

You can then add:

* **Remote drivers**:

  * `local`: copy exported tree into a backup dir.
  * `s3`: put blobs/trees under `bucket/prefix/…` (encrypt first if `policy.encrypt`).
  * `ssh`: rsync exported tree to a server.
  * `git`: `git init` a temp dir, write exported tree, `git commit`, optionally create a patch/branch.

* **Secrets provider** interface to fetch creds/keys without leaking them into logs.

---

# How this pairs with your pipelines

* Pipelines **write artifacts** (as you already wanted) → you **snapshot artifacts** too.
* Applying a pipeline result to working files is a **materialization** + **snapshot** event, not an in-place mutation you can’t undo.
* Your UI can show: *Snapshots*, *Runs*, and *Exports* as distinct timelines.

---

# Minimal CLI you can add

```
prom snap --space promethean --path ./devel/promethean
prom diff --space promethean --from <snapA> --to <snapB>
prom export --space promethean --snap <id> --policy policies/promethean.json --out /tmp/share
prom push --remote s3-prod --snap <id>   # applies policy + encrypts
prom git-bridge --snap <id> --branch chore/pipeline-update
```

Every command is **idempotent** and safe offline.

---

# Why this fits your OS vision

* Works for **any** folder (code, app state, config) with the same guarantees.
* You can layer it directly into your **file explorer UI** (left: files, right: snapshot timeline, bottom: pipeline runs).
* Export is **policy-driven**, so “multiple gits” becomes just different *remotes with different policies*.
* No secrets required to *use* versioning; secrets are only touched at export time.

If you want, I can turn these skeletons into a new package `packages/@promethean/versioning` and wire a tiny CLI so your current IDE-like UI can “Snapshot → Review → Export → (optional) Git bridge” without ever risking a leak or requiring Git to exist.
