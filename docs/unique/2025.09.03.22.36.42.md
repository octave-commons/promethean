---
uuid: 46e6b485-1c74-46d4-833d-8a2f98b3570d
created_at: '2025-09-03T22:36:42Z'
filename: Optimization Improvements for AI Systems
title: Optimization Improvements for AI Systems
description: >-
  This document outlines key improvements to enhance AI system performance,
  including better parallelization of Ollama batching, implementing a function
  memoizer with configurable drivers, and optimizing prompts for local AI. It
  also covers setting up a kitchen sink compose file with vLLM and OpenV. The
  goal is to reduce dead GPU and CPU time while improving overall efficiency.
tags:
  - Optimization
  - Parallelization
  - BatchProcessing
  - Memoizer
  - PromptEngineering
  - vLLM
  - OpenVINO
---
# Todo

- Move Level Cache into Persistence:

- Better Parallelize Ollama Batching:
  - We have to wait for batch processing to happen right now for a batch to start
  - then we have to wait for a batch to finish before we can start preprocessing the next batch
  - This leads to dead gpu time, and dead cpu time.

- Function Memoizer with Configurable Drivers:

- Better Prompts for Local AI:

- Additional Improvements:

- Set up kitchen sink compose file
  - Has vLLM for optimized batched LLM queries
  - Has OpenVINO GenAI model server (NPU)

[[vLLM]] [[OpenVINO]]

#AI #Optimization #Parallelization #BatchProcessing #Memoizer #PromptEngineering
