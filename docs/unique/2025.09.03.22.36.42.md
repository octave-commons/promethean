---
uuid: c953338c-d636-492a-84df-07ad9f5ca4c8
created_at: 2025.09.03.22.36.42.md
filename: Optimizing AI Processing Pipeline
description: >-
  This document outlines key improvements to enhance the efficiency of AI
  processing, including better parallelization of Ollama batching, implementing
  a function memoizer with configurable drivers, and optimizing prompts for
  local AI models. It also covers setting up a comprehensive compose file with
  vLLM and OpenVINO for optimized batched queries and NPU-accelerated model
  serving.
tags:
  - AI
  - Optimization
  - Parallelization
  - vLLM
  - OpenVINO
  - NPU
  - Batch Processing
  - Memoizer
  - Prompt Engineering
---
# Todo

- Move Level Cache into Persistence:

- Better Parallelize Ollama Batching:
  - We have to wait for batch processing to happen right now for a batch to start
  - then we have to wait for a batch to finish before we can start preprocessing the next batch
  - This leads to dead gpu time, and dead cpu time.

- Function Memoizer with Configurable Drivers:

- Better Prompts for Local AI:

- Additional Improvements:

- Set up kitchen sink compose file
  - Has vLLM for optimized batched LLM queries
  - Has OpenVINO GenAI model server (NPU)

[[vLLM]] [[OpenVINO]]

#AI #Optimization #Parallelization #BatchProcessing #Memoizer #PromptEngineering
