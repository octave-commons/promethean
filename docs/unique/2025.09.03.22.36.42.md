---
uuid: 46e6b485-1c74-46d4-833d-8a2f98b3570d
created_at: '2025-09-03T22:36:42Z'
filename: Optimization Improvements for AI Systems
title: Optimization Improvements for AI Systems
description: >-
  This document outlines key improvements to enhance AI system performance,
  including better parallelization of Ollama batching, implementing a function
  memoizer with configurable drivers, and optimizing prompts for local AI. It
  also covers setting up a kitchen sink compose file with vLLM and OpenV. The
  goal is to reduce dead GPU and CPU time while improving overall efficiency.
tags:
  - Optimization
  - Parallelization
  - BatchProcessing
  - Memoizer
  - PromptEngineering
  - vLLM
  - OpenVINO
related_to_uuid:
  - c953338c-d636-492a-84df-07ad9f5ca4c8
  - 7a83075b-6b0e-4064-b97e-2606b0d8a35a
  - 46b3c583-a4e2-4ecc-90de-6fd104da23db
related_to_title:
  - Optimizing AI Processing Pipeline
  - Document Processing Improvements
  - Promethean Event Bus MVP
references:
  - uuid: 7a83075b-6b0e-4064-b97e-2606b0d8a35a
    line: 21
    col: 0
    score: 1
  - uuid: 46b3c583-a4e2-4ecc-90de-6fd104da23db
    line: 973
    col: 0
    score: 1
  - uuid: c953338c-d636-492a-84df-07ad9f5ca4c8
    line: 22
    col: 0
    score: 1
  - uuid: 7a83075b-6b0e-4064-b97e-2606b0d8a35a
    line: 12
    col: 0
    score: 0.93
  - uuid: 7a83075b-6b0e-4064-b97e-2606b0d8a35a
    line: 3
    col: 0
    score: 0.89
  - uuid: 7a83075b-6b0e-4064-b97e-2606b0d8a35a
    line: 8
    col: 0
    score: 0.88
  - uuid: 7a83075b-6b0e-4064-b97e-2606b0d8a35a
    line: 16
    col: 0
    score: 0.88
---
# Todo

- Move Level Cache into Persistence:

- Better Parallelize Ollama Batching:
  - We have to wait for batch processing to happen right now for a batch to start
  - then we have to wait for a batch to finish before we can start preprocessing the next batch
  - This leads to dead gpu time, and dead cpu time.

- Function Memoizer with Configurable Drivers:

- Better Prompts for Local AI:

- Additional Improvements:

- Set up kitchen sink compose file
  - Has vLLM for optimized batched LLM queries
  - Has OpenVINO GenAI model server (NPU)

[[vLLM]] [[OpenVINO]]

#AI #Optimization #Parallelization #BatchProcessing #Memoizer #PromptEngineering
er with Configurable Drivers:

- Better Prompts for Local AI:

- Additional Improvements:

- Set up kitchen sink compose file
  - Has vLLM for optimized batched LLM queries
  - Has OpenVINO GenAI model server (NPU)

[[vLLM]] [[OpenVINO]]

#AI #Optimization #Parallelization #BatchProcessing #Memoizer #PromptEngineering
