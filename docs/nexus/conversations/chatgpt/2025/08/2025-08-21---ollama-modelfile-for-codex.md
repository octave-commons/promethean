---
nexus: nexus-ai-chat-importer
plugin_version: "1.2.0"
provider: chatgpt
aliases: "Ollama modelfile for Codex"
conversation_id: 68a71e81-f514-832c-8a19-a7e6d8c46d5a
create_time: 08/21/2025 at 8:26 AM
update_time: 08/21/2025 at 10:56 AM
---

# Title: Ollama modelfile for Codex

Created: 08/21/2025 at 8:26 AM
Last Updated: 08/21/2025 at 10:56 AM
Chat URL: https://chat.openai.com/c/68a71e81-f514-832c-8a19-a7e6d8c46d5a


>[!nexus_user] **User** - 08/21/2025 at 8:26 AM
> We need to work on an ollama modelfile/prompt for running codex OSS
<!-- UID: f6c4692d-0a8d-4391-95d5-da697a7d755e -->




>[!nexus_agent] **Assistant** - 08/21/2025 at 8:28 AM
> I spun up a fresh canvas with a readyâ€‘toâ€‘use Ollama **Modelfile**, a production **system prompt**, two **template variants** (Qwen + Llamaâ€‘3 style), and a **JSON action schema** your Codex runner can execute. Want me to tailor it to a specific local model youâ€™ve got pulled (e.g., `qwen3`, `llama3.1`, `codegemma`) or wire in extra actions for your broker/heartbeat flows?
<!-- UID: 62b8b48c-19f2-44d8-aa57-2aaa2ede226b -->

---




>[!nexus_user] **User** - 08/21/2025 at 8:32 AM
> Nah that is a bit better for the codex lite we built, but I mean like, to actually use with the existing codex cli. It has to be aware of the agent files. It needs to be able to benefit from the documentation we've been writing. It needs to be convinced to at least execute reads. 
> 
> Do we know what codexes system prompts and tool calls actually look like?
<!-- UID: 06656990-2f2a-42d0-a477-9728a265259d -->




>[!nexus_agent] **Assistant** - 08/21/2025 at 8:32 AM
> Short answer: we donâ€™t â€œknowâ€ for sure until we look at your codexâ€‘cli source. Codex isnâ€™t a standard with a public spec; each CLI glues a system prompt + tool schema together in its own way. So the quickest path is to **instrument your CLI to print the exact system prompt, tool schema, and final messages it sends to the model.** Then we can build an Ollama Modelfile/prompt that mirrors it.
> 
> Hereâ€™s a tight plan you can run right now.
> 
> # 1) Locate Codexâ€™s system prompt & tools
> Use ripgrep from the codexâ€‘cli root:
> 
> ```
> rg -n --hidden --glob '!node_modules' \
>  -e 'systemPrompt|SYSTEM_PROMPT|baseSystem|composeSystem' \
>  -e 'toolsSchema|toolSchema|function.*schema|tool.*registry' \
>  -e 'messages.*build|compose.*prompt|render.*prompt'
> ```
> 
> Common places to check:
> - `src/prompt.ts`, `src/composePrompt.ts`, `src/runner/*.ts`
> - `src/tools/*.ts` (tool registry / validators)
> - `src/agents/*.ts` or `AGENTS.md` loader
> 
> # 2) Add a oneâ€‘flag â€œshow me the promptâ€ tap
> Patch your CLI to support `--dump-prompt` (and always enable for a test run). This prints the **final** payload that goes to Ollama.
> 
> Example patch (adapt paths as needed):
> 
> ```diff
> diff --git a/src/runner/run.ts b/src/runner/run.ts
> @@
> - export async function run(opts: RunOptions) {
> + export async function run(opts: RunOptions) {
> + const dump = !!process.env.CODEX_DUMP_PROMPT || opts.dumpPrompt;
>  const payload = await composePrompt(opts);
> + if (dump) {
> + const fs = await import('node:fs/promises');
> + const outDir = '.codex-debug';
> + await fs.mkdir(outDir, { recursive: true });
> + await fs.writeFile(`${outDir}/system.txt`, payload.system ?? '', 'utf8');
> + await fs.writeFile(`${outDir}/tools.json`, JSON.stringify(payload.tools ?? [], null, 2), 'utf8');
> + await fs.writeFile(`${outDir}/messages.json`, JSON.stringify(payload.messages ?? [], null, 2), 'utf8');
> + console.error('[codex] prompt dumped to ./.codex-debug/');
> + }
>  return sendToModel(payload, opts);
>  }
> ```
> 
> And wire the flag in your CLI entry:
> 
> ```diff
> diff --git a/src/cli.ts b/src/cli.ts
> @@
> - program.option('--model <name>', 'model name');
> + program.option('--model <name>', 'model name');
> + program.option('--dump-prompt', 'dump composed prompt to .codex-debug', false);
> ```
> 
> Now run any codex command with `--dump-prompt` (or env `CODEX_DUMP_PROMPT=1`) and youâ€™ll have:
> - `.codex-debug/system.txt` â† the **system prompt**
> - `.codex-debug/tools.json` â† tool/function **schema/registry**
> - `.codex-debug/messages.json` â† user/assistant history **exactly** as sent
> 
> # 3) Make the model â€œawareâ€ of agent files & docs
> Codex already has the loader for â€œagent filesâ€ (you mentioned it); the trick is guaranteeing theyâ€™re **in the prompt**. In your `composePrompt` step, inject a **Docs Context block** that concatenates targeted docs (readâ€‘only) into a single message (or a short RAG summary if too big):
> 
> - `AGENTS.md`
> - `docs/agile/process.md`
> - `docs/` (topâ€‘level READMEs)
> - `services/**/README.md`
> - `shared/ts/` public API notes
> - Any â€œpermission/second circuitâ€ docs you want Codex to respect
> 
> If your CLI already gathers files, add a **filter** and a **budget**:
> 
> ```ts
> const docGlobs = [
>  'AGENTS.md',
>  'docs/**/*.md',
>  'services/**/README.md',
>  'shared/ts/**/README.md',
> ];
> 
> const maxChars = 20000; // keep under ctx
> const docs = await loadFiles(docGlobs);
> const squeezed = summarizeOrClip(docs, maxChars); // simple chunk->summary->concat
> messages.unshift({
>  role: 'system',
>  content: `# Project Docs (read-only)\n${squeezed}`
> });
> ```
> 
> This guarantees the model â€œbenefits from the documentation youâ€™ve been writingâ€ without inventing new tools.
> 
> # 4) Convince it to execute **reads** (not writes) via tools
> You want it to **use tools for repo reads**. If your tool layer already exposes readâ€‘only ops, label them clearly and **place them first** in the tool registry so the model selects them:
> 
> - `repo.readFile(path)`
> - `repo.search(query)`
> - `fs.stat(path)`
> - `broker.inspect(topic)` (if applicable, readâ€‘only)
> - `gql.query(...)` (if you have internal schemas)
> 
> If tools are JSONâ€‘schemaâ€™d â€œfunctions,â€ ensure the descriptions scream *readâ€‘first safety*:
> 
> ```ts
> {
>  name: "repo.readFile",
>  description: "Read a file from the repo. Use this to confirm details before proposing changes. Safe, read-only.",
>  parameters: { type: "object", properties: { path: { type: "string" } }, required: ["path"] }
> }
> ```
> 
> And in the **system prompt**, add a short policy:
> 
> - â€œPrefer **read tools** before proposing patches.â€
> - â€œNever assume code; **read the file** first.â€
> - â€œWrites require explicit user confirmation or a failing test to justify change.â€
> 
> # 5) Mirror Codexâ€™s tool call style in Ollama
> Ollama doesnâ€™t natively execute tools; your CLI does. Two approaches:
> 
> **A. JSON mode (recommended if your model cooperates)**
> - Set `PARAMETER format json` in your Modelfile (models that support it will emit structured JSON).
> - In your **assistant** turn, require a single JSON object:
>  ```json
>  { "tool": "repo.readFile", "arguments": { "path": "services/ts/vision/src/index.ts" } }
>  ```
> - Your CLI reads it, executes, appends the tool result as a new **assistant/tool** or **system** turn, and re-asks the model.
> 
> **B. Functionâ€‘call emulation**
> If the model wonâ€™t stay JSONâ€‘pure, wrap with sentinels:
> ```
> <<<CALL
> {"tool":"repo.readFile","arguments":{"path":"..."}}
> CALL;
> ```
> â€¦and parse only content between `<<<CALL` â€¦ `CALL;`.
> 
> # 6) Minimal SYSTEM block addendum for Codexâ€‘CLI
> Append this to Codexâ€™s **system** (after you dump and review the current one):
> 
> ```
> Policy: Read-before-write.
> - Use repo.readFile / repo.search to gather facts before proposing diffs.
> - Prefer the smallest actionable change; when uncertain, write a single test first.
> - Respect project conventions:
>  - Use @shared/ts/dist/... for shared TS imports; do not invent new aliases.
>  - Services live in a flat services/; libraries in shared/.
> - Output tool calls in the CLIâ€™s expected JSON envelope. If you must answer in prose, keep it under one paragraph.
> ```
> 
> # 7) Sanityâ€‘check loop (tight)
> 1. Run a codex command with `--dump-prompt`.
> 2. Inspect `.codex-debug/system.txt` and `.codex-debug/tools.json`.
> 3. If tools donâ€™t include **readers**, add the readâ€‘only tools (or improve their descriptions) and reâ€‘dump.
> 4. Verify docs are included via the **Docs Context** block; if too large, enable summarization.
> 5. Test with a prompt that forces a read:
>  - â€œShow the exported API of `@shared/ts/dist/heartbeat` and which services import it.â€
> 
> You should see at least one `repo.search` â†’ `repo.readFile` chain before any patch proposals.
> 
> ---
> 
> If you want, I can draft a small patch set that adds `--dump-prompt`, a docsâ€‘context injector, and a readâ€‘first tool policy to your CLI once you tell me the file layout (or paste the `rg` hits).
<!-- UID: 4bd2e621-ef69-4425-97cc-d0f1b07e3324 -->

---




>[!nexus_user] **User** - 08/21/2025 at 8:36 AM
> Please look in this zip for tool call definitions, and system prompts
<!-- UID: 7e8b5d7c-39bc-40e6-b2ed-8485b4234715 -->




>[!nexus_agent] **Assistant** - 08/21/2025 at 8:39 AM
> Yesâ€”weâ€™ve got the sources right here. I cracked open your zip and found the **canonical system prompt** and the **exact tool definitions** Codex uses.
> 
> # What Codex actually sends the model
> 
> ## System prompt (canonical)
> - Lives at: `codex-rs/core/prompt.md`
> - It defines:
>  - **Personality**: concise, direct, friendly.
>  - **Responsiveness**: send a short *preamble* before using tools (what youâ€™re about to do, grouped and concise).
>  - **Planning**: keep and **update a plan** (via the `update_plan` tool).
>  - **Task execution**: favor *read-first*, small, safe steps; stream progress updates.
>  - **Testing**: add/ run tests where changes are risky.
>  - **Sandbox & approvals**: how/when to request escalations.
>  - **Final answer**: structure + style guidelines.
>  - **Tool guidelines**: detailed rules for `shell` and **apply_patch** (its custom diff envelope and hunk rules). 
>  *(File is long; these are the big sections.)*
> 
> ## Tool calls (exact names, schemas, behavior)
> 
> All are assembled in `codex-rs/core/src/openai_tools.rs` with helpers like `create_shell_tool`, `create_shell_tool_for_sandbox`, `create_apply_patch_tool`, and `plan_tool.rs` for `update_plan`.
> 
> ### 1) `shell`
> - **Name**: `"shell"`
> - **Purpose**: run terminal commands and return stdout/stderr.
> - **Schema** (key fields):
>  - `command`: **array of strings** (argv form) â€“ **required**
>  - `workdir`: string (optional)
>  - `timeout`: number (ms, optional)
> - Variants:
>  - **DefaultShell**
>  - **ShellWithRequest (sandboxed)** â†’ adds:
>  - `with_escalated_permissions`: boolean
>  - `justification`: string
>  - sandbox policy fields
>  - **LocalShell** (for models with local tooling)
> 
> ### 2) `apply_patch`
> - **Name**: `"apply_patch"`
> - **Purpose**: edit files using Codexâ€™s custom patch format.
> - **Schema**:
>  - `input`: string (**required**) â€” the *entire* patch envelope.
> - **Patch envelope** (summarized from the instruction docs):
>  ```
>  *** Begin Patch
>  *** Add File: path/to/file
>  + new content lines...
>  *** Update File: path/to/file
>  @@ optional header
>  context
>  - old
>  + new
>  *** End of File
>  *** Delete File: path/to/file
>  *** End Patch
>  ```
>  (Thereâ€™s a full, strict V4A-ish grammar in the docs; Codex validates this.)
> 
> ### 3) `update_plan`
> - **Name**: `"update_plan"`
> - **Purpose**: maintain a stepwise plan the user can see.
> - **Schema**:
>  - `explanation`: string (optional)
>  - `plan`: array of items (**required**)
>  - each item: `{ step: string, status: "todo" | "in_progress" | "done" | "blocked", id?: string }`
>  - **Rule**: *at most one* `in_progress` step at a time.
> 
> ### Other pieces youâ€™ll care about
> - MCP integration & tool-call config: `codex-rs/mcp-server/src/codex_tool_config.rs`, `codex_tool_runner.rs`.
>  - Config exposes things like `cwd`, `profile`, `approval_policy` (`untrusted`, `on-failure`, `on-request`, `never`), and `sandbox_mode` (`read-only`, `workspace-write`, `danger-full-access`).
> - Additional prompt snippets for compact/init modes:
>  - `core/src/prompt_for_compact_command.md`
>  - `tui/prompt_for_init_command.md`
> - Apply-patch usage doc for shell wrapping:
>  - `apply-patch/apply_patch_tool_instructions.md` 
>  (Codex often invokes `apply_patch` via a **shell call** running `apply_patch <<EOF ... EOF` in the right `workdir`.)
> 
> # How to make your Ollama model behave like Codex (CLI-compatible)
> 
> 1) **Use the same tool names and JSON shapes** in your runner:
>  - `"shell"`, `"apply_patch"`, `"update_plan"`.
>  - Ensure `shell.command` is **argv array**, not a single string.
> 
> 2) **Keep the system prompt** as close as possible to `core/prompt.md`:
>  - Especially the *preamble*, *read-first*, *testing*, *final answer*, and *tool guidelines* sections.
> 
> 3) **Nudge read-first behavior**:
>  - Prepend a short policy line in your Modelfile SYSTEM:
>  > â€œBefore proposing edits, use `shell` to grep (`rg`) and read files in â‰¤250-line chunks.â€
> 
> 4) **Patch execution path**:
>  - Either accept direct `apply_patch` tool calls, **or** mirror Codexâ€™s documented pattern: the model calls `shell` with `["apply_patch", "<<'EOF'\n*** Begin Patch\n...\n*** End Patch\nEOF\n"]`.
> 
> 5) **Planning**:
>  - Keep `update_plan` available and remind the model that **only one** step may be `in_progress`.
> 
> 6) **Approvals/Sandbox** (optional but supported):
>  - If you implement sandboxed shell, surface `with_escalated_permissions` and `justification` to stay in policy.
> 
> # Quick example: a Codex-style turn
> 
> **Assistant â†’ tool (plan):**
> ```json
> {"tool":"update_plan","arguments":{
>  "explanation":"Index sources, then adjust import path.",
>  "plan":[
>  {"step":"Search for HeartbeatClient usages","status":"in_progress"},
>  {"step":"Patch imports to @shared/ts/dist/heartbeat","status":"todo"},
>  {"step":"Run package tests","status":"todo"}
>  ]
> }}
> ```
> 
> **Assistant â†’ tool (read-first shell):**
> ```json
> {"tool":"shell","arguments":{
>  "command":["rg","-n","HeartbeatClient","services","shared","--hidden","--glob","!node_modules"],
>  "workdir":"./",
>  "timeout":15000
> }}
> ```
> 
> **Assistant â†’ tool (apply patch via shell wrapper):**
> ```json
> {"tool":"shell","arguments":{
>  "command":["bash","-lc","apply_patch <<'EOF'\n*** Begin Patch\n*** Update File: services/ts/vision/src/index.ts\n@@\n- import { HeartbeatClient } from '../../../shared/js/heartbeat/index.js';\n+ import { HeartbeatClient } from '@shared/ts/dist/heartbeat/index.js';\n*** End Patch\nEOF"],
>  "workdir":"./"
> }}
> ```
> 
> ---
> 
> If you want, I can cut a small patch that (a) exposes a `--dump-prompt` in your CLI to log the *final* system + tools + messages, and (b) drops a Codexâ€‘faithful SYSTEM block into your Modelfile so Ollama mirrors this flow exactly.
<!-- UID: 47130ddf-3e7f-4035-822c-30e1edffd6ad -->

---




>[!nexus_user] **User** - 08/21/2025 at 8:41 AM
> Is this prompt used when the --oss flag is used?
<!-- UID: 2958a6ea-abce-4377-be4d-1464544a7c21 -->




>[!nexus_agent] **Assistant** - 08/21/2025 at 8:41 AM
> {"queries":["system prompt used with --oss flag","--oss flag Codex CLI system prompt","Codex OSS mode system prompt file","codex cli --oss tool schema prompt","prompt_for_oss codex"],"source_filter":["files_uploaded_in_conversation"]}
<!-- UID: dd8698b6-e187-4674-b03f-b7bed3190420 -->

---




>[!nexus_agent] **Assistant** - 08/21/2025 at 8:41 AM
> From what I see in your zip: the **long system prompt** we just summarized (`core/prompt.md`) is the **default** one that Codex uses. 
> 
> The `--oss` flag switches Codex into **â€œopen-source modeâ€** â€” meaning it doesnâ€™t use OpenAIâ€™s GPT endpoints, but instead routes through your configured local model provider (like Ollama). However, it still pulls the same system prompt and tool schemas. There isnâ€™t a separate `prompt_for_oss.md` or alternative system file in the repo; the `--oss` flag just changes the transport, not the content of the prompt.
> 
> So: 
> - âœ… Yes, the prompt in `core/prompt.md` is still used when you run with `--oss`. 
> - âœ… The same `shell`, `apply_patch`, and `update_plan` tool definitions are loaded. 
> - ðŸ”„ The only difference is how the model call is made (local vs OpenAI), not the instructions Codex sends. 
> 
> Would you like me to trace exactly **where `--oss` is handled in the code** (CLI â†’ runner â†’ model call), so you can see the hook point where the system prompt is passed unchanged?
<!-- UID: b2673850-214e-490b-bbd8-825d533aefd1 -->

---




>[!nexus_user] **User** - 08/21/2025 at 8:42 AM
> yea... it seemed particularly dumb even considering the difference in model scale it didn't generate the right tool calls, at the end of a task when it gave control back over to the user, it seemed like it had no idea there was any prior context at all.
<!-- UID: 8eb104e3-05d2-4969-af03-370a91ed3811 -->




>[!nexus_agent] **Assistant** - 08/21/2025 at 8:42 AM
> Gotcha â€” that symptom set (no tool calls, amnesia when handing control back) almost always comes from **transport/template mismatches** in `--oss` mode, not from the long system prompt itself. Hereâ€™s a focused fix plan you can apply right now.
> 
> # Why this happens in `--oss`
> 1) **Chat template mismatch** 
>  Your CLI likely composes OpenAIâ€‘style messages/tools, but your local model (e.g., Qwen/Llama) needs a different role layout and/or â€œfunctionâ€‘callâ€ emulation. If the template is off by even a little (headers, stops, roles), tool use collapses into plain text.
> 
> 2) **Toolâ€‘call envelope mismatch** 
>  Codex expects either OpenAI â€œtools/functionsâ€ or strict **JSON envelopes**. Many local models wonâ€™t emit OpenAIâ€™s function-call objects unless you emulate them and **force JSON**.
> 
> 3) **History truncation or wrong role for tool results** 
>  If tool outputs are appended as `system` (or dropped), the model â€œlosesâ€ context at task end. Some adapters also clip earlier turns too aggressively.
> 
> 4) **Stop tokens / sampling** 
>  Wrong stops can cut off the JSON midâ€‘object. Tooâ€‘high temperature/top_p increases drift away from tool schemas.
> 
> ---
> 
> # Tight diagnostic checklist (10 minutes)
> - **Log the exact request/response** in `--oss`:
>  - Dump: system, tools JSON, full messages, and the **raw assistant output** from the local model before your parser touches it.
> - **Confirm template** used for `--oss`: 
>  Qwenâ€‘style `<|im_start|>system` â€¦ or Llamaâ€‘3 `<|start_header_id|>system` â€¦? Make sure it matches the model family.
> - **Confirm tool envelope**: Are you sending an OpenAI â€œtoolsâ€ array and expecting an OpenAIâ€‘style `tool_calls` in the reply? If so, your `--oss` adapter must **coerce** model output to that shape.
> - **Check role for tool results**: Append tool/toolâ€‘result messages with the same role Codex expects (often `assistant` + â€œtool resultâ€ suffix or a synthetic `tool` role your planner recognizes). If you mistakenly use `system`, the model wonâ€™t â€œseeâ€ it semantically.
> - **Verify max tokens / truncation**: Ensure the OSS provider isnâ€™t truncating history or clamping `num_ctx` too low.
> 
> ---
> 
> # Minimal hardening for `--oss` (keep your current prompt)
> ## 1) Force JSON tool calls (modelâ€‘agnostic)
> Add a wrapper instruction injected **only in OSS mode** (first line of system):
> 
> ```
> When you invoke a tool, respond with a SINGLE JSON object exactly like:
> {"tool":"<name>","arguments":{...}}
> No prose, no markdown, nothing before or after.
> ```
> 
> If your model supports it, set **Ollama**:
> - `PARAMETER format json`
> - conservative sampling: `temperature 0.2`, `top_p 0.9`, `repeat_penalty 1.05`
> - correct **stop tokens** for the model family
> 
> ## 2) Provide a modelâ€‘family chat template
> - **Qwen** (recommended for tool use):
>  - Use `<|im_start|>system` / `<|im_start|>user` / `<|im_start|>assistant` framing.
>  - Add `stop: "<|im_end|>"` and `stop: "<|im_start|>"`.
> - **Llamaâ€‘3**:
>  - Use `<|start_header_id|>system<|end_header_id|>â€¦<|eot_id|>` format and stops `<|eot_id|>`, `<|eom_id|>`.
> 
> (If you want, I can drop in the exact TEMPLATE blocks tailored to your chosen model.)
> 
> ## 3) Emulate OpenAI function calls on the adapter
> If Codex expects OpenAIâ€‘style tool calls, adapt the JSON object to that shape:
> 
> Input we require from model (OSS):
> ```json
> {"tool":"shell","arguments":{"command":["rg","-n","HeartbeatClient","services"]}}
> ```
> 
> Adapter â†’ Codex (OpenAI shim):
> ```json
> {
>  "role": "assistant",
>  "content": "",
>  "tool_calls": [
>  {
>  "id": "call_1",
>  "type": "function",
>  "function": {
>  "name": "shell",
>  "arguments": "{\"command\":[\"rg\",\"-n\",\"HeartbeatClient\",\"services\"]}"
>  }
>  }
>  ]
> }
> ```
> 
> And when you pass the **tool result back**, append a message Codex will actually feed to the model (donâ€™t hide it in `system`). For OSS, a robust pattern is:
> 
> ```json
> { "role": "tool", "name": "shell", "content": "<stdout/stderr JSON or text>" }
> ```
> 
> If your core expects a different role, normalize consistently.
> 
> ## 4) Readâ€‘first bias
> In **system** (OSSâ€‘only tail addendum):
> 
> - â€œBefore proposing edits, **use `shell` to `rg`/`cat`** to confirm file contents.â€
> - â€œPropose **one** small change at a time; if unsure, **write a test first**.â€
> 
> This sharply increases tool calls on small models.
> 
> ## 5) Preserve context across â€œhand back to userâ€
> - Ensure your â€œfinalâ€ assistant turn **doesnâ€™t reset** the thread. Some adapters start a new chat when the model emits â€œhandoff.â€
> - Append a **oneâ€‘paragraph recap** instruction for the final turn (OSSâ€‘only): 
>  â€œWhen you finish, briefly summarize what you changed and what remains. Do **not** start a new conversation.â€
> 
> ---
> 
> # Dropâ€‘in code snippets
> 
> ### A) Adapter: parse strict JSON tool calls
> ```ts
> function extractToolCall(raw: string) {
>  // find first JSON object
>  const start = raw.indexOf('{');
>  const end = raw.lastIndexOf('}');
>  if (start === -1 || end === -1 || end <= start) return null;
>  try {
>  const obj = JSON.parse(raw.slice(start, end + 1));
>  if (!obj || typeof obj !== 'object') return null;
>  if (typeof obj.tool !== 'string' || typeof obj.arguments !== 'object') return null;
>  return obj as { tool: string; arguments: Record<string, unknown> };
>  } catch { return null; }
> }
> ```
> 
> ### B) Coerce to OpenAI function-call shape (if needed)
> ```ts
> function toOpenAIFunctionCall(tc: { tool: string; arguments: any }) {
>  return {
>  role: "assistant",
>  content: "",
>  tool_calls: [{
>  id: "tc_" + Math.random().toString(36).slice(2),
>  type: "function",
>  function: { name: tc.tool, arguments: JSON.stringify(tc.arguments) }
>  }]
>  };
> }
> ```
> 
> ### C) Tool result message shape (consistent)
> ```ts
> function toolResultMessage(name: string, content: string) {
>  return { role: "tool", name, content };
> }
> ```
> 
> ---
> 
> # Model picks that behave well with tools locally
> - **Qwen2.5â€‘Coder 7B/14B Instruct** (very reliable at JSON & tools)
> - **DeepSeekâ€‘Coderâ€‘V2â€‘Lite** (strong tool compliance)
> - **Llamaâ€‘3.1 8B Instruct** (ok, but needs stricter JSON guardrails)
> 
> (If youâ€™re currently on `qwen3:4b`, thatâ€™s workable for reads/greps, but tool reliability jumps with 7B+.)
> 
> ---
> 
> # Quick acceptance test
> Prompt (user):
> > â€œFind the file that exports `HeartbeatClient`, show me its public API, then update a single import in `services/ts/vision/src/index.ts` to `@shared/ts/dist/heartbeat`. Run the vision tests.â€
> 
> **Expected trace**:
> 1. `update_plan` (optional)
> 2. `shell` â†’ `rg` to locate the file
> 3. `shell` â†’ `sed`/`cat` to show export
> 4. `shell` â†’ `apply_patch <<EOF â€¦`
> 5. `shell` â†’ `pnpm -w test -r --filter vision`
> 6. Final assistant recap (no context reset)
> 
> If step 2 becomes prose instead of a tool call, your **template or adapter** still needs tightening.
> 
> ---
> 
> If you want, tell me the exact model youâ€™ll stick with for `--oss` (e.g., **qwen2.5â€‘coder:7b**) and Iâ€™ll hand you the precise **TEMPLATE + stop tokens** and a tiny **diff** to add OSSâ€‘only JSON/tool coercion to your CLI.
<!-- UID: 210909e9-b9cd-4070-8bc9-ff81696752a2 -->

---




>[!nexus_user] **User** - 08/21/2025 at 10:55 AM
> qwen3:8b
<!-- UID: 2c8bb716-c483-4663-babb-f14fa8806acd -->




>[!nexus_agent] **Assistant** - 08/21/2025 at 10:55 AM
> Perfect â€” **qwen3:8b** is a good choice; itâ€™s small enough to run locally but much stronger at tool-call discipline than qwen3:4b. To get it behaving like Codex OSS, youâ€™ll want to:
> 
> ---
> 
> ## 1. Use the Qwen chat template
> Qwen-family models need `<|im_start|>` / `<|im_end|>` boundaries. In an Ollama `Modelfile`, do:
> 
> ```modelfile
> FROM qwen3:8b
> 
> PARAMETER num_ctx 16384
> PARAMETER temperature 0.2
> PARAMETER top_p 0.9
> PARAMETER repeat_penalty 1.05
> PARAMETER stop "<|im_start|>"
> PARAMETER stop "<|im_end|>"
> 
> TEMPLATE """
> <|im_start|>system
> {{ .System }}<|im_end|>
> {{- range .Messages }}
> <|im_start|>{{ .Role }}
> {{ .Content }}<|im_end|>
> {{- end }}
> <|im_start|>assistant
> """
> ```
> 
> That ensures the roles in your `messages.json` line up with what the model expects.
> 
> ---
> 
> ## 2. Harden the system prompt (OSS-only tail)
> Keep Codexâ€™s long system (`core/prompt.md`), but add at the end:
> 
> ```
> When invoking a tool, respond ONLY with a single JSON object:
> {"tool":"<name>","arguments":{...}}
> No prose, no markdown, no commentary.
> 
> Tools available: shell, apply_patch, update_plan.
> Use repo.reads (shell/rg, cat) before proposing patches.
> ```
> 
> That reduces drift and forces tool call emission.
> 
> ---
> 
> ## 3. Adapter: Coerce JSON â†’ OpenAI tool-calls
> Codex core still expects OpenAI-style function calls. Add a tiny shim in your `--oss` adapter:
> 
> ```ts
> function ossToOpenAIToolCall(raw: string) {
>  try {
>  const obj = JSON.parse(raw);
>  return {
>  role: "assistant",
>  content: "",
>  tool_calls: [{
>  id: "call_" + Math.random().toString(36).slice(2),
>  type: "function",
>  function: { name: obj.tool, arguments: JSON.stringify(obj.arguments) }
>  }]
>  };
>  } catch { return null; }
> }
> ```
> 
> And when you send a tool result back, **donâ€™t drop context**: append it as `{ role: "tool", name: "<toolname>", content: "<result>" }`.
> 
> ---
> 
> ## 4. Sanity stops & sampling
> - Stops: `<|im_end|>` and `<|im_start|>` prevent the model from â€œleakingâ€ extra assistant prose.
> - Temperature 0.2 â†’ deterministic JSON.
> - If you see truncation, bump `num_ctx` (16k is safe).
> 
> ---
> 
> ## 5. Quick acceptance test
> Run:
> 
> ```bash
> codex --oss --model codex-qwen3-8b --dump-prompt
> ```
> 
> Prompt:
> > â€œShow me where HeartbeatClient is defined, then update one import in vision service to @shared/ts/dist/heartbeat and run its tests.â€
> 
> Trace you should see:
> 1. `update_plan`
> 2. `shell` â†’ `rg`
> 3. `shell` â†’ `cat`
> 4. `shell` â†’ `apply_patch`
> 5. `shell` â†’ `pnpm test`
> 6. Short recap
> 
> If step 2 is prose, your adapter isnâ€™t parsing/feeding tool results correctly.
> 
> ---
> 
> Do you want me to draft the **exact `--oss` adapter diff** (TypeScript) for your CLI so `qwen3:8b` outputs get normalized into the OpenAI-style `tool_calls` Codex already consumes? Thatâ€™d be the cleanest way to close the gap.
<!-- UID: 0bcdfa02-76cc-4b61-b100-bf0704006182 -->

---


