You’re on the right scent. With 8 GB VRAM you can realistically test a **tool-calling + reasoning** text model (e.g., Gemma 3 “tools” variants) and a **vision** model (MiniCPM-V 4.5) using CPU offload—then compare them to remote judges (OpenRouter, Z.ai). Below is a tight plan + a TypeScript scaffold (ESM, FP-leaning, AVA tests, Web Components view) so you can start running real numbers today.

---

# What to test (fits your 8 GB target)

* **Tool-calling text (local)**
  Community Gemma 3 builds with *tools* are available on Ollama and explicitly aimed at low-VRAM rigs (several Q2/Q3 quant tags). These hook into Ollama’s native tool-calling API. ([Ollama][1])
  • Examples you can pull: `doomgrave/gemma3-tools` (Q2_K / Q3_K_S for ≤8 GB), `aliafshar/gemma3-it-qat-tools`, `Hituzip/gemma3-tools`, or *Gemma3n with tools* (`mashriram/gemma3nTools`). ([Ollama][1])

* **Vision (local)**
  **MiniCPM-V 4.5** has an Ollama card and a GGUF release; it’s designed for end-side deployment and supports “fast” vs “deep” thinking modes. Expect to lean on CPU RAM and reduce image sizes on an 8 GB GPU. ([Ollama][2])

* **Reasoning (local) optional**
  **Unsloth** dynamic GGUFs for Qwen3/DeepSeek bring `/think` and `/no_think` toggles you can benchmark; great for coding/latency trade-offs even if you rely on Gemma3 for tools. ([Unsloth Docs][3])

* **Remote “judge” / comparison baselines**
  **OpenRouter** gives you a single API to many top models (cheap to mix in a frontier baseline). **Z.ai** exposes **GLM-4.6** with strong coding/agentic claims and an OpenAI-style API—you can use it (or a small “nano” model you trust) as a scoring/judging model. ([OpenRouter][4])

---

# Benchmarks to include (no reinvention)

* **Coding:** Aider **Polyglot** (225 Exercism tasks; Docker harness). It already supports Ollama (`ollama_chat/<model>`). This is the most honest local-coding benchmark you can run easily. ([Aider][5])
* **Tool use:** **BFCL** (Berkeley Function Calling). CLI = `bfcl-eval`; it targets tool selection/args correctness and can evaluate outputs you generate against Ollama’s OpenAI-compatible `/v1`. ([PyPI][6])
* **General knowledge (optional):** **MMLU** (Hendrycks test) for stability checks, especially if you experiment with ultra-low-bit quants. ([GitHub][7])

---

# Test framework: simple, extensible, reproducible

Here’s the skeleton I’d put in your monorepo (flat package, native ESM, functional style, AVA tests). It **wraps** Aider/BFCL rather than re-implementing them, and gives you an SDK for custom tasks.

```txt
packages/
  bench-core/                # TS lib: types + runners + providers
  bench-cli/                 # TS CLI: reads YAML, runs matrix, writes JSON/CSV/MD
  bench-web/                 # Web Components UI (no framework) to view runs
```

## Mermaid: architecture at a glance

```mermaid
flowchart LR
  Config[YAML run config] --> CLI
  subgraph CLI[bench-cli]
    CLI -->|spawns| Aider[Polyglot Docker]
    CLI -->|spawns| BFCL[bfcl-eval]
    CLI --> Core
  end
  subgraph Core[bench-core]
    Core -->|OpenAI schema| ProviderOAI[Provider: OpenAI/OpenRouter/Z.ai]
    Core -->|OpenAI schema| ProviderOllama[Provider: Ollama /v1]
    Core --> Judges[LLM Judges]
    Core --> Store[Results store JSONL/CSV]
  end
  Judges --> Store
  Aider --> Store
  BFCL --> Store
  Store --> Web[bench-web viewer]
```

## Key design choices (why they’re sane)

* **Ride existing harnesses**: Aider Polyglot + BFCL are well-maintained and already wired for OpenAI-style APIs (Ollama supports tools and streaming). Less glue, fewer bugs. ([Aider][5])
* **Judge as a plug-in**: any OpenAI-compatible endpoint works (OpenRouter, Z.ai/GLM-4.6). Configuration decides which judge to use. ([OpenRouter][4])
* **Low-VRAM models**: Gemma3 “tools” variants and MiniCPM-V 4.5 are discoverable on Ollama and Hugging Face; your 8 GB rig can run them with aggressive quant/CPU offload. ([Ollama][1])

---

## Minimal code (TypeScript, ESM, FP-leaning)

> Drop these into `packages/bench-core` & `packages/bench-cli`. Uses **no mutation**, small pure transforms, and **AVA** for tests.

**packages/bench-core/src/types.ts**

```ts
export type Message = { role: 'system'|'user'|'assistant'|'tool'; content: string; name?: string };
export type Tool = { name: string; description: string; parameters: Record<string, unknown> }; // JSON Schema
export type ToolCall = { name: string; arguments: Record<string, unknown> };

export type ChatRequest = {
  model: string;
  messages: ReadonlyArray<Message>;
  tools?: ReadonlyArray<Tool>;
  tool_choice?: 'auto' | { type: 'function'; function: { name: string } };
  temperature?: number;
  max_tokens?: number;
  stream?: boolean;
};

export type ChatResponse = {
  id: string;
  model: string;
  messages: ReadonlyArray<Message>;
  tool_calls?: ReadonlyArray<ToolCall>;
  usage?: { input_tokens: number; output_tokens: number };
};

export type Provider = (req: ChatRequest) => Promise<ChatResponse>;

export type JudgeScore = { score: number; rationale: string; rubric: string };
export type Judger = (prompt: string, output: string, reference?: string) => Promise<JudgeScore>;

export type RunSpec = {
  name: string;
  provider: 'ollama'|'openrouter'|'zai';
  baseUrl?: string;           // e.g. http://localhost:11434/v1 or https://openrouter.ai/api/v1
  apiKeyEnv?: string;         // e.g. OPENROUTER_API_KEY, ZAI_API_KEY
  model: string;              // e.g. local gemma3-tools tag, or remote id
  categories: ReadonlyArray<'coding'|'tooluse'|'vision'|'custom'>;
};
```

**packages/bench-core/src/providers/openaiLike.ts**

```ts
import type { Provider, ChatRequest, ChatResponse } from '../types.js';

const headers = (apiKey?: string) =>
  apiKey ? { 'Content-Type': 'application/json', 'Authorization': `Bearer ${apiKey}` }
         : { 'Content-Type': 'application/json' };

export const makeProvider = (baseUrl: string, apiKey?: string): Provider =>
  async (req: ChatRequest): Promise<ChatResponse> => {
    const body = {
      model: req.model,
      messages: req.messages,
      tools: req.tools && req.tools.map(t => ({ type: 'function', function: t })),
      tool_choice: req.tool_choice,
      temperature: req.temperature ?? 0.2,
      max_tokens: req.max_tokens ?? 1024,
      stream: false
    };
    const r = await fetch(`${baseUrl}/chat/completions`, { method: 'POST', headers: headers(apiKey), body: JSON.stringify(body) });
    if (!r.ok) throw new Error(`HTTP ${r.status} ${await r.text()}`);
    const j = await r.json();
    // Normalize into ChatResponse (OpenAI schema → our schema)
    const choice = j.choices?.[0];
    const toolCalls = choice?.message?.tool_calls?.map((c: any) => ({
      name: c.function?.name, arguments: JSON.parse(c.function?.arguments ?? '{}')
    })) ?? [];
    const messages = [{ role: choice?.message?.role ?? 'assistant', content: choice?.message?.content ?? '' }];
    return { id: j.id, model: j.model, messages, tool_calls: toolCalls, usage: j.usage };
  };
```

**packages/bench-core/src/judges/llmJudge.ts**

```ts
import type { Judger } from '../types.js';
import { makeProvider } from '../providers/openaiLike.js';

export const makeLLMJudge = (baseUrl: string, apiKey: string|undefined, model: string): Judger => {
  const provider = makeProvider(baseUrl, apiKey);
  return async (prompt, output, reference) => {
    const sys = 'You are a strict, concise evaluator. Return JSON: {"score":0-1,"rationale":"..."}';
    const usr = `Rubric:\n${prompt}\n\nCandidate Output:\n${output}\n${reference ? `\nReference:\n${reference}`:''}\n\nScore only.`;
    const res = await provider({ model, messages: [{role:'system',content:sys},{role:'user',content:usr}] });
    try {
      const parsed = JSON.parse(res.messages[0]?.content ?? '{}');
      return { score: Number(parsed.score ?? 0), rationale: String(parsed.rationale ?? ''), rubric: prompt };
    } catch {
      return { score: 0, rationale: 'judge parse error', rubric: prompt };
    }
  };
};
```

**packages/bench-cli/src/config.example.yaml**

```yaml
runs:
  - name: gemma3-tools-local
    provider: ollama
    baseUrl: http://localhost:11434/v1
    model: doomgrave/gemma3-tools:q3_k_s     # <= 8GB VRAM friendly
    categories: [tooluse, coding]

  - name: minicpmv-local
    provider: ollama
    baseUrl: http://localhost:11434/v1
    model: openbmb/minicpm-v4.5
    categories: [vision]

judges:
  openrouter:
    baseUrl: https://openrouter.ai/api/v1
    apiKeyEnv: OPENROUTER_API_KEY
    model: openai/chatgpt-4o-latest

  zai:
    baseUrl: https://open.bigmodel.cn/api/paas/v4
    apiKeyEnv: ZAI_API_KEY
    model: glm-4.6
```

(OpenRouter exposes many models via a single OpenAI-style API; Z.ai documents GLM-4.6 with OpenAI-compatible endpoints.) ([OpenRouter][4])

**packages/bench-cli/src/index.ts**

```ts
#!/usr/bin/env node
import fs from 'node:fs/promises';
import path from 'node:path';
import { execa } from 'execa';
import YAML from 'yaml';
import { makeProvider } from '../../bench-core/src/providers/openaiLike.js';
import { makeLLMJudge } from '../../bench-core/src/judges/llmJudge.js';
import type { RunSpec } from '../../bench-core/src/types.js';

const readYaml = (p: string) => fs.readFile(p,'utf8').then(YAML.parse);
const envKey = (k?: string) => (k ? process.env[k] : undefined);

const runAider = async (modelTag: string) =>
  execa('bash', ['-lc', `cd .third_party/aider || { mkdir -p .third_party && git clone https://github.com/Aider-AI/aider.git .third_party/aider; }
    cd .third_party/aider && ./benchmark/docker_build.sh && ./benchmark/docker.sh &&
    docker exec -i aider-benchmark bash -lc "pip install -e .[dev] && ./benchmark/benchmark.py run --model ollama_chat/${modelTag} --edit-format whole --threads 1 --exercises-dir polyglot-benchmark"`], { stdio: 'inherit' });

const runBFCL = async (baseUrl: string) =>
  execa('bash', ['-lc', `python - <<'PY'\nimport os, subprocess\nsubprocess.check_call(['python','-m','pip','install','-U','bfcl-eval'])\nopen('.bfcl.env','w').write('OPENAI_API_BASE=${baseUrl}\\n')\nsubprocess.run(['bfcl','generate','--model','openai-prompt','--test-category','simple,parallel,multiple','--skip-server-setup'], check=False)\nsubprocess.run(['bfcl','evaluate','--model','openai-prompt','--test-category','simple,parallel,multiple'], check=False)\nPY`], { stdio: 'inherit' });

const writeResult = (file: string, obj: unknown) =>
  fs.mkdir(path.dirname(file), { recursive: true }).then(() => fs.writeFile(file, JSON.stringify(obj, null, 2)));

const runOne = async (spec: RunSpec, judgeCfg: { baseUrl: string; apiKeyEnv?: string; model: string }|undefined) => {
  const base = spec.baseUrl || 'http://localhost:11434/v1';
  const provider = makeProvider(base, envKey(spec.apiKeyEnv));
  // tiny demo custom task: tool-use (calculator)
  const tools = [{ name: 'add', description: 'Add two ints', parameters: { type:'object', properties:{a:{type:'integer'}, b:{type:'integer'}}, required:['a','b']} }];
  const messages = [{ role: 'user', content: 'Use tools to compute 23 + 19.' }];
  const res = await provider({ model: spec.model, messages, tools, tool_choice: 'auto' });

  const outDir = `./bench_results/${spec.name}`;
  await writeResult(`${outDir}/tool-call.json`, res);

  if (spec.categories.includes('coding')) await runAider(spec.model);
  if (spec.categories.includes('tooluse')) await runBFCL(base);

  if (judgeCfg) {
    const judge = makeLLMJudge(judgeCfg.baseUrl, envKey(judgeCfg.apiKeyEnv), judgeCfg.model);
    const scored = await judge('Score 1 if the arithmetic is correct, else 0.', JSON.stringify(res.tool_calls ?? []), '{"expected":{"name":"add","arguments":{"a":23,"b":19}}}');
    await writeResult(`${outDir}/judge.json`, scored);
  }
};

const main = async () => {
  const cfg = await readYaml(process.argv[2] ?? 'config.yaml');
  const judgePref = cfg.judges?.zai ?? cfg.judges?.openrouter;
  for (const spec of cfg.runs as RunSpec[]) await runOne(spec, judgePref);
};

main().catch(e => { console.error(e); process.exit(1); });
```

**packages/bench-cli/ava.config.mjs**

```js
export default { files: ['test/*.test.ts'], extensions: { ts: 'module' }, nodeArguments: ['--loader=ts-node/esm'] }
```

**packages/bench-cli/test/smoke.test.ts**

```ts
import test from 'ava';
import { makeProvider } from '../../bench-core/src/providers/openaiLike.js';

test('normalizes OpenAI-style responses', async t => {
  // mock server in CI or set OLLAMA on dev; keep pure for now
  t.truthy(typeof makeProvider === 'function');
});
```

---

## Runbook (short)

1. **Local models**

   * Pull a Gemma 3 tools model that fits: e.g. `ollama pull doomgrave/gemma3-tools:q3_k_s` (≤ 8 GB VRAM), and **MiniCPM-V 4.5** for vision (`ollama pull openbmb/minicpm-v4.5`). ([Ollama][1])
   * Make sure your Ollama has **tool support** (July 2024+) and use OpenAI `/v1`. ([Ollama][8])

2. **Judges**

   * Set `OPENROUTER_API_KEY` or `ZAI_API_KEY`, pick a judge in `config.yaml` (OpenRouter or Z.ai GLM-4.6). ([OpenRouter][4])

3. **Bench harnesses**

   * Coding: run Polyglot via Docker (`benchmark.py`), which the CLI above shells for you. ([Aider][9])
   * Tool use: run `bfcl-eval` prompt mode against your local `/v1`. ([PyPI][6])
   * (Optional) MMLU with `hendrycks/test` or lm-eval for a sanity sweep. ([GitHub][7])

4. **VRAM squeezers**

   * Use lightweight Gemma3 “tools” builds (Q2/Q3) and keep **KV cache quant** + **Flash Attention** enabled in Ollama. This is officially supported and materially reduces memory pressure. ([Ollama][8])

---

## Reality checks & tips

* **Gemma3 tools** are community builds; behavior varies by template/quant. Stick to popular tags with clear “tools” readmes. (Ollama’s own posts confirm first-class tool-calling in the runtime.) ([Ollama][8])
* **MiniCPM-V 4.5** is strong for OCR/doc QA, but image preprocessing matters on small GPUs—cap resolution aggressively. ([Hugging Face][10])
* **BFCL vs “plain instruct”**: if a model hasn’t been tuned for tool-calling, absolute scores will be low. Use BFCL for *comparative* trends unless you include a true FC-tuned baseline. ([Gorilla][11])
* **Unsloth**: if you fine-tune later, export with Unsloth’s **Ollama/Modelfile** flow to keep chat templates right; then drop the GGUF into this same harness. Their Docker guide is current and clean. ([Unsloth Docs][12])

---

If you want, I’ll expand this into a proper package in your monorepo (`riatzukiza/promethean`) with pnpm workspace, AVA wired, and a small Web Components viewer that reads `bench_results/*.json` and renders sortable comparisons. For now, this scaffold runs **coding (Aider)** + **tool use (BFCL)** + **custom SDK tasks** against **Ollama/OpenRouter/Z.ai**, and it won’t fight your 8 GB limit.

**Refs**

* Ollama tool-calling support & streaming tools. ([Ollama][8])
* Gemma3 “tools” models on Ollama (including low-VRAM variants). ([Ollama][1])
* MiniCPM-V 4.5 (Ollama card & GGUF docs). ([Ollama][2])
* Aider Polyglot benchmark (Docker harness; Ollama integration). ([Aider][5])
* BFCL CLI. ([PyPI][6])
* MMLU (Hendrycks test). ([GitHub][7])
* Unsloth: Docker training, dynamic GGUFs, /think toggle. ([Unsloth Docs][12])
* OpenRouter models & docs; Z.ai GLM-4.6 docs/blog. ([OpenRouter][13])
* Fresh Ollama PR toward **Qwen3-VL** parsing (images + tools)—nice future-proofing signal. ([GitHub][14])

Want me to wire this into a PNPM workspace with an Obsidian-friendly Markdown report generator next?

[1]: https://ollama.com/search?q=gemma3+tools "gemma3 tools · Ollama Search"
[2]: https://ollama.com/openbmb/minicpm-v4.5?utm_source=chatgpt.com "openbmb/minicpm-v4.5"
[3]: https://docs.unsloth.ai/models/qwen3-how-to-run-and-fine-tune?utm_source=chatgpt.com "Qwen3: How to Run & Fine-tune | Unsloth Documentation"
[4]: https://openrouter.ai/docs/models?utm_source=chatgpt.com "Access 400+ AI Models Through One API"
[5]: https://aider.chat/docs/leaderboards/?utm_source=chatgpt.com "Aider LLM Leaderboards"
[6]: https://pypi.org/project/bfcl-eval/?utm_source=chatgpt.com "bfcl-eval"
[7]: https://github.com/hendrycks/test "GitHub - hendrycks/test: Measuring Massive Multitask Language Understanding | ICLR 2021"
[8]: https://ollama.com/blog/tool-support?utm_source=chatgpt.com "Tool support · Ollama Blog"
[9]: https://aider.chat/docs/benchmarks.html?utm_source=chatgpt.com "GPT code editing benchmarks"
[10]: https://huggingface.co/openbmb/MiniCPM-V-4_5-gguf?utm_source=chatgpt.com "openbmb/MiniCPM-V-4_5-gguf"
[11]: https://gorilla.cs.berkeley.edu/leaderboard.html?utm_source=chatgpt.com "Berkeley Function Calling Leaderboard (BFCL) V4"
[12]: https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker?utm_source=chatgpt.com "How to Train LLMs with Unsloth & Docker"
[13]: https://openrouter.ai/models?order=pricing-low-to-high&utm_source=chatgpt.com "Models"
[14]: https://github.com/ollama/ollama/pull/12526 "Grace/qwen3 vl renderer by gr4ceG · Pull Request #12526 · ollama/ollama · GitHub"
